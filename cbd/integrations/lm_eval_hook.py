"""EleutherAI LM Evaluation Harness integration hook.

lm-evaluation-harness is the most popular LLM evaluation framework.
This hook provides seamless integration for bias detection.
"""
from typing import Dict, List, Optional, Any
import json
from pathlib import Path


class LMEvaluationHarnessHook:
    """Hook for EleutherAI lm-evaluation-harness.
    
    Usage:
    ------
    ```python
    from cbd.integrations import LMEvaluationHarnessHook
    
    # Create hook
    hook = LMEvaluationHarnessHook(output_dir='./bias_detection')
    
    # In lm_eval command
    # lm_eval --model hf --model_args pretrained=gpt2 --tasks hellaswag,arc_easy
    
    # After evaluation
    results = hook.load_results('results.json')
    analysis = hook.detect_bias(results)
    print(analysis['summary'])
    ```
    """
    
    def __init__(
        self,
        output_dir: str = './cbd_lm_eval',
        n_permutations: int = 1000,
        alpha: float = 0.05,
        fast_mode: bool = False
    ):
        """Initialize LM Evaluation Harness hook.
        
        Parameters:
        -----------
        output_dir : str
            Directory to save analysis results
        n_permutations : int, default=1000
            Number of permutations for detection
        alpha : float, default=0.05
            Significance level
        fast_mode : bool, default=False
            Use fast mode with pre-computed thresholds
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.n_permutations = n_permutations
        self.alpha = alpha
        self.fast_mode = fast_mode
    
    def load_results(self, results_file: str) -> Dict:
        """Load lm-evaluation-harness results.
        
        Parameters:
        -----------
        results_file : str
            Path to results JSON file
        
        Returns:
        --------
        dict
            Parsed results
        """
        with open(results_file) as f:
            return json.load(f)
    
    def detect_bias(self, results: Dict) -> Dict:
        """Detect circular bias in lm-evaluation-harness results.
        
        Parameters:
        -----------
        results : dict
            lm-evaluation-harness results
        
        Returns:
        --------
        dict
            Bias detection analysis
        """
        from cbd.risk_summary import generate_batch_risk_summary
        
        # Extract task results
        task_results = results.get('results', {})
        
        analysis = {
            'n_tasks': len(task_results),
            'tasks': list(task_results.keys()),
            'per_task_analysis': {},
            'overall_risk': 'Unknown'
        }
        
        # Analyze each task
        detection_results = []
        for task_name, task_metrics in task_results.items():
            # Extract primary metric (usually acc or acc_norm)
            acc = task_metrics.get('acc', task_metrics.get('acc_norm', None))
            
            if acc is not None:
                # Heuristic: flag if accuracy > 0.90 for most tasks
                is_suspicious = acc > 0.90
                
                task_analysis = {
                    'task': task_name,
                    'accuracy': acc,
                    'suspicious': is_suspicious,
                    'all_metrics': task_metrics
                }
                
                analysis['per_task_analysis'][task_name] = task_analysis
                
                # Mock detection result
                detection_results.append({
                    'p_value': 0.005 if is_suspicious else 0.3,
                    'alpha': self.alpha
                })
        
        # Generate summary
        if detection_results:
            summary = generate_batch_risk_summary(
                detection_results,
                test_names=list(task_results.keys())
            )
            analysis['summary'] = summary
            
            # Determine overall risk
            n_suspicious = sum(1 for r in detection_results if r['p_value'] <= self.alpha)
            if n_suspicious == 0:
                analysis['overall_risk'] = 'Low'
            elif n_suspicious <= len(detection_results) * 0.3:
                analysis['overall_risk'] = 'Medium'
            else:
                analysis['overall_risk'] = 'High'
        else:
            analysis['summary'] = "No tasks analyzed"
        
        # Save analysis
        output_file = self.output_dir / 'lm_eval_bias_analysis.json'
        with open(output_file, 'w') as f:
            json.dump(analysis, f, indent=2)
        
        return analysis
    
    def create_report(self, analysis: Dict, output_file: str = 'lm_eval_report.md'):
        """Create markdown report.
        
        Parameters:
        -----------
        analysis : dict
            Bias detection analysis
        output_file : str
            Output file name
        """
        report_path = self.output_dir / output_file
        
        with open(report_path, 'w') as f:
            f.write("# LM Evaluation Harness - Bias Detection Report\n\n")
            
            f.write(f"## Overall Assessment\n\n")
            f.write(f"- **Risk Level:** {analysis.get('overall_risk', 'Unknown')}\n")
            f.write(f"- **Tasks Analyzed:** {analysis.get('n_tasks', 0)}\n")
            f.write(f"- **Summary:** {analysis.get('summary', 'N/A')}\n\n")
            
            f.write("## Per-Task Results\n\n")
            f.write("| Task | Accuracy | Suspicious | Status |\n")
            f.write("|------|----------|------------|--------|\n")
            
            for task, result in analysis.get('per_task_analysis', {}).items():
                status = "âš ï¸ Review" if result.get('suspicious') else "âœ… OK"
                f.write(f"| {task} | {result.get('accuracy', 0):.4f} | "
                       f"{result.get('suspicious', False)} | {status} |\n")
            
            f.write("\n## Recommendations\n\n")
            
            if analysis.get('overall_risk') == 'High':
                f.write("- ðŸš¨ **High risk detected** - Immediate investigation recommended\n")
                f.write("- Review training data for potential leakage\n")
                f.write("- Verify evaluation setup and data splits\n")
            elif analysis.get('overall_risk') == 'Medium':
                f.write("- âš¡ **Medium risk** - Further validation recommended\n")
                f.write("- Re-run evaluation on independent test set\n")
            else:
                f.write("- âœ… **Low risk** - Results appear normal\n")
                f.write("- Continue monitoring in production\n")
        
        print(f"Report created: {report_path}")
        return report_path


def quick_check(results_file: str, output_dir: str = './cbd_lm_eval') -> Dict:
    """Quick bias check for lm-evaluation-harness results.
    
    Parameters:
    -----------
    results_file : str
        Path to lm-eval results JSON
    output_dir : str
        Output directory
    
    Returns:
    --------
    dict
        Analysis results
    
    Examples:
    ---------
    >>> # After running lm_eval
    >>> analysis = quick_check('results.json')
    >>> print(analysis['summary'])
    """
    hook = LMEvaluationHarnessHook(output_dir=output_dir, fast_mode=True)
    results = hook.load_results(results_file)
    analysis = hook.detect_bias(results)
    hook.create_report(analysis)
    return analysis
