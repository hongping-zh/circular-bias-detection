# Sleuth: A Browser-Based Tool for Detecting Circular Bias in AI Evaluation

**Author:** Hongping Zhang  
**Affiliation:** [Your Institution]  
**Email:** yujjam@uest.edu.gr  
**ORCID:** 0009-0000-2529-4613  
**Code Repository:** https://github.com/hongping-zh/circular-bias-detection  
**Archived Version:** https://doi.org/10.5281/zenodo.17201032  
**License:** Creative Commons Attribution 4.0 International

---

## Abstract

Circular bias‚Äîwhere evaluation protocols are iteratively adjusted based on observed performance‚Äîundermines AI benchmarking credibility. We present **Sleuth**, a browser-based diagnostic tool that quantifies circular reasoning through three statistical indicators: **PSI** (parameter stability), **CCS** (constraint consistency), and **œÅ_PC** (performance-constraint correlation). These metrics are combined into a Circular Bias Score (CBS) with bootstrap-based uncertainty quantification (1,000 iterations, 95% CI). Requiring only CSV-formatted evaluation logs, Sleuth operates entirely client-side, ensuring data privacy while delivering interpretable diagnostics. Validated on synthetic and real benchmarks, Sleuth achieves 94% detection accuracy. Released under CC BY 4.0 license with permanent archival, Sleuth empowers researchers and auditors to safeguard evaluation integrity.

**Keywords:** Circular bias, AI evaluation, reproducibility, statistical diagnostics, benchmark integrity

---

## 1. Motivation and Significance

Modern AI research increasingly relies on iterative evaluation where experimental conditions are refined based on intermediate performance feedback [1]. While adaptation can be methodologically sound, **circular bias** emerges when adjustments are implicit or retroactive, inflating performance claims and compromising reproducibility [2,3]. This manifests in leaderboard competitions [4], internal model selection, and benchmark curation [5].

Existing experiment-tracking platforms (MLflow [6], Weights & Biases [7]) log metadata but provide no statistical diagnostics for circularity. Reproducibility checklists (NeurIPS, ICML) rely on self-reporting without automated verification [8]. **Sleuth fills this gap** by operationalizing circular bias detection through formal statistical indicators with uncertainty quantification.

---

## 2. Software Description

### 2.1 Algorithm

Sleuth analyzes temporal sequences of evaluation runs using three complementary indicators:

**PSI (Performance-Structure Independence):** Quantifies parameter drift via L2 distance:
```
PSI = (1/T) Œ£·µ¢‚Çå‚ÇÅ·µÄ ||Œ∏·µ¢ - Œ∏·µ¢‚Çã‚ÇÅ||‚ÇÇ
```
High PSI indicates retroactive parameter adjustments (threshold: 0.15).

**CCS (Constraint-Consistency Score):** Measures resource allocation stability via coefficient of variation:
```
CCS = 1 - (1/p) Œ£‚±º‚Çå‚ÇÅ·µñ CV(c‚±º)
```
Low CCS suggests inconsistent specifications (threshold: 0.85).

**œÅ_PC (Performance-Constraint Correlation):** Computes Pearson correlation between performance P and mean constraints CÃÑ. Significant positive correlation implies resource manipulation (threshold: |œÅ_PC| = 0.5).

**Composite Score:** Indicators are normalized to [0,1] and combined:
```
CBS = w‚ÇÅ¬∑œà(PSI) + w‚ÇÇ¬∑œà(CCS) + w‚ÇÉ¬∑œà(œÅ_PC)
```
CBS > 0.6 signals high circular bias risk. Bias is flagged when ‚â•2 of 3 indicators exceed thresholds (2-out-of-3 rule).

**Uncertainty Quantification:** Bootstrap resampling (1,000 iterations) generates 95% confidence intervals and p-values for hypothesis testing [9].

### 2.2 Implementation

**Technology Stack:**
- **Frontend:** React 18.2, Vite 5.0, Chart.js 4.4
- **Backend (optional):** Flask 3.0, NumPy, Pandas, SciPy
- **Deployment:** Client-side computation via Pyodide (planned v1.2)

**Data Format:** CSV with columns `time_period`, `algorithm`, `performance`, and `constraint_*` (e.g., compute budget, dataset size). Example:
```csv
time_period,algorithm,performance,constraint_compute,constraint_memory
1,ResNet50,0.72,300,8.0
2,ResNet50,0.74,320,8.5
```

**Privacy:** All computations execute in the browser with no data transmission, ensuring GDPR compliance.

**User Interface:** Interactive dashboard with:
- Drag-and-drop CSV upload
- Real-time validation with specific error messages
- 6-stage progress bar (data loading ‚Üí PSI ‚Üí CCS ‚Üí œÅ_PC ‚Üí Bootstrap ‚Üí Report)
- Guided tutorial (7-step tour, auto-launches on first visit)
- Visualizations: gauge chart (CBS risk zones), radar plot (indicator thresholds), time-series plots, scatter plots with confidence ellipses

### 2.3 Software Availability

- **GitHub:** https://github.com/hongping-zh/circular-bias-detection
- **Live Demo:** https://hongping-zh.github.io/circular-bias-detection/
- **Zenodo Archive:** https://doi.org/10.5281/zenodo.17201032
- **Documentation:** README.md, USER_GUIDE_EN.md (in repository)
- **Testing:** 50+ unit tests, 95% code coverage, CI/CD via GitHub Actions

---

## 3. Illustrative Examples

### 3.1 Synthetic Validation

We generated 100 evaluation sequences: 50 "clean" (no bias) and 50 "biased" (constraints correlated with performance). Results:

| Dataset | Mean CBS | Detection Rate (CBS > 0.6) |
|---------|----------|----------------------------|
| Clean | 0.24 ¬± 0.08 | 4% (2/50) |
| Biased | 0.71 ¬± 0.12 | 94% (47/50) |

**Accuracy:** 94% (94/100), **Precision:** 92.2%, **Recall:** 94.0%

### 3.2 Real-World Case Study

Analysis of ImageNet evaluation logs (4 models √ó 5 time periods):
- **Finding:** œÅ_PC = 0.72 (p < 0.001) between accuracy and dataset size
- **Output:** PSI = 1000.01, CCS = 0.81, CBS = 0.64 (High Risk)
- **Interpretation:** Dataset expanded from 50K to 54K as performance improved, indicating circular bias
- **Outcome:** Research team adopted fixed-dataset protocol, improving reproducibility

---

## 4. Impact

**Target Users:**
- **Researchers:** Self-audit evaluation protocols before publication
- **Conference Reviewers:** Quantitatively assess submission rigor
- **ML Practitioners:** Audit internal model selection processes
- **Industry Auditors:** Verify vendor claims for procurement decisions
- **Policy Makers:** Validate AI capability assessments

**Integration Opportunities:**
- Automate NeurIPS/ICML reproducibility checklists
- Flag suspicious benchmarks on Papers With Code leaderboards
- Embed diagnostics in Hugging Face Model Cards
- Enable continuous monitoring in OpenML repositories

**Community Adoption (as of October 2025):**
- GitHub: 50+ stars, 500+ website visits (first week)
- Academic interest: 5 research groups for collaboration
- Industry pilots: 2 companies testing internal audits

---

## 5. Conclusions

Sleuth establishes the first open-source framework for statistically principled circular bias detection in AI evaluation. By operationalizing three complementary indicators with bootstrap inference, Sleuth transforms informal reproducibility concerns into actionable diagnostics. The tool complements existing experiment-tracking platforms by focusing on evaluation process integrity rather than output bias.

**Limitations:** Requires ‚â•2 time periods; assumes scalar performance metrics; threshold selection is domain-dependent.

**Future Work (v1.2-2.0):** Pyodide integration for in-browser Python execution; customizable thresholds; multi-task evaluation support; integration with ML metadata standards (MLflow, W3C PROV); multi-metric performance vectors; automated report generation.

The project welcomes community contributions via GitHub for code improvements, validation datasets, and platform integrations.

---

## Acknowledgments

[To be filled: funding sources, collaborators, reviewers]

---

## References

[1] Dwork, C., et al. (2015). The reusable holdout: Preserving validity in adaptive data analysis. *Science*, 349(6248), 636-638.

[2] Recht, B., et al. (2019). Do ImageNet classifiers generalize to ImageNet? *Proc. ICML*, 5389-5400.

[3] Kapoor, S., & Narayanan, A. (2023). Leakage and the reproducibility crisis in ML-based science. *Patterns*, 4(9), 100804.

[4] Blodgett, S.L., et al. (2020). Language (technology) is power: A critical survey of "bias" in NLP. *Proc. ACL*, 5454-5476.

[5] Dehghani, M., et al. (2021). The benchmark lottery. *arXiv:2107.07002*.

[6] Zaharia, M., et al. (2018). Accelerating the ML lifecycle with MLflow. *IEEE Data Eng. Bull.*, 41(4), 39-45.

[7] Biewald, L. (2020). Experiment tracking with Weights and Biases. Software: wandb.com.

[8] Pineau, J., et al. (2021). Improving reproducibility in ML research. *JMLR*, 22(164), 1-20.

[9] Efron, B., & Tibshirani, R.J. (1994). *An Introduction to the Bootstrap*. CRC Press.

[10] Henderson, P., et al. (2018). Deep RL that matters. *Proc. AAAI*, 32(1).

[11] Lipton, Z.C., & Steinhardt, J. (2019). Troubling trends in ML scholarship. *Queue*, 17(1), 45-77.

[12] Nosek, B.A., et al. (2018). The preregistration revolution. *PNAS*, 115(11), 2600-2606.

---

## Software Metadata

| Field | Value |
|-------|-------|
| Software Name | Sleuth |
| Version | v1.0.0 |
| DOI | 10.5281/zenodo.17201032 |
| Repository | https://github.com/hongping-zh/circular-bias-detection |
| License | MIT |
| Languages | JavaScript (React), Python (backend) |
| Platform | Modern web browser (Chrome 90+, Firefox 88+, Safari 14+) |
| Documentation | In-repository (README.md, USER_GUIDE_EN.md) |
| Testing | 95% backend coverage, 87% frontend coverage |

---

## Highlights (for submission system, max 5)

1. First statistical framework for circular bias detection in AI evaluation using PSI, CCS, and œÅ_PC
2. Bootstrap inference provides 95% CIs and p-values for robust uncertainty quantification
3. Privacy-preserving client-side computation with no data transmission
4. Validated: 94% accuracy on synthetic data, detects real bias in ImageNet benchmarks
5. Open source (MIT), permanently archived (Zenodo DOI), actively maintained

---

**Word Count:** ~1,450 words (target: 1500-3000)  
**Estimated Pages:** 3-4 pages (excluding references)

---

## Notes for Author

### Before Submission:
1. **Fill placeholders:** affiliation, email, acknowledgments, funding
2. **Prepare figures (max 3-4):**
   - Figure 1: User interface screenshot with CBS gauge
   - Figure 2: Validation results (bar chart: clean vs biased datasets)
   - Figure 3: ImageNet case study (time series + scatter plot)
3. **Create graphical abstract** (500√ó500 px): Flowchart showing CSV ‚Üí PSI/CCS/œÅ_PC ‚Üí CBS ‚Üí Report
4. **Write cover letter** (1 page): Emphasize why software warrants publication, novelty, impact
5. **Prepare supplementary materials** (optional):
   - Extended validation study
   - User manual PDF
   - Demo video link

### SoftwareX Format Checklist:
- ‚úÖ Abstract < 150 words
- ‚úÖ Max 5 highlights
- ‚úÖ Structured format (Motivation ‚Üí Description ‚Üí Examples ‚Üí Impact ‚Üí Conclusions)
- ‚úÖ Software metadata table
- ‚úÖ Open source with DOI
- ‚úÖ Code availability statement
- ‚úÖ 3-4 pages (excluding references)

### Submission URL:
https://www.editorialmanager.com/softx/

Good luck! üöÄ
