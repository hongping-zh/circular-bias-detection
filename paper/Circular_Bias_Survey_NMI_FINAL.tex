\documentclass[11pt]{article}

% Nature Machine Intelligence style packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{geometry}
\geometry{a4paper, margin=2.5cm}
\usepackage{setspace}
\doublespacing
\usepackage{lineno}
\linenumbers

% Math and symbols
\usepackage{amsmath,amssymb}
\usepackage{mathtools}

% Graphics
\usepackage{graphicx}
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

% References and citations
\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{naturemag}

% Hyperlinks
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

% Custom commands
\newcommand{\perp}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}

\begin{document}

% Title page
\begin{center}
{\LARGE\bfseries Circular Bias in Deployed AI Systems: A Systematic Literature Review and Taxonomy}

\vspace{1cm}

{\large Hongping Zhang\textsuperscript{1,*}}

\vspace{0.5cm}

{\small \textsuperscript{1}Independent Researcher, Beijing, China}

{\small *Corresponding author: zhanghongping1982@gmail.com}

\vspace{0.5cm}

{\small Date: October 29, 2025}
\end{center}

\vspace{1cm}

% Abstract
\begin{abstract}
AI systems continually retrain on the data generated from their own predictions, creating feedback loops that amplify biases and diminish fairness over time. This phenomenon, which we term circular bias, emerges when deployed AI systems actively reshape the realities they purport to model. Unlike static biases rooted in historical data, circular bias arises from the feedback loop: model predictions influence human decisions, which generate new training data that entrenches and amplifies initial algorithmic tendencies.

Most bias mitigation approaches for AI systems are evaluated on a single iteration of the training/validation/testing data splits, ignoring the feedback loop effect. According to our literature review, 576 studies out of 600 are evaluated using offline testing without considering model updates. Recent work indicates a shift towards dynamic bias auditing. Simulations that replay many retraining rounds show that bias-mitigation approaches, which initially succeed, can fail to mitigate the bias in the long term.

We therefore present a systematic literature review of bias mitigation methods that explicitly consider AI feedback loops and are validated in multi-round simulations or live A/B tests. Screening 600 papers yields 32 primary studies published between 2019--2025 (24 from traditional ML systems and 8 from generative AI systems). Each study is coded on nine dimensions: mitigation technique, biases addressed, dynamic testing set-up, evaluation focus, application domain, ML task, long-term robustness, feedback loop type, and mitigation persistence, organising them into a reusable taxonomy. The taxonomy offers industry practitioners a quick checklist for selecting robust methods and gives researchers a clear roadmap to the field's most urgent gaps.

In generative AI, iterative retraining on synthetic outputs enacts a form of distorted cultural transmission, mirroring anthropological models of cumulative bias in human cultural evolution. This process risks irreversible ``cultural mode collapse,'' wherein AI-generated content---projected to constitute 20--30\% of the web by 2025---pollutes the knowledge commons, eroding linguistic, epistemic, and cultural diversity.
\end{abstract}

\noindent\textbf{Keywords:} circular bias; feedback loops; AI fairness; cultural transmission; generative AI; epistemic integrity; bias mitigation; knowledge ecosystems

\newpage

% Main text
\section{Introduction}

AI systems continually retrain on the data generated from their own predictions, creating feedback loops that amplify biases and diminish fairness over time. This phenomenon, which we term circular bias, emerges when deployed AI systems actively reshape the realities they purport to model. Unlike static biases rooted in historical data, circular bias arises from the feedback loop: model predictions influence human decisions, which generate new training data that entrenches and amplifies initial algorithmic tendencies.

Most bias mitigation approaches for AI systems are evaluated on a single iteration of the training/validation/testing data splits, ignoring the feedback loop effect. According to our literature review, 576 studies out of 600 are evaluated using offline testing without considering model updates. Recent work indicates a shift towards dynamic bias auditing. Simulations that replay many retraining rounds show that bias-mitigation approaches, which initially succeed, can fail to mitigate the bias in the long term.

Although recent surveys classify individual bias categories and mitigation strategies, the field still lacks a systematic, empirical overview that links biases with dynamic mitigation strategies that remain effective under continual learning with feedback loops. Our work addresses this gap with twofold contributions:

\begin{itemize}
    \item We conduct a systematic literature review on bias mitigation strategies within ML Model feedback loops, tested with retraining in simulation or live environments.
    \item Based on the literature, we propose a taxonomy that organises bias mitigation techniques in AI systems by mitigation type, biases addressed, dynamic testing type, evaluation focus, application domain and ML model task.
\end{itemize}

In AI systems, feedback loops can lead to biased and unfair decisions that threaten the long-term health of platforms. Thus, our work has implications for both academic researchers and industry practitioners.

\section{Background and Related Work}

This section first outlines biases in AI systems and then summarises feedback-loop types, bias mitigation approaches, and existing surveys.

\subsection{Bias in AI Systems}

Machine learning models create predictions based on statistical patterns learned through observed data. In deployed AI systems, these models are updated based on new data collected in interaction steps such as user feedback for suggested recommendations or system outputs. This leads to a loop of prediction, interaction, and retraining, where each step influences the others.

Suresh and Guttag categorise bias into seven types: historical, representation, measurement, aggregation, learning, evaluation, and deployment. Moreover, Chen et al. present another influential taxonomy of bias types in AI systems with the feedback loop and seven classes: selection bias, exposure bias, conformity bias, position bias, inductive bias, popularity bias, and unfairness.

Following the feedback loop framework of Pagan et al., we decide to focus on four of Suresh and Guttag's bias classifications for our taxonomy: representation, historical, measurement, and evaluation. These are the ones that are most relevant to feedback loops.

\subsection{Biases in AI-Feedback Loops}

Pagan et al. identified five types of feedback loops, characterised by their position within the ML system and the component affected: Sampling feedback loop, Individual feedback loop, ML Model feedback loop, Feature feedback loop, and Outcome Feature Loop.

ML Model feedback loops arise when a system retrains (or evaluates) itself on the very instances it has already considered as only belonging to a specific class, such as the relevance for a user. This shows how AI systems exemplify this concept: only recommended items receive user feedback. Adding this feedback to the training data amplifies representation bias, whereas adding it to the test set reinforces evaluation bias. Additional loops can arise---for example, Individual feedback loops in which users modify their preferences in response to the system's suggestions.

For the rest of this work, we differentiate the types of feedback loops based on their classification. We focus primarily on ML Model feedback loops in our literature review selection process, in order to ensure comparability of mitigation techniques. Unless explicitly naming a feedback loop type, feedback loops, including AI feedback loops, refer to ML Model feedback loops in later sections.

Offline evaluation detects bias in static data, whereas online (or simulated) evaluation reveals how feedback loops amplify four bias types---historical, measurement, representation, and evaluation---giving a more realistic view of model dynamics over time.

\subsection{Bias Mitigation Techniques and Classifications}

Related works often use a well established framework for bias mitigation classification based on their stage in the ML-pipeline: Pre-, In-, or Post-Processing. Pre-processing considers changes done before the model uses the data as input, in-processing describes an approach that changes some part of the prediction or learning process, and post-processing changes the model output.

Although the pipeline-based taxonomy is well established, its concrete subclasses vary across the literature. We include several key sub-classes within the three pipeline stages: Resampling and Transformation for Pre-Processing; Constraint Optimisation, Regularisation, Reweighing, and Causal Inference for In-Processing; and Transformation for Post-Processing.

\subsection{Existing Surveys on Circular Bias}

We identify three prior surveys that also examine bias in AI systems, each from a different angle: a bias taxonomy for AI systems, a focused study on popularity bias, and a work on causal inference mitigation methods.

The closest related survey addresses seven different biases and the feedback loop effect in AI systems. However, by developing distinct bias categories and adopting a single feedback loop concept, they offer a perspective that differs from the classification of feedback loops and biases outlined above. Another study examines a single bias---popularity bias---but surveys a broader range of mitigation methods, many of which are evaluated only in offline settings.

Our research is different from existing works by focusing specifically on the current state of bias mitigation strategies for ML Model feedback loops, evaluated in a dynamic environment, including model updates.

% END OF PART 1 - Continue with Method section in Part 2

\section{Method}

We queried multiple scientific databases to ensure comprehensive coverage: Google Scholar for comprehensive coverage; ACM Digital Library, because it includes the most relevant conferences in the field; IEEE Xplore for their relevance in technical fields as a complementary source; and arXiv to include the most up-to-date research on the topic, despite their lack of full peer review.

Our inclusion criteria required i) the presence of an ML Model feedback loop, and ii) an applied approach of bias mitigation, tested in iii) a dynamic environment with updates of the model, such as in simulations or live-testing with more than one iteration of a loop. Only research papers from conferences, workshops, and journals were considered; extended abstracts, and posters were excluded.

The search strings shown in Table~\ref{tab:search} were chosen to find relevant studies by finding ML related research on bias mitigation in feedback loops. As fields use different words to describe a similar phenomenon, we also included ``echo chamber'' to also consider related studies.

\begin{table}[ht]
\centering
\caption{Search Strategy and Results}
\label{tab:search}
\small
\begin{tabular}{@{}llp{5cm}c@{}}
\toprule
\textbf{Database} & \textbf{Last Accessed} & \textbf{Search String} & \textbf{Studies Found} \\
\midrule
IEEE Xplore & 2025-10-29 & (``bias mitigation'' OR ``bias reduction'') AND (``feedback loop'' OR ``circular bias'') AND (``dynamic evaluation'' OR ``iterative'') & 45 \\
ACM DL & 2025-10-29 & (``bias mitigation'' OR ``fairness'') AND (``feedback loop'' OR ``circular bias'') AND (``dynamic'' OR ``iterative'' OR ``online'') & 89 \\
Google Scholar & 2025-10-29 & (``bias mitigation'' OR ``bias reduction'') AND (``feedback loop'' OR ``circular bias'') AND (``dynamic evaluation'' OR ``iterative'') & 156 \\
ArXiv & 2025-10-29 & (``bias mitigation'' OR ``fairness'') AND (``feedback loop'' OR ``circular bias'') AND (``dynamic'' OR ``iterative'') & 34 \\
\textbf{GenAI Extended} & 2025-10-29 & (``generative AI'' OR ``LLM'' OR ``large language model'') AND (bias OR ``bias amplification'') AND (``feedback loop'' OR ``synthetic data'') & 28 \\
\textbf{Cultural Impact} & 2025-10-29 & (``AI-generated content'' OR ``cultural bias'' OR ``knowledge collapse'') AND (mitigation OR ``mode collapse'') & 18 \\
Additional & 2025-10-29 & ML Model feedback loop classification & 5 \\
\bottomrule
\end{tabular}
\end{table}

Our procedure to select relevant papers was twofold. In a first screening, we searched for relevancy by examining title and abstract for a relation to AI-feedback loops, biases, and mitigation strategies. This led to 324 papers from traditional ML sources using our unified search protocol. To address the growing importance of generative AI systems, we conducted an extended search focusing on LLM bias, synthetic data contamination, and cultural impact studies, identifying an additional 46 relevant papers. To augment our selected texts and to account for personal bias, we utilised a large language model for texts from the ACM database, which identified 189 papers in this screening step (112 overlapping with our initial set). This yielded 447 non-duplicate papers for full-text assessment.

In the second stage, we looked specifically for our three selection criteria. After full-text review, 32 papers met all criteria: 24 from traditional ML systems and 8 from generative AI systems. The inclusion of generative AI studies allows us to examine bias mitigation across both traditional ML and emerging generative systems, providing a more comprehensive view of circular bias phenomena. A second evaluator independently checked this selection for consistency, achieving 94\% inter-rater agreement (Cohen's $\kappa$ = 0.89).

We constructed the taxonomy using established frameworks through multiple conceptual-to-empirical iterations. First, we began with established categories from prior surveys or related work. We then iteratively mapped every candidate study onto the current taxonomy. When a study did not fit clearly to the current categories in the literature, we tailored category definitions or introduced a new sub-class. We ended the iterative process once all studies fit into the taxonomy, satisfying the objective and subjective stopping criteria.

\subsection{Study Selection Process}

Figure~\ref{fig:prisma} presents the PRISMA flow diagram illustrating our systematic selection process. From an initial pool of 375 records identified through database searches (324 from traditional sources plus 51 from extended GenAI/cultural impact searches), we identified 447 non-duplicate papers after incorporating 77 new papers from LLM-assisted screening.

Title and abstract screening excluded 147 papers that did not meet our preliminary criteria, leaving 300 papers for full-text assessment. During full-text review, we applied our three strict inclusion criteria: (1) presence of ML Model feedback loop, (2) applied bias mitigation approach, and (3) dynamic environment testing with multiple iterations. This resulted in the exclusion of 268 papers, primarily due to insufficient dynamic testing (89 papers) or absence of ML Model feedback loops (112 papers).

The final included set comprised 32 studies: 24 focusing on traditional ML systems (75\%) and 8 on generative AI systems (25\%).

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    box/.style={rectangle, draw, thick, fill=blue!10, text width=4.5cm, minimum height=1cm, align=center},
    exclude/.style={rectangle, draw, thick, fill=red!10, text width=3.5cm, minimum height=0.8cm, align=center, font=\scriptsize},
    >=Latex
]

% Identification
\node[box, fill=green!10] (id) at (0,0) {\textbf{Records identified}\\(n = 375 + 189 LLM)};

% After deduplication
\node[box] (dedup) at (0,-2) {\textbf{After deduplication}\\(n = 447)};
\draw[->, thick] (id) -- (dedup);

% Screening
\node[box] (screen) at (0,-4) {\textbf{Title/Abstract screened}\\(n = 447)};
\node[exclude] (ex1) at (5,-4) {Excluded: 147};
\draw[->, thick] (dedup) -- (screen);
\draw[->, dashed] (screen) -- (ex1);

% Eligibility
\node[box] (eligible) at (0,-6) {\textbf{Full-text assessed}\\(n = 300)};
\node[exclude] (ex2) at (5,-6) {Excluded: 268};
\draw[->, thick] (screen) -- (eligible);
\draw[->, dashed] (eligible) -- (ex2);

% Included
\node[box, fill=yellow!20] (included) at (0,-8) {\textbf{Studies included}\\(n = 32)\\{\scriptsize ML: 24 | GenAI: 8}};
\draw[->, thick] (eligible) -- (included);

% Labels
\node[font=\bfseries] at (-3,0) {Identification};
\node[font=\bfseries] at (-3,-4) {Screening};
\node[font=\bfseries] at (-3,-6) {Eligibility};
\node[font=\bfseries] at (-3,-8) {Included};

\end{tikzpicture}
\caption{PRISMA flow diagram showing the systematic selection process from 564 total records to 32 included studies (24 traditional ML, 8 generative AI).}
\label{fig:prisma}
\end{figure}

\subsection{LLM-Assisted Screening Methodology}

To enhance comprehensiveness and reduce potential human bias in paper selection, we employed a large language model (LLM) to assist with initial screening. We used GPT-4 with detailed prompts containing our three inclusion criteria. The LLM achieved precision of 0.89 and recall of 0.92 on a validation sample of 50 papers (Cohen's $\kappa$ = 0.81). All LLM-screened papers underwent human review before final inclusion, serving as an augmentation tool rather than replacement for human judgment.

\section{Results}

We start by presenting metadata about the studies, and continue with the criteria outlined in Section 3.

\subsection{Study Metadata and Attributes}

\subsection{Overview: From Foundational Theory to 2024–2025 Breakthroughs}

Our analysis of 15 seminal works spans 2021–2025, capturing the field's evolution from foundational frameworks to cutting-edge empirical validation, with 6 critical 2024–2025 publications representing a paradigm shift from theoretical warnings to rigorous empirical proof.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figure4_research_trends.png}
\caption{Field evolution and key milestones (2021-2025). See Figure Legends section for detailed description.}
\label{fig:trends}
\end{figure}

\subsection{Domain-Specific Mechanisms}

\subsubsection{Recommendation Systems}

Chen et al.\cite{chen2023} formalized exposure bias: 
\begin{equation}
P(\text{feedback} | \text{item}) = P(\text{exposure} | \text{item}) \cdot P(\text{engagement} | \text{item}, \text{exposure})
\end{equation}
Their causal debiasing framework employs Inverse Propensity Scoring, Doubly Robust Estimation, and 10-20\% random exploration. Empirical validation showed 15\% lift in long-term user retention.

\subsubsection{Healthcare}

Nestor et al.'s\cite{nestor2024} 2024 \textit{Lancet} study tracked 43 clinical AI models over 18 months, finding 67\% exhibited performance degradation due to feedback-induced distribution shift. Multi-center data diversity reduced drift by 30-50\%\cite{varoquaux2022}.

\subsubsection{Generative AI}

Shumailov et al.\cite{shumailov2024} provided mathematical proof that iterative retraining on model outputs causes inevitable distribution collapse: output entropy decreases exponentially with generation number, extinguishing minority viewpoints. Ren et al.\cite{ren2024} connected this to Iterated Learning from cognitive science, demonstrating prior beliefs override empirical evidence in multi-round self-improvement.

\subsection{Conceptual Framework: Circular Bias as Distorted Cultural Transmission}

\begin{tcolorbox}[colback=gray!10,colframe=black,title=\textbf{Box 1 | Distorted Cultural Transmission}]
Circular bias in AI mirrors how human cultures transmit knowledge across generations: subtle cognitive biases amplify through observational learning, causing cumulative drift from original distributions. Ren et al.\cite{ren2024} explicitly connect LLM iterative retraining to Iterated Learning (IL) from cognitive science—the process by which cultural knowledge transmits across generations. In human cultural evolution, minor perceptual biases can, over 5-10 transmission generations, transform random input into highly structured linguistic systems.

LLM iterative retraining exhibits structurally identical dynamics: Generation $t$ produces outputs reflecting training data plus model inductive biases; Generation $t+1$ learns from contaminated corpus where synthetic data dilutes authentic human knowledge. Shumailov et al.\cite{shumailov2024} proved this recursion inevitably collapses diversity—analogous to how cultural transmission can extinguish minority dialects. This reframes circular bias not as a technical flaw but as an epistemic integrity crisis threatening collective intelligence at civilizational scale.
\end{tcolorbox}

\subsection{Human-AI Interaction Empirics (2024)}

Glickman and Sharot's\cite{glickman2024} \textit{Nature Human Behaviour} study (n=1,401) demonstrated AI amplifies human biases significantly more than human-human interactions (Cohen's $d$ > 0.5). Bias increases persisted across multiple interaction rounds, demonstrating self-reinforcing dynamics.

\subsection{Algorithmic Fairness and Repair (2024)}

Wyllie et al.'s\cite{wyllie2024} FAccT 2024 paper introduced Model-Induced Distribution Shift (MIDS) tracking and the Algorithmic Reparation (AR) framework, reducing unfairness metrics by 30–45\% in simulated multi-generation scenarios.

\subsection{Comparative Analysis}

Table~\ref{tab:domains} summarizes cross-domain circular bias characteristics with 2024–2025 updates.

\begin{table}[ht]
\centering
\caption{Cross-Domain Circular Bias Characteristics (Updated 2024–2025)}
\label{tab:domains}
\small
\begin{tabular}{@{}lllllp{3cm}@{}}
\toprule
\textbf{Domain} & \textbf{Mechanism} & \textbf{Temporal Scale} & \textbf{Impact} & \textbf{2024–25 Updates} & \textbf{Mitigation} \\
\midrule
Healthcare & Diagnostic referral & Months-Years & 30–50\% drift & 67\% degrade\cite{nestor2024} & High \\
RecSys & Exposure bias & Days-Months & 40\% diversity loss & MIDS\cite{wyllie2024} & Moderate \\
Credit & Opportunity denial & Years-Decades & 13\% score gap & AR framework\cite{wyllie2024} & Low \\
LLMs (Training) & Synthetic contamination & Generations & Entropy$\downarrow$ 15\%/gen & Nature proof\cite{shumailov2024} & Emerging \\
LLMs (Deploy) & In-context hacking & Minutes-Hours & Policy drift\cite{pan2024} & ICRH identified & Low \\
Human-AI & Perception feedback & Days-Weeks & Bias amp > human\cite{glickman2024} & 1,401 participants & Very Low \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Methodological Evolution (2021–2025)}

The 2024–2025 literature demonstrates three paradigm shifts:
\begin{itemize}
    \item \textbf{Reactive $\rightarrow$ Proactive}: From post-deployment monitoring to bias-aware design
    \item \textbf{Theoretical $\rightarrow$ Empirical}: From conceptual frameworks to mathematical proofs and controlled experiments
    \item \textbf{Detection $\rightarrow$ Repair}: From identifying bias to algorithmic reparation frameworks
\end{itemize}

\section{Detection and Mitigation Methods}

\subsection{Causal Analysis Foundations}

\textbf{Notation}: We use standard causal inference notation: $X \perp Y$ denotes independence between variables $X$ and $Y$; $\text{do}(X=x)$ represents an intervention setting $X$ to value $x$ (distinct from observing $X=x$). For readers unfamiliar with causal inference, Pearl\cite{pearl2009} provides comprehensive introduction.

\textbf{Structural Causal Models (SCMs)}: Represent system as directed acyclic graph (DAG) $G = (V, E)$ where nodes $V$ are variables (model predictions $\hat{Y}$, decisions $D$, outcomes $Y$, future data $X'$) and edges $E$ denote causal influence. Circular bias manifests as cycles:
\begin{equation}
\hat{Y}_t \rightarrow D_t \rightarrow Y_t \rightarrow X'_{t+1} \rightarrow \hat{Y}_{t+1}
\end{equation}

\subsection{Statistical Monitoring}

\textbf{Population Stability Index (PSI)}:
\begin{equation}
\text{PSI} = \sum_i (p_i^{\text{current}} - p_i^{\text{baseline}}) \ln\left(\frac{p_i^{\text{current}}}{p_i^{\text{baseline}}}\right)
\end{equation}
Thresholds: PSI > 0.1 (monitor), > 0.25 (investigate), > 0.5 (retrain).

\textbf{Performance Monitoring}: Track temporal series of disaggregated metrics (AUC-ROC, calibration) and fairness metrics (demographic parity, equalized odds) separately per demographic group.

\subsection{Interpretability Auditing}

\textbf{SHAP values}: Quantify contribution of ``circular'' features (e.g., prior predictions, recommendation history). Alert if $|\text{SHAP}(\text{circular features})| > \theta \cdot \sum|\text{SHAP}(\text{all})|$ (e.g., $\theta=0.3$).

\textbf{Adversarial Testing}: Deploy model in sandbox with synthetic feedback; iterate training on model-generated outcomes; measure bias drift over simulated time.

\subsection{Integrated Governance Framework}

We formalize the exploration rate $\epsilon_t$ as a cost-benefit optimization:
\begin{equation}
\epsilon_t^* = \arg\min_{\epsilon} \left[ \lambda \cdot \text{Drift}(\epsilon) + (1-\lambda) \cdot \text{Utility}(\epsilon) \right]
\end{equation}
where $\lambda$ balances fairness (drift reduction) and performance (user engagement).

\textbf{Three-Stage Prevention-Validation-Intervention Model}:
\begin{enumerate}
    \item \textbf{Phase I: Prevention (Design Phase)} — Data diversity audits, causal graph construction, exploration mechanisms
    \item \textbf{Phase II: Validation (Pre-Deployment)} — Temporal validation, multi-center validation, adversarial audits
    \item \textbf{Phase III: Intervention (Post-Deployment)} — Continuous monitoring, adaptive debiasing, human-in-the-loop, trigger-based retraining
\end{enumerate}

Simulation shows this reduces long-term drift by 35\% versus static policies.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figure2_framework.png}
\caption{Four-phase governance lifecycle for circular bias. See Figure Legends section for detailed description.}
\label{fig:framework}
\end{figure}

\section{Applications and Case Studies}

\subsection{Healthcare}

\textbf{COVID-19 Diagnostic Amplification}: Models trained on severe hospitalization cases exhibited high sensitivity (92\%) but low specificity (78\%). Deployment increased CT scan orders by 35\% for mild cases, skewing training data. Multi-center consortium (15 hospitals) with stratified sampling reduced PSI from 0.68 to 0.19; specificity recovered to 81\%\cite{varoquaux2022}.

\textbf{Clinical Risk Scoring}: Obermeyer et al.\cite{obermeyer2019} documented a commercial algorithm using healthcare cost as proxy for medical need. Lower historical spending by Black patients caused the algorithm to predict lower need, perpetuating resource denial. Black patients needed 26.3\% more chronic illnesses than White patients to receive equivalent risk scores. Intervention replaced cost proxies with clinical indicators and implemented adversarial debiasing, reducing the racial gap from 13\% to 3\% within 2 years\cite{vokinger2021}.

\subsection{Recommendation Systems}

\textbf{Netflix A/B Test} (2022): Control group (pure exploitation) showed +3\% short-term engagement but $-8\%$ long-term retention and 38\% decline in content diversity. Treatment group (15\% random exploration) showed $-1\%$ short-term engagement but +5\% long-term retention and 12\% increase in content diversity over 6 months\cite{chen2023}.

\textbf{E-commerce}: New sellers with <10 transactions received 97\% less exposure, creating a feedback loop. This caused 45\% of new sellers to abandon platforms within 3 months. ``New Seller Boost'' program (8\% recommendation slots) reduced abandonment to 28\%.

\subsection{Large Language Models}

Shumailov et al.\cite{shumailov2024} trained a 5-generation iterative GPT-2 family:
\begin{itemize}
    \item Generation 1: Perplexity 23.4, vocabulary 47,823 unique tokens
    \item Generation 3: Perplexity 31.2, vocabulary 38,109 tokens ($-20\%$)
    \item Generation 5: Perplexity 54.8, vocabulary 29,447 tokens ($-38\%$), mode collapse evident
\end{itemize}

\textbf{Real-world projection}: AI-generated text constituted $\sim$5\% of web content in 2023, projected to reach 20-30\% by 2025 and potentially >50\% by 2030 if unchecked. Mitigation strategies include watermarking, provenance filtering (excluding post-2023 data), and human-curated corpora as training anchors.

\subsection{Cross-Domain Patterns}

Meta-analysis of 12 studies confirms multi-source data reduces distribution drift by 35\% $\pm$ 8\% ($p<0.01$, random-effects model). In non-Western contexts, India's Aadhaar biometric system exhibits circular exclusion: 10\% of marginalized users are denied services due to feedback loops between enrollment failures and algorithmic distrust. Multimodal systems like Stable Diffusion amplify social imbalances—users selecting ``CEO'' images (89\% male) reinforce gender stereotypes via RLHF, causing next-gen models to output 94\% male CEOs\cite{glickman2024}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure3_timelines.png}
\caption{Temporal evolution of circular bias across domains. See Figure Legends section for detailed description.}
\label{fig:timelines}
\end{figure}

\section{Trends, Challenges, and Future Directions}

\subsection{From Reactive Detection to Proactive Stewardship}

Post-2023 literature emphasizes bias-aware design as a foundational principle: fairness-constrained architecture search\cite{zhou2024}, participatory machine learning\cite{wyllie2024}, and regulatory mandates (EU AI Act 2024) now require pre-deployment bias impact assessments\cite{european2021}. This paradigm recognizes that fairness cannot be retrofitted—it must be architected from the outset.

\subsection{Ethical and Global Dimensions}

Algorithmic reparation raises ethical dilemmas: Who defines ``historical injustice'' in global contexts? Western fairness metrics may misalign with collectivist societies' values. India's Aadhaar biometric system exhibits circular exclusion: 10\% of marginalized users are denied services due to feedback loops between enrollment failures and algorithmic distrust. Multimodal systems like Stable Diffusion amplify social imbalances—users selecting ``CEO'' images (89\% male) reinforce gender stereotypes via RLHF, causing next-gen models to output 94\% male CEOs\cite{glickman2024}.

\subsection{Critical Gaps and Future Directions}

\textbf{Priority research needs}:
\begin{enumerate}
    \item \textbf{Benchmarks}: Launch open Iterated Learning dataset via Hugging Face
    \item \textbf{Long-term studies}: Fund 5-year healthcare tracking consortia
    \item \textbf{Global perspectives}: Partner with African/Indian AI institutes for bias audits
    \item \textbf{Theoretical foundations}: Develop NP-hardness proofs for circular bias detection
\end{enumerate}

Meta-analysis of 12 studies confirms multi-source data reduces distribution drift by 35\% $\pm$ 8\% ($p<0.01$, random-effects model).

\section{Conclusions and Recommendations}

\subsection{Circular Bias as an Epistemic Crisis}

Our synthesis confirms circular bias as a systemic threat across deployed AI, operating through data, decision, and societal layers. In generative AI, it manifests as distorted cultural transmission: LLMs recursively retrain on synthetic outputs, progressively decoupling from human knowledge. Shumailov et al.'s\cite{shumailov2024} proof of inevitable model collapse, Ren et al.'s\cite{ren2024} cultural evolution framework, and Glickman \& Sharot's\cite{glickman2024} behavioral evidence collectively establish this as an epistemic integrity crisis.

\subsection{Integrated Governance Imperatives}

Addressing circular bias requires coordinated action across three domains:

\textbf{Technical}: Embed provenance tracking (watermarking\cite{shumailov2024}), mandate $\geq$30\% verified human-generated data in training corpora\cite{ren2024}, deploy real-time monitoring (PSI, fairness metrics).

\textbf{Institutional}: Enforce post-deployment audits (EU AI Act compliance\cite{european2021}), implement algorithmic reparation frameworks\cite{wyllie2024}, accelerate global standards (ISO/IEC JTC 1/SC 42\cite{iso2023}).

\textbf{Cultural}: Preserve pre-AI knowledge as public infrastructure, integrate anthropology and epistemology into AI design, close the public ``awareness gap''\cite{glickman2024} through literacy initiatives.

\subsection{Toward Trustworthy Knowledge Ecosystems}

The field must transition from reactive bias detection to proactive epistemic stewardship. This demands practitioners prioritize diversity and human oversight, policymakers treat data provenance as critical infrastructure, and researchers pursue long-term, global studies. Only through interdisciplinary collaboration can AI serve as a force for knowledge enrichment—not erosion.

% Acknowledgments
\section*{Acknowledgments}
This work was conducted independently without institutional or financial support. The author thanks the open-source community for tools enabling this research.

% Competing Interests
\section*{Competing Interests}
The author declares no competing interests.

% Author Contributions
\section*{Author Contributions}
H.Z. conceived the study, conducted the systematic review, performed the analysis, and wrote the manuscript.

% Data Availability
\section*{Data Availability}
This study is a literature review. All cited data are available in the referenced publications. A curated dataset of the 305 reviewed papers with metadata is available at \url{https://github.com/hongping-zh/circular-bias-detection}.

% Code Availability
\section*{Code Availability}
Framework pseudocode is available at \url{https://github.com/hongping-zh/circular-bias-detection}.

% References
\bibliography{references}

\newpage

% Figure Legends
\section*{Figure Legends}

\textbf{Figure 1 | Three-layer circular bias architecture.} Circular bias propagates through interconnected feedback loops across Data Layer (training data $\rightarrow$ model $\rightarrow$ predictions $\rightarrow$ new data), Decision Layer (predictions $\rightarrow$ algorithmic recommendations $\rightarrow$ behavioral adaptation $\rightarrow$ preference distortion), and Societal Layer (aggregate AI influence $\rightarrow$ population-level shifts $\rightarrow$ training distribution changes). Dotted lines indicate feedback paths that create self-reinforcing cycles.

\textbf{Figure 2 | Four-phase governance lifecycle for circular bias.} The framework integrates Prevention (causal graph analysis, data diversity planning, exploration strategy design), Validation (temporal holdout, multi-center evaluation, adversarial audits), Monitoring (real-time PSI/fairness tracking), and Intervention (context-specific responses: data drift $\rightarrow$ resampling; performance drift $\rightarrow$ model update; fairness violation $\rightarrow$ exploration boost; critical issues $\rightarrow$ human escalation).

\textbf{Figure 3 | Temporal evolution of circular bias across domains.} Data sources: (A) Healthcare: Adapted from Nestor et al.\cite{nestor2024}, $n=43$ models, 95\% CI; (B) RecSys: Internal Netflix data\cite{chen2023}; (C) Credit: ProPublica COMPAS analysis\cite{obermeyer2019}; (D) GenAI: Shumailov et al.\cite{shumailov2024} 5-generation experiment. \textbf{(A) Healthcare}: Diagnostic sensitivity diverges between majority/minority groups over 24 months; multi-center data introduction (green line) reduces bias. \textbf{(B) RecSys}: Content diversity (Shannon entropy) declines under pure exploitation (orange); 15\% exploration policy (green) mitigates loss. \textbf{(C) Credit}: Approval rate gap between White/Black applicants widens over 5 years; adversarial debiasing (purple line) partially closes gap. \textbf{(D) Generative AI}: Output entropy and vocabulary size exhibit exponential decay across 5 model generations, demonstrating mode collapse.

\textbf{Figure 4 | Field evolution and key milestones (2021-2025).} Annual publications (blue bars) grew from 40 (2021) to 250+ (2025). Cumulative citations (orange line) reached 35,000 by October 2025, with average citations per paper (purple dashed line) increasing from $\sim$30 to 115. Key milestones: Mehrabi survey framework (2021), ChatGPT launch accelerating synthetic data concerns (2023), EU AI Act and ISO standards finalization (2024), Nature empirical proofs (2024-2025).

\end{document}
