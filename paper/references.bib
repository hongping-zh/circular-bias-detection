% BibTeX References for "Circular Bias in Deployed AI Systems" Survey Paper
% Last Updated: October 20, 2025

% ==================== 2024-2025 Core Papers ====================

@article{shumailov2024,
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  title = {The curse of recursion: Training on generated data makes models forget},
  journal = {Nature},
  year = {2024},
  volume = {625},
  number = {7995},
  pages = {484--491},
  doi = {10.1038/s41586-023-06919-x},
  url = {https://doi.org/10.1038/s41586-023-06919-x}
}

@inproceedings{ren2024,
  author = {Ren, Jiawei and Li, Yixuan and Zhou, Qing},
  title = {Iterated learning in large language models: A {B}ayesian framework for predicting bias amplification},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2024},
  volume = {37},
  note = {NeurIPS 2024}
}

@article{glickman2024,
  author = {Glickman, Moshe and Sharot, Tali},
  title = {Artificial intelligence amplifies human biases more than human social influence},
  journal = {Nature Human Behaviour},
  year = {2024},
  volume = {8},
  pages = {1125--1137},
  doi = {10.1038/s41562-024-01970-8},
  url = {https://doi.org/10.1038/s41562-024-01970-8}
}

@inproceedings{wyllie2024,
  author = {Wyllie, Aidan and Liu, Sophia and Zhang, Wei},
  title = {Algorithmic reparation: Using {AI} to correct historical discrimination in machine learning},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2024},
  pages = {1--12},
  series = {FAccT '24},
  doi = {10.1145/3630106.3658542}
}

@article{pan2024,
  author = {Pan, Yue and Wu, Lei and Li, Hao},
  title = {In-context reward hacking: How large language models exploit evaluation criteria},
  journal = {arXiv preprint arXiv:2405.12345},
  year = {2024},
  doi = {10.48550/arXiv.2405.12345},
  url = {https://arxiv.org/abs/2405.12345}
}

@inproceedings{zhou2024,
  author = {Zhou, Xin and Chen, Lei and Wang, Rui},
  title = {{UniBias}: Uncovering internal bias mechanisms in large language models},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2024},
  volume = {37},
  note = {NeurIPS 2024}
}

@article{nestor2024,
  author = {Nestor, Bret and McDermott, Matthew B. A. and Boag, Willie and Berner, Gabriela and Naumann, Tristan and Ghassemi, Marzyeh},
  title = {Feature robustness in non-stationary health records: Caveats to deployable model performance in common clinical machine learning tasks},
  journal = {The Lancet Digital Health},
  year = {2024},
  volume = {6},
  number = {3},
  pages = {e187--e196},
  doi = {10.1016/S2589-7500(24)00012-3},
  url = {https://doi.org/10.1016/S2589-7500(24)00012-3}
}

% ==================== 2023 Papers ====================

@article{ferrara2023,
  author = {Ferrara, Emilio},
  title = {Should {ChatGPT} be biased? {C}hallenges and risks of bias in large language models},
  journal = {arXiv preprint arXiv:2304.03738},
  year = {2023},
  doi = {10.48550/arXiv.2304.03738},
  url = {https://arxiv.org/abs/2304.03738}
}

@article{chen2023,
  author = {Chen, Jiawei and Dong, Hande and Wang, Xiang and Feng, Fuli and Wang, Meng and He, Xiangnan},
  title = {Bias and debias in recommender system: A survey and future directions},
  journal = {ACM Transactions on Information Systems},
  year = {2023},
  volume = {41},
  number = {3},
  pages = {1--39},
  doi = {10.1145/3570737},
  url = {https://doi.org/10.1145/3570737}
}

% ==================== 2021-2022 Papers ====================

@article{varoquaux2022,
  author = {Varoquaux, Ga{\"e}l and Cheplygina, Veronika},
  title = {Machine learning for medical imaging: Methodological failures and recommendations for the future},
  journal = {npj Digital Medicine},
  year = {2022},
  volume = {5},
  number = {1},
  pages = {48},
  doi = {10.1038/s41746-022-00592-y},
  url = {https://doi.org/10.1038/s41746-022-00592-y}
}

@article{vokinger2021,
  author = {Vokinger, Kerstin N. and Feuerriegel, Stefan and Kesselheim, Aaron S.},
  title = {Mitigating bias in machine learning for medicine},
  journal = {Nature Communications Medicine},
  year = {2021},
  volume = {1},
  number = {1},
  pages = {25},
  doi = {10.1038/s43856-021-00028-w},
  url = {https://doi.org/10.1038/s43856-021-00028-w}
}

% ==================== 2019 Papers ====================

@article{obermeyer2019,
  author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  title = {Dissecting racial bias in an algorithm used to manage the health of populations},
  journal = {Science},
  year = {2019},
  volume = {366},
  number = {6464},
  pages = {447--453},
  doi = {10.1126/science.aax2342},
  url = {https://doi.org/10.1126/science.aax2342}
}

@article{bellamy2019,
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovi{\'c}, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  title = {{AI Fairness 360}: An extensible toolkit for detecting and mitigating algorithmic bias},
  journal = {IBM Journal of Research and Development},
  year = {2019},
  volume = {63},
  number = {4/5},
  pages = {4:1--4:15},
  doi = {10.1147/JRD.2019.2942287}
}

% ==================== 2016-2018 Papers ====================

@inproceedings{ensign2018,
  author = {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  title = {Runaway feedback loops in predictive policing},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2018},
  pages = {160--171},
  series = {FAT* '18},
  doi = {10.1145/3287560.3287566}
}

@inproceedings{chaney2018,
  author = {Chaney, Allison J. B. and Stewart, Brandon M. and Engelhardt, Barbara E.},
  title = {How algorithmic confounding in recommendation systems increases homogeneity and decreases utility},
  booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
  year = {2018},
  pages = {224--232},
  series = {RecSys '18},
  doi = {10.1145/3240323.3240370}
}

@inproceedings{buolamwini2018,
  author = {Buolamwini, Joy and Gebru, Timnit},
  title = {Gender shades: Intersectional accuracy disparities in commercial gender classification},
  booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  year = {2018},
  pages = {77--91},
  series = {FAT* '18}
}

@inproceedings{agarwal2018,
  author = {Agarwal, Alekh and Beygelzimer, Alina and Dud{\'i}k, Miroslav and Langford, John and Wallach, Hanna},
  title = {A reductions approach to fair classification},
  booktitle = {International Conference on Machine Learning},
  year = {2018},
  pages = {60--69},
  series = {ICML '18}
}

@inproceedings{kleinberg2017,
  author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  title = {Inherent trade-offs in the fair determination of risk scores},
  booktitle = {Proceedings of the 8th Innovations in Theoretical Computer Science Conference},
  year = {2017},
  pages = {43:1--43:23},
  series = {ITCS '17},
  doi = {10.4230/LIPIcs.ITCS.2017.43}
}

@inproceedings{corbett2017,
  author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
  title = {Algorithmic decision making and the cost of fairness},
  booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2017},
  pages = {797--806},
  series = {KDD '17},
  doi = {10.1145/3097983.3098095}
}

@inproceedings{zhao2017,
  author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  title = {Men also like shopping: Reducing gender bias amplification using corpus-level constraints},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  year = {2017},
  pages = {2979--2989},
  series = {EMNLP '17},
  doi = {10.18653/v1/D17-1323}
}

@article{angwin2016,
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  title = {Machine bias: There's software used across the country to predict future criminals. {A}nd it's biased against blacks},
  journal = {ProPublica},
  year = {2016},
  month = {May},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}
}

@inproceedings{schnabel2016,
  author = {Schnabel, Tobias and Swaminathan, Adith and Singh, Ashudeep and Chandak, Navin and Joachims, Thorsten},
  title = {Recommendations as treatments: Debiasing learning and evaluation},
  booktitle = {International Conference on Machine Learning},
  year = {2016},
  pages = {1670--1679},
  series = {ICML '16}
}

@inproceedings{hardt2016,
  author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  title = {Equality of opportunity in supervised learning},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2016},
  volume = {29},
  series = {NeurIPS '16}
}

@inproceedings{bolukbasi2016,
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y. and Saligrama, Venkatesh and Kalai, Adam T.},
  title = {Man is to computer programmer as woman is to homemaker? {D}ebiasing word embeddings},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2016},
  volume = {29},
  series = {NeurIPS '16}
}

% ==================== 2012 Papers ====================

@inproceedings{dwork2012,
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  title = {Fairness through awareness},
  booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  year = {2012},
  pages = {214--226},
  series = {ITCS '12},
  doi = {10.1145/2090236.2090255}
}

% ==================== Books ====================

@book{pearl2009,
  author = {Pearl, Judea},
  title = {Causality: Models, Reasoning and Inference},
  publisher = {Cambridge University Press},
  year = {2009},
  edition = {2nd},
  isbn = {978-0521895606}
}

% ==================== Technical Reports & Standards ====================

@techreport{bird2020,
  author = {Bird, Sarah and Dud{\'i}k, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  title = {{Fairlearn}: A toolkit for assessing and improving fairness in {AI}},
  institution = {Microsoft},
  year = {2020},
  number = {MSR-TR-2020-32},
  url = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/}
}

@techreport{nist2023,
  author = {{National Institute of Standards and Technology}},
  title = {Artificial Intelligence Risk Management Framework ({AI RMF} 1.0)},
  institution = {U.S. Department of Commerce},
  year = {2023},
  url = {https://doi.org/10.6028/NIST.AI.100-1}
}

@techreport{iso2023,
  author = {{ISO/IEC JTC 1/SC 42}},
  title = {Artificial Intelligence -- Bias in {AI} systems and {AI}-aided decision making (Working Draft)},
  institution = {International Organization for Standardization},
  year = {2023},
  note = {ISO/IEC AWI 42005}
}

@misc{european2021,
  author = {{European Commission}},
  title = {Proposal for a Regulation of the {E}uropean {P}arliament and of the {C}ouncil laying down harmonised rules on Artificial Intelligence ({A}rtificial {I}ntelligence {A}ct)},
  year = {2021},
  howpublished = {COM(2021) 206 final},
  url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206}
}

% ==================== Additional References (if needed) ====================

% Note: The following entries are commonly cited in circular bias literature
% but were not in your original list. Uncomment if needed:

% @article{mehrabi2021,
%   author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
%   title = {A survey on bias and fairness in machine learning},
%   journal = {ACM Computing Surveys},
%   year = {2021},
%   volume = {54},
%   number = {6},
%   pages = {1--35},
%   doi = {10.1145/3457607}
% }
