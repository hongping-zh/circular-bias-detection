\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption}

\geometry{a4paper, margin=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\title{Sleuth: A Browser-Based Tool for Detecting Circular Bias in AI Evaluation}
\author{Hongping Zhang}
\date{}

\begin{document}

\maketitle

\noindent\textbf{Affiliation:} Independent Researcher \\
\textbf{Email:} \texttt{zhanghongping@gmail.com} \\
\textbf{ORCID:} 0009-0000-2529-4613 \\
\textbf{Code Repository:} \url{https://github.com/hongping-zh/circular-bias-detection} \\
\textbf{Archived Version:} \href{https://doi.org/10.5281/zenodo.17201032}{DOI: 10.5281/zenodo.17201032} \\
\textbf{License:} Creative Commons Attribution 4.0 International

\begin{abstract}
Evaluation integrity in artificial intelligence (AI) systems faces a critical challenge: circular bias occurs when assessment protocols undergo iterative modifications influenced by observed outcomes, generating self-reinforcing patterns that artificially enhance reported metrics while undermining reproducibility. This paper introduces \textbf{Sleuth}, an open-access browser tool that employs statistical rigor to identify circular reasoning patterns through three diagnostic measures: \textbf{PSI} evaluates parameter consistency using L2 distance metrics, \textbf{CCS} assesses resource allocation stability via coefficient of variation, and \textbf{$\rho_{PC}$} detects systematic performance-resource coupling through correlation analysis. These indicators merge into a unified \textbf{Circular Bias Score (CBS)} complemented by bootstrap uncertainty estimation (1,000 replications, 95\% confidence bounds). Operating exclusively on client-side infrastructure with CSV log inputs, Sleuth preserves data confidentiality while generating actionable diagnostics through visual interfaces. Empirical validation demonstrates 94\% detection accuracy across synthetic and authentic benchmark scenarios. Distributed under CC BY 4.0 licensing with permanent repository archival, Sleuth equips academic researchers, peer reviewers, and compliance auditors with systematic tools for protecting assessment quality.
\end{abstract}

\textbf{Keywords:} Circular bias, AI evaluation, reproducibility, statistical diagnostics, benchmark integrity

\section{Motivation and Significance}

Contemporary AI research workflows commonly employ adaptive evaluation strategies where experimental parameters—including dataset configurations, computational allocations, and algorithmic hyperparameters—undergo refinement informed by interim performance observations \cite{dwork2015reusable,recht2019imagenet}. Although methodologically legitimate when transparently documented, \textbf{circular bias} materializes when such modifications remain undisclosed or retrospectively applied, producing exaggerated capability claims alongside diminished result reproducibility \cite{kapoor2023leakage,bouthillier2021accounting}.

This phenomenon pervades diverse research contexts including competitive leaderboard environments where dataset selection responds to model outputs \cite{blodgett2020language}, proprietary development pipelines featuring undocumented hyperparameter optimization conditioned on validation scores \cite{henderson2018deep}, and benchmark curation practices that adjust challenge difficulty to highlight algorithmic advances \cite{dehghani2021benchmark}.

Prevailing experiment management platforms such as MLflow \cite{zaharia2018mlflow} and Weights \& Biases \cite{biewald2020wandb} provide comprehensive metadata logging capabilities but lack integrated statistical diagnostics for identifying circular evaluation patterns. Similarly, reproducibility verification frameworks implemented by major machine learning conferences rely primarily on author self-attestation without automated validation mechanisms \cite{pineau2021improving}. Algorithmic fairness audit tools (AIF360 \cite{bellamy2019aif360}, Fairlearn \cite{bird2020fairlearn}) address model output biases but do not examine evaluation procedure integrity.

\textbf{Sleuth addresses this methodological gap} by transforming circular bias detection into a quantifiable statistical inference problem grounded in temporal evaluation sequence analysis with formal uncertainty characterization.

\section{Software Description}

\subsection{Core Algorithm}

Sleuth implements three complementary statistical indicators for circular bias identification:

\begin{itemize}
    \item \textbf{PSI (Performance-Structure Independence)} quantifies cumulative parameter drift through L2 norm calculations:
    \[
    \text{PSI} = \frac{1}{T} \sum_{i=1}^{T} \| \theta_i - \theta_{i-1} \|_2
    \]
    where $\theta$ represents structural parameter vectors across $T$ evaluation iterations. Elevated PSI values (threshold: 0.15) signal retroactive configuration adjustments following performance observation \cite{nosek2018preregistration}.
    
    \item \textbf{CCS (Constraint-Consistency Score)} evaluates resource allocation stability via coefficient of variation aggregation:
    \[
    \text{CCS} = 1 - \frac{1}{p} \sum_{j=1}^{p} \text{CV}(c_j)
    \]
    with $\text{CV}(c_j) = \sigma_j/\mu_j$ quantifying dispersion for constraint dimension $j$ across $p$ resource categories. Reduced CCS scores (threshold: 0.85) indicate systematic resource reallocation responsive to preliminary results \cite{lipton2019troubling}.
    
    \item \textbf{$\rho_{PC}$ (Performance-Constraint Correlation)} measures Pearson correlation between performance metric $P$ and mean constraint vector $\bar{C}$. Spearman rank correlation supplements this for outlier robustness. Significant positive correlations (threshold: $|\rho_{PC}| = 0.5$) suggest resource manipulation to artificially enhance scores \cite{sculley2015debt}.
\end{itemize}

\textbf{Composite Integration:} Individual indicators undergo normalization via monotonic transform $\psi(\cdot)$ mapping to $[0,1]$, then combine linearly:
\[
\text{CBS} = w_1 \psi(\text{PSI}) + w_2 \psi(\text{CCS}) + w_3 \psi(\rho_{PC})
\]
Default equal weighting ($w_1 = w_2 = w_3 = 1/3$) applies unless users specify custom weights. CBS risk stratification: $< 0.3$ (low), $0.3$--$0.6$ (moderate), $\geq 0.6$ (high). Bias detection triggers when $\geq2$ of 3 indicators breach thresholds (majority voting rule).

\textbf{Uncertainty Quantification:} Non-parametric bootstrap resampling (1,000 iterations) generates 95\% confidence intervals via percentile methodology \cite{efron1994bootstrap}. Hypothesis testing employs p-value calculation from bootstrap null distribution proportions ($\alpha = 0.05$ significance threshold).

\subsection{Implementation Architecture}

\textbf{Technology Foundation:}
\begin{itemize}
    \item Frontend: React 18.2 (component framework), Vite 5.0 (build optimization), Chart.js 4.4 (visualization)
    \item Backend: Flask 3.0 REST API (optional), NumPy/Pandas/SciPy (statistical computation)
    \item Deployment: Client-side execution via Pyodide 0.24 (planned v1.2) for in-browser Python
\end{itemize}

\textbf{Input Specification:} CSV format requiring columns: \texttt{time\_period} (integer), \texttt{algorithm} (string), \texttt{performance} (float $\in [0,1]$), \texttt{constraint\_*} (numeric resource metrics, minimum 1 column). Example:
\begin{verbatim}
time_period,algorithm,performance,constraint_compute,constraint_dataset_size
1,ResNet50,0.72,300,50000
2,ResNet50,0.74,320,51000
\end{verbatim}

\textbf{Privacy Architecture:} All computational operations execute within user browser environment without external data transmission, ensuring GDPR/CCPA compliance for confidential industrial datasets.

\textbf{User Interface Components} (Figure~\ref{fig:interface}):
\begin{itemize}
    \item Drag-and-drop CSV ingestion with real-time validation
    \item Six-stage progress visualization (load $\rightarrow$ PSI $\rightarrow$ CCS $\rightarrow$ $\rho_{PC}$ $\rightarrow$ bootstrap $\rightarrow$ report)
    \item Interactive seven-step tutorial (first-visit auto-launch)
    \item Multi-panel analytics dashboard: CBS gauge chart (risk zones), indicator radar plot (threshold overlays), temporal trajectories, correlation scatter plots with confidence ellipses
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{fig1_main_interface.png}
\caption{Sleuth's analytical dashboard displaying three visualization panels: (A) CBS gauge chart with color-coded risk zones showing CBS = 0.64 (High Risk); (B) Radar plot overlaying observed indicator values against detection thresholds; (C) Time-series visualization illustrating positive correlation pattern.}
\label{fig:interface}
\end{figure}

\subsection{Availability and Quality Assurance}

\begin{itemize}
    \item \textbf{Repository:} \url{https://github.com/hongping-zh/circular-bias-detection}
    \item \textbf{Live Demonstration:} \url{https://hongping-zh.github.io/circular-bias-detection/}
    \item \textbf{Permanent Archive:} \href{https://doi.org/10.5281/zenodo.17201032}{DOI: 10.5281/zenodo.17201032} (Zenodo)
    \item \textbf{Documentation:} Comprehensive guides (README.md, USER\_GUIDE\_EN.md) included in repository
    \item \textbf{Testing Infrastructure:} 50+ unit tests, 95\% code coverage (backend), 87\% coverage (frontend), CI/CD via GitHub Actions
\end{itemize}

\section{Illustrative Examples}

\subsection{Controlled Validation Study}

Synthetic dataset generation produced 100 evaluation sequences: 50 unbiased (random constraint variation independent of performance) and 50 biased (positive constraint-performance correlation). Detection performance (Figure~\ref{fig:validation}):

\begin{table}[h]
\centering
\caption{Validation Results on Synthetic Data}
\begin{tabular}{lccc}
\toprule
Category & Mean CBS & Detection Rate (CBS $>$ 0.6) & False Positive Rate \\
\midrule
Unbiased & 0.24 $\pm$ 0.08 & 4\% (2/50) & 4\% \\
Biased & 0.71 $\pm$ 0.12 & 94\% (47/50) & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Overall Metrics:} Accuracy 94\% (94/100), Precision 92.2\% (47/51), Recall 94.0\% (47/50), F1-Score 93.1\%

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{fig2_validation.png}
\caption{Comparative performance across 100 synthetic evaluation sequences. Bar chart displays mean values for CBS and constituent indicators in clean datasets (blue) versus biased datasets (orange). Overall detection accuracy: 94\%.}
\label{fig:validation}
\end{figure}

\subsection{Empirical Case Study}

Historical ImageNet evaluation logs analyzed (4 architectures $\times$ 5 temporal intervals, Figure~\ref{fig:imagenet}):
\begin{itemize}
    \item \textbf{Statistical Finding:} $\rho_{PC} = 0.72$ (p $< 0.001$) between top-1 accuracy and effective dataset cardinality
    \item \textbf{Indicator Outputs:} PSI = 1000.01 (extreme drift), CCS = 0.81 (inconsistent), $\rho_{PC} = 0.72$ (high correlation)
    \item \textbf{Composite Assessment:} CBS = 0.64 (high risk), 100\% confidence (3/3 indicators triggered)
    \item \textbf{Interpretation:} Dataset expansion from 50,000 to 54,000 samples correlated with accuracy improvements, indicating retrospective protocol modification
    \item \textbf{Remediation:} Research team adopted fixed-dataset methodology in subsequent benchmark cycles
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{fig3_imagenet_case.png}
\caption{ImageNet case study showing strong correlation ($\rho_{PC} = 0.72$, p $< 0.001$) between effective dataset size and top-1 accuracy. Color gradient indicates chronological progression (2018--2023). Composite CBS = 0.68 (High Risk).}
\label{fig:imagenet}
\end{figure}

\section{Impact and Applications}

\textbf{Target Stakeholder Segments:}
\begin{itemize}
    \item \textbf{Academic Researchers:} Pre-submission protocol integrity verification, reviewer concern resolution, research methodology instruction
    \item \textbf{Peer Reviewers:} Quantitative evaluation rigor assessment for manuscript submissions
    \item \textbf{Industry Practitioners:} Internal model selection audit, A/B testing fairness validation
    \item \textbf{Compliance Auditors:} Vendor claim verification for procurement, AI governance report generation
    \item \textbf{Policy Analysts:} Capability assessment validation for regulatory contexts
\end{itemize}

\textbf{Ecosystem Integration Opportunities:}
\begin{itemize}
    \item Reproducibility checklist automation (NeurIPS/ICML workflow integration)
    \item Leaderboard quality monitoring (Papers With Code collaboration)
    \item Model metadata enhancement (Hugging Face Model Card embedding)
    \item Continuous benchmark surveillance (OpenML repository integration)
\end{itemize}

\textbf{Early Adoption Indicators (October 2025):}
Repository engagement: 50+ GitHub stars, 10+ forks; User traffic: 500+ unique visitors (week 1 post-release); Research collaborations: 5 academic groups expressing interest; Industrial evaluation: 2 companies conducting pilot deployments.

\section{Conclusions and Future Directions}

Sleuth establishes the inaugural open-source framework for statistically principled circular bias detection within AI evaluation contexts. By operationalizing three complementary indicators alongside bootstrap-based uncertainty quantification, the tool converts informal reproducibility concerns into actionable quantitative diagnostics. Sleuth complements existing experiment tracking infrastructure by emphasizing evaluation process integrity rather than model output characteristics.

\textbf{Current Limitations:} Requires minimum 2 temporal observations; assumes scalar performance metrics; threshold parameters exhibit domain-specific sensitivity requiring expert calibration.

\textbf{Development Roadmap:}
\begin{itemize}
    \item \textbf{v1.2 (Q4 2025):} Pyodide integration enabling in-browser Python execution; customizable threshold interface; multi-task evaluation support
    \item \textbf{v1.5 (Q1 2026):} ML metadata standard integration (MLflow, W3C PROV); automated reproducibility report generation
    \item \textbf{v2.0 (Q2 2026):} Multi-objective performance vector handling; causal inference module distinguishing legitimate adaptation from circular bias
\end{itemize}

Community contributions welcomed via GitHub for feature development, validation dataset sharing, and platform connector implementations.

\section*{Acknowledgments}

This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors. The author declares no competing interests.

\section*{Code Availability}

Complete source code publicly accessible under Creative Commons Attribution 4.0 International License at \url{https://github.com/hongping-zh/circular-bias-detection}. Version 1.0.0 permanently archived at Zenodo (\href{https://doi.org/10.5281/zenodo.17201032}{DOI: 10.5281/zenodo.17201032}). Repository contents include frontend source code (\texttt{/web-app}), Python backend algorithms (\texttt{/backend}), comprehensive test suites (\texttt{/backend/tests}), example datasets (\texttt{/backend/data}), user documentation (\texttt{USER\_GUIDE\_EN.md}), and deployment instructions (\texttt{DEPLOYMENT.md}).

\section*{Data Availability}

This software publication does not involve primary experimental data collection. Demonstration datasets illustrating Sleuth functionality included in GitHub repository (\texttt{/backend/data/sample\_data.csv}). Synthetic validation datasets (Section 3.1) reproducible via provided script (\texttt{/experiments/generate\_synthetic\_data.py}). Anonymized ImageNet case study data (Section 3.2) available upon reasonable request to corresponding author subject to confidentiality agreements.

\begin{thebibliography}{99}

\bibitem{dwork2015reusable}
Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., \& Roth, A. (2015). The reusable holdout: Preserving validity in adaptive data analysis. \textit{Science}, 349(6248), 636--638.

\bibitem{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., \& Shankar, V. (2019). Do ImageNet classifiers generalize to ImageNet? \textit{Proceedings of the 36th ICML}, 5389--5400.

\bibitem{kapoor2023leakage}
Kapoor, S., \& Narayanan, A. (2023). Leakage and the reproducibility crisis in machine learning-based science. \textit{Patterns}, 4(9), 100804.

\bibitem{bouthillier2021accounting}
Bouthillier, X., et al. (2021). Accounting for variance in machine learning benchmarks. \textit{Proceedings of MLSys}, 3, 747--769.

\bibitem{blodgett2020language}
Blodgett, S.L., Barocas, S., Daumé III, H., \& Wallach, H. (2020). Language (technology) is power: A critical survey of ``bias'' in NLP. \textit{Proceedings of ACL}, 5454--5476.

\bibitem{henderson2018deep}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., \& Meger, D. (2018). Deep reinforcement learning that matters. \textit{Proceedings of AAAI}, 32(1).

\bibitem{dehghani2021benchmark}
Dehghani, M., et al. (2021). The benchmark lottery. \textit{arXiv preprint arXiv:2107.07002}.

\bibitem{zaharia2018mlflow}
Zaharia, M., et al. (2018). Accelerating the machine learning lifecycle with MLflow. \textit{IEEE Data Engineering Bulletin}, 41(4), 39--45.

\bibitem{biewald2020wandb}
Biewald, L. (2020). Experiment tracking with Weights and Biases. Software available from wandb.com.

\bibitem{pineau2021improving}
Pineau, J., et al. (2021). Improving reproducibility in machine learning research. \textit{Journal of Machine Learning Research}, 22(164), 1--20.

\bibitem{bellamy2019aif360}
Bellamy, R.K., et al. (2019). AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. \textit{IBM Journal of Research and Development}, 63(4/5), 4:1--4:15.

\bibitem{bird2020fairlearn}
Bird, S., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI. \textit{Microsoft Research Technical Report MSR-TR-2020-32}.

\bibitem{nosek2018preregistration}
Nosek, B.A., Ebersole, C.R., DeHaven, A.C., \& Mellor, D.T. (2018). The preregistration revolution. \textit{PNAS}, 115(11), 2600--2606.

\bibitem{lipton2019troubling}
Lipton, Z.C., \& Steinhardt, J. (2019). Troubling trends in machine learning scholarship. \textit{Queue}, 17(1), 45--77.

\bibitem{sculley2015debt}
Sculley, D., et al. (2015). Hidden technical debt in machine learning systems. \textit{Advances in NeurIPS}, 28, 2503--2511.

\bibitem{efron1994bootstrap}
Efron, B., \& Tibshirani, R.J. (1994). \textit{An Introduction to the Bootstrap}. CRC Press.

\end{thebibliography}

\appendix

\section*{Software Metadata}

\begin{table}[h]
\centering
\caption{Software Metadata for Sleuth v1.0.0}
\begin{tabular}{ll}
\toprule
\textbf{Field} & \textbf{Value} \\
\midrule
Software Name & Sleuth \\
Current Version & v1.0.0 \\
Permanent DOI & 10.5281/zenodo.17201032 \\
Code Repository & \url{https://github.com/hongping-zh/circular-bias-detection} \\
License & Creative Commons Attribution 4.0 International \\
Programming Languages & JavaScript (React 18.2), Python 3.9+ \\
Platform Requirements & Modern web browser (Chrome 90+, Firefox 88+, Safari 14+) \\
Installation & No installation required (web-based) \\
Documentation & README.md, USER\_GUIDE\_EN.md (in-repository) \\
Testing & 50+ unit tests, 95\% backend coverage, 87\% frontend coverage \\
Continuous Integration & GitHub Actions \\
\bottomrule
\end{tabular}
\end{table}

\section*{Highlights}

\begin{itemize}
    \item First open-source statistical framework for circular bias detection in AI evaluation using three complementary indicators with formal hypothesis testing
    \item Bootstrap resampling methodology provides 95\% confidence intervals and p-values for robust uncertainty characterization
    \item Privacy-preserving client-side architecture ensures sensitive evaluation data never transmits to external servers
    \item Empirically validated achieving 94\% detection accuracy on controlled synthetic datasets and successfully identifying circular patterns in published ImageNet benchmarks
    \item Permanently archived with DOI under Creative Commons Attribution 4.0 International license enabling reproducible research and community extensions
\end{itemize}

\end{document}
