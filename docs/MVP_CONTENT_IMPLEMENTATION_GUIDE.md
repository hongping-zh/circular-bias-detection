# MVP ç½‘ç«™å†…å®¹å®æ–½æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜äº†å¦‚ä½•å®æ–½ä¸?CBD é¡¹ç›®å’?MVP ç½‘ç«™å‡†å¤‡çš„ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼š

1. **æ•°æ®æ”¶é›†ç­–ç•¥** - Hugging Face æ•°æ®é›†å…³é”®è¯å’Œç­›é€?2. **è¯­ä¹‰é‡å†™æ„é€ æ³„éœ?* - Python å®ç°å’Œæ¼”ç¤?3. **æ¡ˆä¾‹ç ”ç©¶æ–‡æ¡ˆ** - "æ±¡æŸ“å±æœº"å®Œæ•´å†…å®¹å’Œå¯è§†åŒ–

---

## ğŸ“ æ–°åˆ›å»ºçš„æ–‡ä»¶ç»“æ„

```
circular-bias-detection/
â”?â”œâ”€â”€ docs/
â”?  â”œâ”€â”€ DATA_COLLECTION_STRATEGY.md          # æ•°æ®æ”¶é›†ç­–ç•¥æ–‡æ¡£
â”?  â”œâ”€â”€ CASE_STUDY_CONTAMINATION_CRISIS.md   # æ¡ˆä¾‹ç ”ç©¶å®Œæ•´æ–‡æ¡ˆ
â”?  â””â”€â”€ MVP_CONTENT_IMPLEMENTATION_GUIDE.md  # æœ¬æŒ‡å?â”?â”œâ”€â”€ data/
â”?  â””â”€â”€ huggingface_data_collector.py        # HF æ•°æ®æ”¶é›†è„šæœ¬
â”?â””â”€â”€ examples/
    â”œâ”€â”€ semantic_rewrite_leakage.py          # è¯­ä¹‰é‡å†™å®ç°
    â””â”€â”€ generate_case_study_visualizations.py # å¯è§†åŒ–ç”Ÿæˆå™¨
```

---

## ğŸš€ å¿«é€Ÿå¼€å§?
### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ?
```bash
# åŸºç¡€ä¾èµ–ï¼ˆå·²åœ?requirements.txt ä¸­ï¼‰
pip install numpy pandas matplotlib seaborn

# Hugging Face æ•°æ®é›†åº“
pip install datasets

# è¯­ä¹‰åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºé«˜çº§æ³„éœ²æ£€æµ‹ï¼‰
pip install sentence-transformers

# å¯é€‰ï¼šåŠ é€Ÿè®¡ç®?pip install scikit-learn
```

### æ­¥éª¤ 2ï¼šè¿è¡Œæ•°æ®æ”¶é›?
```bash
cd data
python huggingface_data_collector.py
```

**é¢„æœŸè¾“å‡ºï¼?*
- `./collected_data/` ç›®å½•ä¸­çš„æ•°æ®é›†æ–‡ä»?- `dataset_inventory.csv` - æ•°æ®é›†æ¸…å?- `collection_report.txt` - æ”¶é›†æŠ¥å‘Š

**è¿è¡Œæ—¶é—´ï¼?* çº?30-60 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦å’Œæ•°æ®é›†å¤§å°ï¼?
### æ­¥éª¤ 3ï¼šç”Ÿæˆè¯­ä¹‰é‡å†™æ³„éœ²ç¤ºä¾?
```bash
cd examples
python semantic_rewrite_leakage.py
```

**é¢„æœŸè¾“å‡ºï¼?*
```
============================================================
è¯­ä¹‰é‡å†™æ„é€ æ³„éœ?- æ¼”ç¤º
============================================================

ã€ç¤ºä¾?1ã€‘æ„é€ å•ä¸ªæ³„éœ²å¯¹
----------------------------------------------------------------------
è®­ç»ƒæ•°æ®:
  The Statue of Liberty was a gift from the people of France...

æ³„éœ²è¯„ä¼°é—®é¢˜:
  The people of which nation presented the Statue of Liberty...

è¯­ä¹‰ç›¸ä¼¼åº? 0.875
è¡¨é¢ç›¸ä¼¼åº? 0.342
é¢„æœŸ C_score: 0.875 (ğŸ”´ CRITICAL)

ã€ç¤ºä¾?2ã€‘æ‰¹é‡æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†
----------------------------------------------------------------------
ç”Ÿæˆçš„æ•°æ®é›†æ ·æœ¬:
...

âœ?æ³„éœ²æ•°æ®é›†å·²ä¿å­˜åˆ? leaked_dataset_sample.csv
```

### æ­¥éª¤ 4ï¼šç”Ÿæˆæ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ–

```bash
cd examples
python generate_case_study_visualizations.py
```

**é¢„æœŸè¾“å‡ºï¼?*
- `./case_study_figures/` ç›®å½•ä¸­çš„ PNG å›¾è¡¨
  - `contamination_risk_map.png`
  - `performance_reality_check.png`
  - `leakage_type_distribution.png`
  - `sample_contamination_heatmap.png`
- `contamination_data.csv` - æ¨¡æ‹Ÿæ•°æ®

---

## ğŸ“Š è¯¦ç»†å®æ–½æ­¥éª¤

### ä¸€ã€æ•°æ®æ”¶é›†å…³é”®è¯å’Œç­–ç•?
#### 1.1 æŸ¥çœ‹ç­–ç•¥æ–‡æ¡£

æ‰“å¼€å¹¶é˜…è¯?`docs/DATA_COLLECTION_STRATEGY.md`ï¼Œè¯¥æ–‡æ¡£åŒ…å«ï¼?
- âœ?**ç­–ç•¥ A**ï¼šäº¤å‰æ±¡æŸ“è¯„ä¼°åŸºå‡†ï¼ˆæœºå™¨ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ã€RAGï¼?- âœ?**ç­–ç•¥ B**ï¼šè®­ç»ƒé›†ä»£è¡¨æ€§æ ·æœ¬ï¼ˆWikipediaã€C4/The Pileï¼?- âœ?**å…³é”®è¯åˆ—è¡?*ï¼šç”¨äº?Hugging Face æœç´¢
- âœ?**ä¼˜å…ˆçº§æ•°æ®é›†**ï¼šå¯ç›´æ¥ä¸‹è½½çš„æ•°æ®é›†åˆ—è¡¨

#### 1.2 è‡ªå®šä¹‰æ•°æ®æ”¶é›?
ç¼–è¾‘ `data/huggingface_data_collector.py` ä¸­çš„ä¼˜å…ˆçº§æ•°æ®é›†é…ç½®ï¼?
```python
PRIORITY_DATASETS = {
    "qa": [
        {"id": "squad_v2", "name": "SQuAD v2.0", "risk": "high", ...},
        # æ·»åŠ æ‚¨è‡ªå·±çš„æ•°æ®é›?        {"id": "your_dataset_id", "name": "Your Dataset", "risk": "high", ...},
    ],
    ...
}
```

#### 1.3 æ‰§è¡Œæ”¶é›†

```python
from data.huggingface_data_collector import HuggingFaceDataCollector

# åˆå§‹åŒ?collector = HuggingFaceDataCollector(output_dir="./my_collected_data")

# åˆ›å»ºæ¸…å•
inventory = collector.create_dataset_inventory()

# æ”¶é›†æ•°æ®é›?collected = collector.collect_all_priority_datasets(
    max_samples_per_dataset=1000,  # æ¯ä¸ªæ•°æ®é›†é‡‡æ ·æ•°é‡?    save_format="csv"
)

# ç”ŸæˆæŠ¥å‘Š
report = collector.generate_collection_report(collected)
print(report)
```

---

### äºŒã€è¯­ä¹‰é‡å†™æ„é€ æ³„éœ?
#### 2.1 ç†è§£æ ¸å¿ƒæ¦‚å¿µ

**ç›®æ ‡ï¼?* æ„é€ è¡¨é¢ä¸åŒä½†è¯­ä¹‰ç›¸ä¼¼çš„æ•°æ®å¯¹ï¼Œä»¥æµ‹è¯• CBD çš„æ£€æµ‹èƒ½åŠ›ã€?
**å…³é”®æŠ€æœ¯ï¼š**
- åŒä¹‰è¯æ›¿æ?- å¥å¼é‡ç»„ï¼ˆä¸»åŠ?â†?è¢«åŠ¨ï¼?- é‡Šä¹‰ç”Ÿæˆ

#### 2.2 ä½¿ç”¨è¯­ä¹‰é‡å†™å™?
```python
from examples.semantic_rewrite_leakage import SemanticRewriter, LeakageSimulator

# åˆå§‹åŒ–é‡å†™å™¨
rewriter = SemanticRewriter()

# æ„é€ å•ä¸ªæ³„éœ²å¯¹
train_text = "Your training data sentence here."
pair = rewriter.construct_leaked_pair(
    train_sentence=train_text,
    leakage_intensity=0.8  # 0=å®Œå…¨ä¸åŒ, 1=å®Œå…¨ç›¸åŒ
)

print(f"è®­ç»ƒæ–‡æœ¬: {pair.train_text}")
print(f"æ³„éœ²é—®é¢˜: {pair.eval_question}")
print(f"è¯­ä¹‰ç›¸ä¼¼åº? {pair.semantic_similarity:.3f}")
print(f"è¡¨é¢ç›¸ä¼¼åº? {pair.surface_similarity:.3f}")
```

#### 2.3 æ‰¹é‡æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›?
```python
# åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨
simulator = LeakageSimulator()

# æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›?df_leaked = simulator.simulate_leakage_dataset(
    num_samples=100,        # æ ·æœ¬æ•°é‡
    leakage_ratio=0.4,      # æ³„éœ²æ ·æœ¬æ¯”ä¾‹ï¼?0%ï¼?    leakage_intensity=0.75  # æ³„éœ²å¼ºåº¦
)

# åˆ†ææ³„éœ²åˆ†å¸ƒ
analysis = simulator.analyze_leakage_distribution(df_leaked)
print(f"æ³„éœ²æ ·æœ¬: {analysis['leaked_samples']}")
print(f"å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæ³„éœ²ï¼? {analysis['avg_semantic_sim_leaked']:.3f}")

# ä¿å­˜æ•°æ®é›?df_leaked.to_csv("my_leaked_dataset.csv", index=False)
```

#### 2.4 ä¸?CBD æ¡†æ¶é›†æˆ

```python
from circular_bias_detector import BiasDetector

# ä½¿ç”¨ç”Ÿæˆçš„æ³„éœ²æ•°æ®é›†æµ‹è¯• CBD
detector = BiasDetector()

# å‡†å¤‡æ•°æ®çŸ©é˜µï¼ˆç¤ºä¾‹ï¼‰
# è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„å…·ä½“æ•°æ®æ ¼å¼è¿›è¡Œè°ƒæ•?performance_matrix = ...  # ä»?df_leaked æ„å»º
constraint_matrix = ...    # çº¦æŸæ¡ä»¶

# æ£€æµ‹åå·?results = detector.detect_bias(
    performance_matrix=performance_matrix,
    constraint_matrix=constraint_matrix
)

print(f"æ£€æµ‹åˆ°åå·®: {results['overall_bias']}")
```

---

### ä¸‰ã€æ¡ˆä¾‹ç ”ç©¶æ–‡æ¡ˆå’Œå¯è§†åŒ?
#### 3.1 æŸ¥çœ‹æ¡ˆä¾‹ç ”ç©¶æ–‡æ¡£

æ‰“å¼€ `docs/CASE_STUDY_CONTAMINATION_CRISIS.md`ï¼Œè¯¥æ–‡æ¡£åŒ…å«ï¼?
- âœ?**æ‰§è¡Œæ‘˜è¦**ï¼šå…³é”®å‘ç°å’Œå•†ä¸šå½±å“
- âœ?**èƒŒæ™¯ä¸åŠ¨æœ?*ï¼šè¯„ä¼°åœºæ™¯å’Œåˆæ­¥ç»“æœ
- âœ?**CBD åˆ†ææµç¨‹**ï¼šå®Œæ•´çš„ Python ä»£ç ç¤ºä¾‹
- âœ?**æ ¸å¿ƒå‘ç°**ï¼šä¸‰å¤§å‘ç°å’Œæ•°æ®æ”¯æŒ
- âœ?**å›¾è¡¨æè¿°**ï¼šä¸¤ä¸ªæ ¸å¿ƒå›¾è¡¨çš„è¯¦ç»†è§„æ ¼
- âœ?**ä¿®æ­£æªæ–½**ï¼šåŸºäº?CBD å‘ç°çš„è¡ŒåŠ¨è®¡åˆ?
#### 3.2 ç”Ÿæˆå¯è§†åŒ?
```python
from examples.generate_case_study_visualizations import CaseStudyVisualizer

# åˆå§‹åŒ–å¯è§†åŒ–å™?visualizer = CaseStudyVisualizer(output_dir="./my_figures")

# å›¾è¡¨ 1: åå·®åˆ†æ•°åˆ†å¸ƒ
c_scores = np.random.beta(2, 5, 10000)  # æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®æ•°æ?visualizer.generate_contamination_risk_map(
    c_scores=c_scores,
    save_path="risk_map.png"
)

# å›¾è¡¨ 2: æ€§èƒ½ä¿®æ­£å¯¹æ¯”
visualizer.generate_performance_reality_check(
    original_acc=95.1,
    corrected_acc=58.3,
    save_path="performance_check.png"
)

# å›¾è¡¨ 3: æ³„éœ²ç±»å‹åˆ†å¸ƒ
leakage_types = {
    'Exact Match': 120,
    'Paraphrase': 850,
    'Partial Overlap': 1530,
    'Semantic Similar': 1500
}
visualizer.generate_leakage_type_distribution(
    leakage_types=leakage_types,
    save_path="leakage_types.png"
)
```

#### 3.3 è‡ªå®šä¹‰å¯è§†åŒ–

ç¼–è¾‘ `examples/generate_case_study_visualizations.py` ä¸­çš„ç»˜å›¾å‡½æ•°ï¼Œè°ƒæ•´ï¼š

- é¢œè‰²æ–¹æ¡ˆ
- å­—ä½“å¤§å°
- æ ‡ç­¾æ–‡æœ¬
- å›¾ä¾‹ä½ç½®
- DPI åˆ†è¾¨ç?
---

## ğŸŒ MVP ç½‘ç«™é›†æˆ

### å°†å†…å®¹é›†æˆåˆ°ç½‘ç«™çš„å»ºè®?
#### 1. æ•°æ®æ”¶é›†é¡µé¢

**ä½ç½®ï¼?* `/data-collection` æˆ?`/datasets`

**å†…å®¹ï¼?*
- åµŒå…¥ `DATA_COLLECTION_STRATEGY.md` çš„ä¸»è¦å†…å®?- æ·»åŠ äº¤äº’å¼æ•°æ®é›†æœç´¢åŠŸèƒ½
- æä¾›æ•°æ®é›†ä¸‹è½½é“¾æ?
**ä»£ç ç¤ºä¾‹ï¼ˆReactï¼‰ï¼š**
```jsx
import React from 'react';

const DataCollectionPage = () => {
  return (
    <div className="container">
      <h1>Data Collection Strategy</h1>
      <section>
        <h2>Priority Datasets</h2>
        <DatasetTable datasets={priorityDatasets} />
      </section>
      <section>
        <h2>Quick Start</h2>
        <CodeBlock language="python">
          {`from datasets import load_dataset
dataset = load_dataset("squad_v2")`}
        </CodeBlock>
      </section>
    </div>
  );
};
```

#### 2. æ¡ˆä¾‹ç ”ç©¶é¡µé¢

**ä½ç½®ï¼?* `/case-studies/contamination-crisis`

**å†…å®¹ï¼?*
- æ˜¾ç¤º `CASE_STUDY_CONTAMINATION_CRISIS.md` çš„å†…å®?- åµŒå…¥ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨
- æ·»åŠ äº¤äº’å¼å…ƒç´ ï¼ˆå¦‚æ‚¬åœæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰

**ä»£ç ç¤ºä¾‹ï¼ˆReactï¼‰ï¼š**
```jsx
const CaseStudyPage = () => {
  return (
    <div className="case-study">
      <h1>Contamination Crisis</h1>
      
      {/* æ‰§è¡Œæ‘˜è¦ */}
      <section className="summary">
        <StatCard 
          title="Performance Drop"
          value="36.8%"
          icon={<TrendingDownIcon />}
        />
        <StatCard 
          title="Contaminated Samples"
          value="40%"
          icon={<AlertIcon />}
        />
      </section>
      
      {/* å›¾è¡¨ */}
      <section className="visualizations">
        <img 
          src="/figures/contamination_risk_map.png"
          alt="Risk Map"
          className="responsive-chart"
        />
        <img 
          src="/figures/performance_reality_check.png"
          alt="Performance Check"
          className="responsive-chart"
        />
      </section>
      
      {/* è¯¦ç»†åˆ†æ */}
      <section className="analysis">
        <Markdown content={caseStudyContent} />
      </section>
    </div>
  );
};
```

#### 3. äº¤äº’å¼æ¼”ç¤ºé¡µé?
**ä½ç½®ï¼?* `/demo/semantic-leakage`

**åŠŸèƒ½ï¼?*
- ç”¨æˆ·è¾“å…¥è®­ç»ƒå¥å­
- å®æ—¶ç”Ÿæˆæ³„éœ²é—®é¢˜
- æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†æ•?- å¯è§†åŒ–è¯­ä¹‰é‡å†™è¿‡ç¨?
**ä»£ç ç¤ºä¾‹ï¼ˆReactï¼‰ï¼š**
```jsx
const SemanticLeakageDemo = () => {
  const [trainText, setTrainText] = useState('');
  const [leakedPair, setLeakedPair] = useState(null);
  
  const handleGenerate = async () => {
    const response = await fetch('/api/generate-leakage', {
      method: 'POST',
      body: JSON.stringify({ train_text: trainText }),
      headers: { 'Content-Type': 'application/json' }
    });
    const data = await response.json();
    setLeakedPair(data);
  };
  
  return (
    <div className="demo-container">
      <h2>Semantic Rewrite Demo</h2>
      
      <textarea
        value={trainText}
        onChange={(e) => setTrainText(e.target.value)}
        placeholder="Enter training data sentence..."
        rows={4}
      />
      
      <button onClick={handleGenerate}>Generate Leaked Question</button>
      
      {leakedPair && (
        <div className="results">
          <div className="result-card">
            <h3>Leaked Question</h3>
            <p>{leakedPair.eval_question}</p>
          </div>
          
          <div className="similarity-scores">
            <ScoreBar 
              label="Semantic Similarity"
              value={leakedPair.semantic_similarity}
              color={leakedPair.semantic_similarity > 0.75 ? 'red' : 'green'}
            />
            <ScoreBar 
              label="Surface Similarity"
              value={leakedPair.surface_similarity}
              color="blue"
            />
          </div>
        </div>
      )}
    </div>
  );
};
```

---

## ğŸ“ˆ åç«¯ API é›†æˆ

### API ç«¯ç‚¹å»ºè®®

#### 1. æ•°æ®æ”¶é›† API

```python
# backend/routes/data_collection.py
from flask import Blueprint, request, jsonify
from data.huggingface_data_collector import HuggingFaceDataCollector

data_bp = Blueprint('data', __name__)

@data_bp.route('/api/datasets/search', methods=['POST'])
def search_datasets():
    """æœç´¢ Hugging Face æ•°æ®é›?""
    keywords = request.json.get('keywords', [])
    collector = HuggingFaceDataCollector()
    results = collector.search_datasets_by_keyword(keywords, limit=50)
    return jsonify({'datasets': results})

@data_bp.route('/api/datasets/inventory', methods=['GET'])
def get_inventory():
    """è·å–æ•°æ®é›†æ¸…å?""
    collector = HuggingFaceDataCollector()
    inventory = collector.create_dataset_inventory()
    return jsonify(inventory.to_dict('records'))
```

#### 2. è¯­ä¹‰é‡å†™ API

```python
# backend/routes/semantic_rewrite.py
from flask import Blueprint, request, jsonify
from examples.semantic_rewrite_leakage import SemanticRewriter

rewrite_bp = Blueprint('rewrite', __name__)

@rewrite_bp.route('/api/generate-leakage', methods=['POST'])
def generate_leakage():
    """ç”Ÿæˆæ³„éœ²æ•°æ®å¯?""
    train_text = request.json.get('train_text', '')
    intensity = request.json.get('intensity', 0.75)
    
    rewriter = SemanticRewriter()
    pair = rewriter.construct_leaked_pair(train_text, intensity)
    
    return jsonify({
        'train_text': pair.train_text,
        'eval_question': pair.eval_question,
        'eval_question_clean': pair.eval_question_clean,
        'semantic_similarity': float(pair.semantic_similarity),
        'surface_similarity': float(pair.surface_similarity),
        'leakage_type': pair.leakage_type
    })

@rewrite_bp.route('/api/simulate-dataset', methods=['POST'])
def simulate_dataset():
    """æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›?""
    num_samples = request.json.get('num_samples', 100)
    leakage_ratio = request.json.get('leakage_ratio', 0.4)
    
    simulator = LeakageSimulator()
    df = simulator.simulate_leakage_dataset(num_samples, leakage_ratio)
    
    return jsonify(df.to_dict('records'))
```

#### 3. å¯è§†åŒ–ç”Ÿæˆ?API

```python
# backend/routes/visualizations.py
from flask import Blueprint, send_file, request
from examples.generate_case_study_visualizations import CaseStudyVisualizer
import io

viz_bp = Blueprint('viz', __name__)

@viz_bp.route('/api/visualizations/risk-map', methods=['POST'])
def generate_risk_map():
    """ç”Ÿæˆé£é™©åˆ†å¸ƒå›?""
    c_scores = request.json.get('c_scores', [])
    
    visualizer = CaseStudyVisualizer(output_dir="./temp")
    visualizer.generate_contamination_risk_map(
        c_scores=np.array(c_scores),
        save_path="risk_map.png"
    )
    
    return send_file(
        "./temp/risk_map.png",
        mimetype='image/png',
        as_attachment=False
    )
```

---

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯?
### å•å…ƒæµ‹è¯•

åˆ›å»º `tests/test_new_features.py`ï¼?
```python
import unittest
import numpy as np
from data.huggingface_data_collector import HuggingFaceDataCollector
from examples.semantic_rewrite_leakage import SemanticRewriter, LeakageSimulator
from examples.generate_case_study_visualizations import CaseStudyVisualizer

class TestDataCollection(unittest.TestCase):
    
    def setUp(self):
        self.collector = HuggingFaceDataCollector(output_dir="./test_data")
    
    def test_search_datasets(self):
        results = self.collector.search_datasets_by_keyword(['qa'], limit=5)
        self.assertIsInstance(results, list)
        self.assertGreater(len(results), 0)
    
    def test_create_inventory(self):
        inventory = self.collector.create_dataset_inventory()
        self.assertGreater(len(inventory), 0)

class TestSemanticRewrite(unittest.TestCase):
    
    def setUp(self):
        self.rewriter = SemanticRewriter()
    
    def test_construct_leaked_pair(self):
        train_text = "Test sentence for rewriting."
        pair = self.rewriter.construct_leaked_pair(train_text, 0.7)
        
        self.assertIsNotNone(pair.eval_question)
        self.assertGreater(pair.semantic_similarity, 0)
        self.assertLess(pair.surface_similarity, 1)

class TestVisualizations(unittest.TestCase):
    
    def setUp(self):
        self.visualizer = CaseStudyVisualizer(output_dir="./test_figures")
    
    def test_generate_risk_map(self):
        c_scores = np.random.beta(2, 5, 100)
        self.visualizer.generate_contamination_risk_map(c_scores, "test_map.png")
        
        # éªŒè¯æ–‡ä»¶ç”Ÿæˆ
        import os
        self.assertTrue(os.path.exists("./test_figures/test_map.png"))

if __name__ == '__main__':
    unittest.main()
```

è¿è¡Œæµ‹è¯•ï¼?```bash
python -m pytest tests/test_new_features.py -v
```

---

## ğŸ“ æ–‡æ¡£å’Œç»´æŠ?
### æ›´æ–°ä¸?README

åœ?`README.md` ä¸­æ·»åŠ æ–°åŠŸèƒ½çš„é“¾æ¥ï¼š

```markdown
## ğŸ†• New Features

### Data Collection Strategy
Learn how to collect high-risk evaluation datasets from Hugging Face. 
[Read the guide â†’](docs/DATA_COLLECTION_STRATEGY.md)

### Semantic Leakage Construction
Understand how to construct subtle data leakage for testing CBD.
[See examples â†’](examples/semantic_rewrite_leakage.py)

### Case Study: Contamination Crisis
Explore a real-world case study showing CBD's impact.
[Read the full case study â†’](docs/CASE_STUDY_CONTAMINATION_CRISIS.md)
```

### åˆ›å»ºå˜æ›´æ—¥å¿—

åœ?`CHANGELOG.md` ä¸­æ·»åŠ ï¼š

```markdown
## [Unreleased]

### Added
- Data collection strategy for Hugging Face datasets
- Semantic rewrite module for leakage construction
- Case study: Contamination Crisis with complete documentation
- Visualization generator for case study figures
- API endpoints for data collection and semantic rewriting

### Documentation
- DATA_COLLECTION_STRATEGY.md - Comprehensive data collection guide
- CASE_STUDY_CONTAMINATION_CRISIS.md - Full case study documentation
- MVP_CONTENT_IMPLEMENTATION_GUIDE.md - Implementation guide
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ?
### ä¼˜å…ˆçº§ä»»åŠ¡æ¸…å?
- [ ] **é«˜ä¼˜å…ˆçº§**
  - [ ] è¿è¡Œæ‰€æœ‰ä¸‰ä¸ªè„šæœ¬ï¼ŒéªŒè¯è¾“å‡º
  - [ ] ç”ŸæˆçœŸå®çš„å¯è§†åŒ–å›¾è¡¨
  - [ ] å°†å›¾è¡¨é›†æˆåˆ° MVP ç½‘ç«™

- [ ] **ä¸­ä¼˜å…ˆçº§**
  - [ ] ä¸‹è½½ 2-3 ä¸?Hugging Face æ•°æ®é›†è¿›è¡Œåˆæ­¥æµ‹è¯?  - [ ] ç¼–å†™ API ç«¯ç‚¹å¹¶è¿›è¡Œé›†æˆæµ‹è¯?  - [ ] åˆ›å»ºäº¤äº’å¼æ¼”ç¤ºé¡µé?
- [ ] **ä½ä¼˜å…ˆçº§**
  - [ ] ä¼˜åŒ–å¯è§†åŒ–çš„é¢œè‰²å’Œæ ·å¼?  - [ ] æ·»åŠ å¤šè¯­è¨€æ”¯æŒï¼ˆä¸­æ–?è‹±æ–‡ï¼?  - [ ] ç¼–å†™æ›´å¤šå•å…ƒæµ‹è¯•

### æ—¶é—´ä¼°ç®—

| ä»»åŠ¡ | ä¼°è®¡æ—¶é—´ |
|------|----------|
| éªŒè¯æ‰€æœ‰è„šæœ?| 2-3 å°æ—¶ |
| é›†æˆåˆ°ç½‘ç«™å‰ç«?| 4-6 å°æ—¶ |
| åç«¯ API å¼€å?| 3-4 å°æ—¶ |
| æµ‹è¯•å’Œè°ƒè¯?| 2-3 å°æ—¶ |
| æ–‡æ¡£å®Œå–„ | 1-2 å°æ—¶ |
| **æ€»è®¡** | **12-18 å°æ—¶** |

---

## ğŸ†˜ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q1: Hugging Face æ•°æ®é›†ä¸‹è½½å¤±è´?*
```bash
# è§£å†³æ–¹æ¡ˆï¼šè®¾ç½®ä»£ç†æˆ–ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

**Q2: å¯è§†åŒ–ç”Ÿæˆæ—¶ä¸­æ–‡æ˜¾ç¤ºä¸ºæ–¹æ¡?*
```python
# è§£å†³æ–¹æ¡ˆï¼šå®‰è£…ä¸­æ–‡å­—ä½?import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
```

**Q3: å†…å­˜ä¸è¶³é”™è¯¯ï¼ˆå¤§æ•°æ®é›†ï¼‰**
```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æµå¼åŠ è½?from datasets import load_dataset
dataset = load_dataset("dataset_id", streaming=True)
```

---

## ğŸ“ æ”¯æŒå’Œåé¦?
å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»ï¼š

- **é‚®ç®±ï¼?* yujjam@uest.edu.gr
- **GitHub Issuesï¼?* [æäº¤é—®é¢˜](https://github.com/hongping-zh/circular-bias-detection/issues)

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼?* v1.0  
**æœ€åæ›´æ–°ï¼š** 2024-10-27  
**ä½œè€…ï¼š** Hongping Zhang
