# ğŸ†• æ–°åŠŸèƒ½è¯´æ˜?- MVP å†…å®¹å‡†å¤‡

## ğŸ“… æ›´æ–°æ—¥æœŸï¼?024å¹?0æœ?7æ—?
---

## ğŸ¯ æ¦‚è¿°

æœ¬æ¬¡æ›´æ–°ä¸?CBD é¡¹ç›®å’?MVP ç½‘ç«™å‡†å¤‡äº†å®Œæ•´çš„å†…å®¹åŸºç¡€ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ç­–ç•¥ã€è¯­ä¹‰é‡å†™å®ç°å’Œæ¡ˆä¾‹ç ”ç©¶æ–‡æ¡ˆã€?
**æ ¸å¿ƒç›®æ ‡ï¼?* åœ?PR å’?JOSS è®ºæ–‡åé¦ˆæœŸé—´ï¼Œæ·±å…¥æ¨è¿›ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼Œä¸ºé¡¹ç›®æä¾›ä¸°å¯Œçš„æ¼”ç¤ºå†…å®¹å’Œå¯è§†åŒ–èµ„äº§ã€?
---

## ğŸ“¦ æ–°å¢æ–‡ä»¶æ€»è§ˆ

### ğŸ“ æ–‡æ¡£æ–‡ä»¶ï¼?ä¸ªï¼‰

| æ–‡ä»¶å?| ä½ç½® | ç”¨é€?|
|--------|------|------|
| `DATA_COLLECTION_STRATEGY.md` | `docs/` | Hugging Face æ•°æ®é›†æ”¶é›†ç­–ç•?|
| `CASE_STUDY_CONTAMINATION_CRISIS.md` | `docs/` | å®Œæ•´çš„æ¡ˆä¾‹ç ”ç©¶æ–‡æ¡ˆå’Œå›¾è¡¨æè¿° |
| `MVP_CONTENT_IMPLEMENTATION_GUIDE.md` | `docs/` | è¯¦ç»†çš„å®æ–½å’Œé›†æˆæŒ‡å— |
| `NEW_FEATURES_README.md` | `docs/` | æœ¬æ–‡ä»¶ï¼Œæ–°åŠŸèƒ½è¯´æ˜?|
| `IMPLEMENTATION_SUMMARY.md` | é¡¹ç›®æ ¹ç›®å½?| å®Œæˆå·¥ä½œçš„æ€»ç»“æŠ¥å‘Š |
| `QUICK_START_MVP_CONTENT.md` | é¡¹ç›®æ ¹ç›®å½?| å¿«é€Ÿå¯åŠ¨å‚è€ƒå¡ |

### ğŸ Python è„šæœ¬ï¼?ä¸ªï¼‰

| æ–‡ä»¶å?| ä½ç½® | åŠŸèƒ½ |
|--------|------|------|
| `huggingface_data_collector.py` | `data/` | è‡ªåŠ¨åŒ–æ”¶é›?HF æ•°æ®é›?|
| `semantic_rewrite_leakage.py` | `examples/` | æ„é€ è¯­ä¹‰æ³„éœ²ç¤ºä¾?|
| `generate_case_study_visualizations.py` | `examples/` | ç”Ÿæˆæ¡ˆä¾‹ç ”ç©¶å›¾è¡¨ |
| `run_mvp_content_generation.py` | é¡¹ç›®æ ¹ç›®å½?| ä¸€é”®è¿è¡Œä¸»è„šæœ¬ |

**æ€»è®¡ï¼?0 ä¸ªæ–°æ–‡ä»¶**

---

## ğŸš€ å¿«é€Ÿå¼€å§?
### ç¬¬ä¸€æ­¥ï¼šè¿è¡Œä¸»è„šæœ?
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd c:\Users\14593\CascadeProjects\circular-bias-detection

# è¿è¡Œä¸»è„šæœ?python run_mvp_content_generation.py
```

**äº¤äº’å¼èœå•ä¼šå¼•å¯¼æ‚¨å®Œæˆæ‰€æœ‰æ­¥éª¤ã€?*

### ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹ç”Ÿæˆçš„å†…å®?
```bash
# å¯è§†åŒ–å›¾è¡?dir mvp_case_study_figures\*.png

# æ¨¡æ‹Ÿæ•°æ®
type mvp_leaked_dataset.csv

# æ”¶é›†çš„æ•°æ®é›†ï¼ˆå¦‚æœè¿è¡Œäº†æ­¥éª¤1ï¼?dir mvp_collected_data\
```

### ç¬¬ä¸‰æ­¥ï¼šé˜…è¯»æ–‡æ¡£

1. **æ¡ˆä¾‹ç ”ç©¶æ–‡æ¡ˆ**ï¼š`docs/CASE_STUDY_CONTAMINATION_CRISIS.md`
2. **æ•°æ®æ”¶é›†ç­–ç•¥**ï¼š`docs/DATA_COLLECTION_STRATEGY.md`
3. **å®æ–½æŒ‡å—**ï¼š`docs/MVP_CONTENT_IMPLEMENTATION_GUIDE.md`

---

## ğŸ“Š æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. æ•°æ®æ”¶é›†ç­–ç•¥

**æ–‡ä»¶ï¼?* `docs/DATA_COLLECTION_STRATEGY.md` + `data/huggingface_data_collector.py`

**åŠŸèƒ½ï¼?*
- âœ?å®šä¹‰é«˜é£é™©è¯„ä¼°æ•°æ®é›†çš„æœç´¢ç­–ç•?- âœ?æä¾› Hugging Face æ•°æ®é›†å…³é”®è¯åˆ—è¡¨
- âœ?è‡ªåŠ¨åŒ–ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ?- âœ?ç”Ÿæˆæ•°æ®é›†æ¸…å•å’Œæ”¶é›†æŠ¥å‘Š

**é€‚ç”¨åœºæ™¯ï¼?*
- ä¸?CBD å®éªŒå¯»æ‰¾åˆé€‚çš„è¯„ä¼°æ•°æ®é›?- æ„å»ºè®­ç»ƒ-è¯„ä¼°äº¤å‰æ±¡æŸ“æ£€æµ‹çš„æµ‹è¯•é›?- æ¼”ç¤º CBD åœ¨çœŸå®æ•°æ®ä¸Šçš„åº”ç”?
**ä½¿ç”¨ç¤ºä¾‹ï¼?*
```python
from data.huggingface_data_collector import HuggingFaceDataCollector

collector = HuggingFaceDataCollector(output_dir="./my_data")

# æœç´¢æ•°æ®é›?qa_datasets = collector.search_datasets_by_keyword(["question", "qa"])

# ä¸‹è½½æ•°æ®é›?df = collector.download_dataset("squad_v2", max_samples=1000)

# æ‰¹é‡æ”¶é›†
collected = collector.collect_all_priority_datasets()
```

---

### 2. è¯­ä¹‰é‡å†™æ„é€ æ³„éœ?
**æ–‡ä»¶ï¼?* `examples/semantic_rewrite_leakage.py`

**åŠŸèƒ½ï¼?*
- âœ?åŒä¹‰è¯æ›¿æ¢ï¼ˆSynonym Replacementï¼?- âœ?å¥å¼é‡ç»„ï¼ˆActive â†?Passive Voiceï¼?- âœ?é‡Šä¹‰é—®é¢˜ç”Ÿæˆï¼ˆParaphrase Questionsï¼?- âœ?æ‰¹é‡æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›?
**é€‚ç”¨åœºæ™¯ï¼?*
- æµ‹è¯• CBD å¯¹è¯­ä¹‰æ³„éœ²çš„æ£€æµ‹èƒ½åŠ?- ç”Ÿæˆå¯¹ç…§å®éªŒæ•°æ®ï¼ˆæ³„éœ?vs å¹²å‡€ï¼?- æ¼”ç¤ºéšè”½æ³„éœ²çš„æ„é€ è¿‡ç¨?
**ä½¿ç”¨ç¤ºä¾‹ï¼?*
```python
from examples.semantic_rewrite_leakage import SemanticRewriter, LeakageSimulator

# æ„é€ å•ä¸ªæ³„éœ²å¯¹
rewriter = SemanticRewriter()
pair = rewriter.construct_leaked_pair(
    train_sentence="France gave the Statue of Liberty to the US.",
    leakage_intensity=0.8
)

print(f"æ³„éœ²é—®é¢˜: {pair.eval_question}")
print(f"è¯­ä¹‰ç›¸ä¼¼åº? {pair.semantic_similarity:.3f}")

# æ‰¹é‡æ¨¡æ‹Ÿæ•°æ®é›?simulator = LeakageSimulator()
df = simulator.simulate_leakage_dataset(
    num_samples=100,
    leakage_ratio=0.4,
    leakage_intensity=0.75
)
```

**è¾“å‡ºç¤ºä¾‹ï¼?*
```
è®­ç»ƒæ•°æ®: "The Statue of Liberty was a gift from France..."
æ³„éœ²é—®é¢˜: "Which nation provided the Statue of Liberty?"
è¯­ä¹‰ç›¸ä¼¼åº? 0.875 (ğŸ”´ CRITICAL)
è¡¨é¢ç›¸ä¼¼åº? 0.342
```

---

### 3. æ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ?
**æ–‡ä»¶ï¼?* `examples/generate_case_study_visualizations.py`

**åŠŸèƒ½ï¼?*
- âœ?ç”Ÿæˆåå·®åˆ†æ•°åˆ†å¸ƒå›¾ï¼ˆå¸¦é£é™©åˆ†åŒºï¼‰
- âœ?ç”Ÿæˆæ€§èƒ½ä¿®æ­£å¯¹æ¯”å›¾ï¼ˆæŸ±çŠ¶å›¾ï¼‰
- âœ?ç”Ÿæˆæ³„éœ²ç±»å‹åˆ†å¸ƒå›¾ï¼ˆé¥¼å›¾ï¼?- âœ?ç”Ÿæˆæ ·æœ¬æ±¡æŸ“çƒ­åŠ›å›¾ï¼ˆçŸ©é˜µï¼?
**é€‚ç”¨åœºæ™¯ï¼?*
- ä¸ºæ¡ˆä¾‹ç ”ç©¶ç”Ÿæˆä¸“ä¸šå›¾è¡?- ä¸?MVP ç½‘ç«™å‡†å¤‡å¯è§†åŒ–èµ„äº?- æ¼”ç¤º CBD æ£€æµ‹ç»“æ?
**ä½¿ç”¨ç¤ºä¾‹ï¼?*
```python
from examples.generate_case_study_visualizations import CaseStudyVisualizer
import numpy as np

visualizer = CaseStudyVisualizer(output_dir="./figures")

# å›¾è¡¨ 1: é£é™©åˆ†å¸ƒ
c_scores = np.random.beta(2, 5, 10000)
visualizer.generate_contamination_risk_map(c_scores, "risk_map.png")

# å›¾è¡¨ 2: æ€§èƒ½å¯¹æ¯”
visualizer.generate_performance_reality_check(
    original_acc=95.1,
    corrected_acc=58.3,
    save_path="performance_check.png"
)
```

**ç”Ÿæˆçš„å›¾è¡¨ï¼š**
1. `contamination_risk_map.png` - å±•ç¤º C_score åˆ†å¸ƒ
2. `performance_reality_check.png` - 95.1% vs 58.3% å¯¹æ¯”
3. `leakage_type_distribution.png` - å››ç§æ³„éœ²ç±»å‹
4. `sample_contamination_heatmap.png` - 50x50 çƒ­åŠ›å›?
---

### 4. æ¡ˆä¾‹ç ”ç©¶æ–‡æ¡ˆ

**æ–‡ä»¶ï¼?* `docs/CASE_STUDY_CONTAMINATION_CRISIS.md`

**å†…å®¹ç»“æ„ï¼?*
1. **æ‰§è¡Œæ‘˜è¦** - å…³é”®å‘ç°å’Œå•†ä¸šå½±å“?2. **èƒŒæ™¯ä¸åŠ¨æœ?* - è¯„ä¼°åœºæ™¯å’Œé—®é¢?3. **CBD åˆ†ææµç¨‹** - å®Œæ•´ä»£ç ç¤ºä¾‹
4. **æ ¸å¿ƒå‘ç°** - ä¸‰å¤§å‘ç°å’Œæ•°æ®æ”¯æŒ?5. **å›¾è¡¨æè¿°** - è¯¦ç»†çš„å¯è§†åŒ–è§„æ ¼å’Œæ–‡æ¡?6. **ä¿®æ­£æªæ–½** - åŸºäº CBD çš„è¡ŒåŠ¨å»ºè®?7. **æ•™è®­ä¸å¯ç¤?* - æœ€ä½³å®è·?
**é€‚ç”¨åœºæ™¯ï¼?*
- MVP ç½‘ç«™çš„æ¡ˆä¾‹ç ”ç©¶é¡µé¢å†…å®?- è¥é”€ææ–™å’Œå®£ä¼ æ–‡æ¡?- ç”¨æˆ·æ•™è‚²å’ŒåŸ¹è®­ææ–?
**æ ¸å¿ƒæ•°æ®ç‚¹ï¼š**
- åŸå§‹å‡†ç¡®ç‡ï¼š**95.1%**
- ä¿®æ­£åå‡†ç¡®ç‡ï¼?*58.3%**
- æ€§èƒ½ä¸‹é™ï¼?*-36.8%**
- æ±¡æŸ“æ ·æœ¬ï¼?*40%** (4,000/10,000)
- æœ€é«?C_scoreï¼?*0.87** (ğŸ”´ CRITICAL)
- é¿å…æŸå¤±ï¼?*$7-15M**

---

## ğŸŒ MVP ç½‘ç«™é›†æˆè·¯çº¿å›?
### é˜¶æ®µ 1ï¼šå†…å®¹å±•ç¤ºï¼ˆ1-2 å¤©ï¼‰

**ä»»åŠ¡ï¼?*
- [ ] å°?4 ä¸?PNG å›¾è¡¨å¤åˆ¶åˆ°ç½‘ç«™çš„ `/public/figures/` ç›®å½•
- [ ] åˆ›å»ºæ¡ˆä¾‹ç ”ç©¶é¡µé¢ `/case-studies/contamination-crisis`
- [ ] åµŒå…¥å›¾è¡¨å’Œæ–‡æ¡ˆå†…å®?- [ ] æ·»åŠ ä¸‹è½½é“¾æ¥ï¼ˆPDF æŠ¥å‘Šï¼?
**é¢„æœŸæ•ˆæœï¼?*
- ç”¨æˆ·å¯ä»¥æµè§ˆå®Œæ•´çš„æ¡ˆä¾‹ç ”ç©?- é«˜è´¨é‡çš„å¯è§†åŒ–å¢å¼ºå¯ä¿¡åº¦

---

### é˜¶æ®µ 2ï¼šåç«?APIï¼?-3 å¤©ï¼‰

**ä»»åŠ¡ï¼?*
- [ ] å®ç°æ•°æ®é›†æœç´?API (`/api/datasets/search`)
- [ ] å®ç°è¯­ä¹‰é‡å†™ API (`/api/generate-leakage`)
- [ ] å®ç°å¯è§†åŒ–ç”Ÿæˆ?API (`/api/visualizations/risk-map`)
- [ ] ç¼–å†™ API æ–‡æ¡£ï¼ˆSwaggerï¼?
**API ç«¯ç‚¹ç¤ºä¾‹ï¼?*
```python
# Flask ç¤ºä¾‹
@app.route('/api/generate-leakage', methods=['POST'])
def generate_leakage():
    from examples.semantic_rewrite_leakage import SemanticRewriter
    
    train_text = request.json.get('train_text', '')
    intensity = request.json.get('intensity', 0.75)
    
    rewriter = SemanticRewriter()
    pair = rewriter.construct_leaked_pair(train_text, intensity)
    
    return jsonify({
        'eval_question': pair.eval_question,
        'semantic_similarity': float(pair.semantic_similarity),
        'surface_similarity': float(pair.surface_similarity)
    })
```

---

### é˜¶æ®µ 3ï¼šäº¤äº’å¼æ¼”ç¤ºï¼?-4 å¤©ï¼‰

**ä»»åŠ¡ï¼?*
- [ ] åˆ›å»ºæ•°æ®æ”¶é›†ç­–ç•¥é¡µé¢ `/data-collection`
- [ ] åˆ›å»ºè¯­ä¹‰æ³„éœ²æ¼”ç¤ºé¡µé¢ `/demo/semantic-leakage`
- [ ] å®ç°å‰ç«¯è¡¨å•å’Œç»“æœå±•ç¤?- [ ] é›†æˆ API è°ƒç”¨

**React ç»„ä»¶ç¤ºä¾‹ï¼?*
```jsx
const SemanticLeakageDemo = () => {
  const [trainText, setTrainText] = useState('');
  const [result, setResult] = useState(null);
  
  const handleGenerate = async () => {
    const response = await fetch('/api/generate-leakage', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ train_text: trainText, intensity: 0.8 })
    });
    const data = await response.json();
    setResult(data);
  };
  
  return (
    <div>
      <textarea 
        value={trainText}
        onChange={(e) => setTrainText(e.target.value)}
        placeholder="Enter training data sentence..."
      />
      <button onClick={handleGenerate}>Generate Leaked Question</button>
      
      {result && (
        <div>
          <p><strong>Leaked Question:</strong> {result.eval_question}</p>
          <p>Semantic Similarity: {result.semantic_similarity.toFixed(3)}</p>
        </div>
      )}
    </div>
  );
};
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœå’Œä»·å€?
### å¯?MVP ç½‘ç«™çš„ä»·å€?
1. **ä¸°å¯Œçš„å†…å®?* - æ¡ˆä¾‹ç ”ç©¶æä¾›äº†çœŸå®çš„ä½¿ç”¨åœºæ™¯
2. **ä¸“ä¸šçš„å¯è§†åŒ–** - é«˜è´¨é‡çš„å›¾è¡¨å¢å¼ºå¯ä¿¡åº?3. **äº¤äº’å¼ä½“éª?* - ç”¨æˆ·å¯ä»¥äº²è‡ªè¯•éªŒè¯­ä¹‰é‡å†™
4. **æ•™è‚²ä»·å€?* - æ•°æ®æ”¶é›†ç­–ç•¥å¸®åŠ©ç”¨æˆ·ç†è§£é—®é¢˜

### å¯¹ç”¨æˆ·çš„ä»·å€?
1. **ç†è§£é—®é¢˜** - é€šè¿‡æ¡ˆä¾‹ç ”ç©¶äº†è§£æ•°æ®æ³„éœ²çš„ä¸¥é‡æ€?2. **å­¦ä¹ æ–¹æ³•** - æŒæ¡æ•°æ®æ”¶é›†å’Œæ³„éœ²æ£€æµ‹çš„æŠ€å·?3. **å®é™…åº”ç”¨** - è·å¾—å¯å¤ç°çš„ä»£ç ç¤ºä¾‹
4. **å»ºç«‹ä¿¡ä»»** - çœ‹åˆ° CBD çš„å®é™…æ•ˆæœå’Œå•†ä¸šä»·å€?
---

## ğŸ§ª æµ‹è¯•éªŒè¯

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•æ•°æ®æ”¶é›†å™?python -c "from data.huggingface_data_collector import HuggingFaceDataCollector; c = HuggingFaceDataCollector(); print(c.create_dataset_inventory())"

# æµ‹è¯•è¯­ä¹‰é‡å†™
python -c "from examples.semantic_rewrite_leakage import SemanticRewriter; r = SemanticRewriter(); print(r.construct_leaked_pair('Test sentence.', 0.7))"

# æµ‹è¯•å¯è§†åŒ–ç”Ÿæˆ?python examples/generate_case_study_visualizations.py
```

### éªŒè¯è¾“å‡º

- [ ] æ‰€æœ‰è„šæœ¬æ— é”™è¯¯è¿è¡Œ
- [ ] ç”Ÿæˆçš?PNG å›¾è¡¨æ¸…æ™°å¯è¯»
- [ ] CSV æ•°æ®æ ¼å¼æ­£ç¡®
- [ ] æ–‡æ¡£é“¾æ¥æœ‰æ•ˆ

---

## ğŸ“š ç›¸å…³æ–‡æ¡£ç´¢å¼•

### å¿«é€Ÿå‚è€?- **å¿«é€Ÿå¯åŠ¨ï¼š** `QUICK_START_MVP_CONTENT.md`
- **å®Œæˆæ€»ç»“ï¼?* `IMPLEMENTATION_SUMMARY.md`

### è¯¦ç»†æ–‡æ¡£
- **æ•°æ®æ”¶é›†ï¼?* `docs/DATA_COLLECTION_STRATEGY.md`
- **æ¡ˆä¾‹ç ”ç©¶ï¼?* `docs/CASE_STUDY_CONTAMINATION_CRISIS.md`
- **å®æ–½æŒ‡å—ï¼?* `docs/MVP_CONTENT_IMPLEMENTATION_GUIDE.md`

### ä»£ç æ–‡æ¡£
- **æ•°æ®æ”¶é›†å™¨ï¼š** `data/huggingface_data_collector.py` (å†…å« docstrings)
- **è¯­ä¹‰é‡å†™ï¼?* `examples/semantic_rewrite_leakage.py` (å†…å« docstrings)
- **å¯è§†åŒ–ï¼š** `examples/generate_case_study_visualizations.py` (å†…å« docstrings)

---

## ğŸ”§ æŠ€æœ¯ä¾èµ?
### æ–°å¢ä¾èµ–

```bash
# Hugging Face æ•°æ®é›?pip install datasets

# å¯è§†åŒ–ï¼ˆå·²æœ‰ï¼?pip install matplotlib seaborn

# ç§‘å­¦è®¡ç®—ï¼ˆå·²æœ‰ï¼‰
pip install numpy pandas scikit-learn

# å¯é€‰ï¼šè¯­ä¹‰åµŒå…¥ï¼ˆç”¨äºé«˜çº§æ³„éœ²æ£€æµ‹ï¼‰
pip install sentence-transformers
```

### å…¼å®¹æ€?
- **Pythonï¼?* 3.8+
- **æ“ä½œç³»ç»Ÿï¼?* Windows / Linux / macOS
- **ç½‘ç»œï¼?* éœ€è¦äº’è”ç½‘è¿æ¥ï¼ˆç”¨äºä¸‹è½?HF æ•°æ®é›†ï¼‰

---

## ğŸ› å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡?
### é—®é¢˜ 1ï¼šHugging Face æ•°æ®é›†ä¸‹è½½æ…¢

**è§£å†³æ–¹æ¡ˆï¼?*
```bash
# ä½¿ç”¨é•œåƒ
set HF_ENDPOINT=https://hf-mirror.com
python data/huggingface_data_collector.py
```

### é—®é¢˜ 2ï¼šMatplotlib ä¸­æ–‡æ˜¾ç¤ºä¹±ç 

**è§£å†³æ–¹æ¡ˆï¼?*
```python
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # ä½¿ç”¨é»‘ä½“
plt.rcParams['axes.unicode_minus'] = False
```

### é—®é¢˜ 3ï¼šå†…å­˜ä¸è¶³ï¼ˆå¤§æ•°æ®é›†ï¼?
**è§£å†³æ–¹æ¡ˆï¼?*
```python
# ä½¿ç”¨æµå¼åŠ è½½
from datasets import load_dataset
dataset = load_dataset("dataset_id", streaming=True)
```

---

## ğŸ“ è·å–æ”¯æŒ

### æŸ¥çœ‹æ–‡æ¡£
```bash
# æ‰“å¼€å®æ–½æŒ‡å—
start docs/MVP_CONTENT_IMPLEMENTATION_GUIDE.md

# æŸ¥çœ‹æ¡ˆä¾‹ç ”ç©¶
start docs/CASE_STUDY_CONTAMINATION_CRISIS.md
```

### è”ç³»æ–¹å¼
- **é‚®ç®±ï¼?* yujjam@uest.edu.gr
- **GitHubï¼?* [æäº¤ Issue](https://github.com/hongping-zh/circular-bias-detection/issues)

---

## ğŸ‰ æ€»ç»“

âœ?**10 ä¸ªæ–°æ–‡ä»¶**å·²æˆåŠŸåˆ›å»? 
âœ?**ä¸‰ä¸ªå…³é”®æ­¥éª¤**å…¨éƒ¨å®æ–½å®Œæˆ  
âœ?**MVP å†…å®¹åŸºç¡€**å·²å‡†å¤‡å°±ç»? 
âœ?**å¯ç›´æ¥é›†æˆ?*åˆ°ç½‘ç«™å’Œåº”ç”¨

**ä¸‹ä¸€æ­¥ï¼š** è¿è¡Œ `python run_mvp_content_generation.py` å¼€å§‹ä½“éªŒï¼

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼?* v1.0  
**æœ€åæ›´æ–°ï¼š** 2024-10-27  
**ä½œè€…ï¼š** Hongping Zhang
