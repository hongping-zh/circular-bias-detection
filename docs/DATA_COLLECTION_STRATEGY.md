# æ•°æ®æ”¶é›†ç­–ç•¥ï¼šHugging Face æ•°æ®é›?
## ç›®æ ‡

å¯»æ‰¾é«˜é£é™©æ•°æ®é›†æˆ–æ˜“äºæ„é€ æ³„éœ²çš„åŸºçŸ³æ•°æ®é›†ï¼Œä»¥éªŒè¯?CBD æ¡†æ¶åœ¨æ£€æµ‹è®­ç»ƒæ•°æ®ä¸è¯„ä¼°æ•°æ®äº¤å‰æ±¡æŸ“æ–¹é¢çš„èƒ½åŠ›ã€?
---

## ç­–ç•¥ Aï¼šç›®æ ‡å¯»æ‰?äº¤å‰æ±¡æŸ“"çš„è¯„ä¼°åŸºå‡?
æˆ‘ä»¬å¯»æ‰¾é‚£äº›åŸºäºå¤§å‹é€šç”¨è¯­æ–™åº“ï¼ˆå¦?Wikipediaã€C4ï¼‰æ„å»ºï¼Œä¸”å­˜åœ¨é«˜é‡å é£é™©çš„è¯„ä¼°æ•°æ®é›†ã€?
### 1. æœºå™¨ç¿»è¯‘è¯„ä¼°é›?
**å…³é”®è¯?ç­›é€‰æ¡ä»¶ï¼š**
```
translation + en-zh
translation + wmt
translation + flores
```

**ç›®æ ‡æ•°æ®é›†ï¼š**
- `wmt14`, `wmt19` - WMT æœºå™¨ç¿»è¯‘ç«èµ›æ•°æ®é›?- `flores` - Facebook å¤šè¯­è¨€ç¿»è¯‘åŸºå‡†
- `iwslt2017` - å›½é™…å£è¯­ç¿»è¯‘ç ”è®¨ä¼šæ•°æ?
**æ½œåœ¨é£é™©ï¼?*
- å¥å­ç»“æ„é‡ç”¨ï¼Œç‰¹åˆ«æ˜¯ç»å…¸æˆ–å®˜æ–¹æœ¯è¯­çš„é‡ç”¨
- è®­ç»ƒè¯­æ–™ä¸è¯„ä¼°è¯­æ–™å¯èƒ½æ¥è‡ªç›¸åŒçš„æ–°é—»æºæˆ–ç»´åŸºç™¾ç§‘ç‰ˆæœ¬

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
- æ˜“äºæ£€æµ‹è¡¨é¢å’Œä¸­åº¦æ³„éœ²
- è¯­ä¹‰ç›¸ä¼¼åº¦é«˜ï¼Œé€‚åˆæµ‹è¯• CBD çš„è¯­ä¹‰é‡å†™æ£€æµ‹èƒ½åŠ?
**Hugging Face æŸ¥è¯¢å‘½ä»¤ï¼?*
```python
from datasets import load_dataset

# ç¤ºä¾‹ï¼šåŠ è½?WMT14 è‹±å¾·ç¿»è¯‘æ•°æ®é›?dataset = load_dataset("wmt14", "de-en")

# ç¤ºä¾‹ï¼šåŠ è½?FLORES-200 å¤šè¯­è¨€æ•°æ®é›?dataset = load_dataset("facebook/flores", "eng_Latn-zho_Hans")
```

---

### 2. æ‘˜è¦æ•°æ®é›?
**å…³é”®è¯?ç­›é€‰æ¡ä»¶ï¼š**
```
summarization + cnn_dailymail
summarization + xsum
abstractive summarization
```

**ç›®æ ‡æ•°æ®é›†ï¼š**
- `cnn_dailymail` - CNN/DailyMail æ–°é—»æ‘˜è¦
- `xsum` - BBC æ–°é—»æç«¯æ‘˜è¦
- `multi_news` - å¤šæ–‡æ¡£æ‘˜è¦?
**æ½œåœ¨é£é™©ï¼?*
- è¯„ä¼°é—®é¢˜ï¼ˆæç¤ºï¼‰ä¸æºæ–‡æ¡£ä¸­çš„å¥å­é«˜åº¦ç›¸ä¼¼
- è®­ç»ƒæ•°æ®ä¸è¯„ä¼°æ•°æ®çš„æºå¤´ç›¸åŒï¼ˆæ–°é—»æ–‡ç« ï¼‰ï¼Œé£é™©æé«?- æ¨¡å‹å¯èƒ½è®°å¿†äº†ç‰¹å®šæ–°é—»äº‹ä»¶çš„æè¿°

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
- æ‘˜è¦ä»»åŠ¡å¤©ç„¶å®¹æ˜“äº§ç”Ÿæ³„éœ²ï¼Œå› ä¸ºè¾“å…¥å’Œè¾“å‡ºéƒ½æ¥è‡ªåŒä¸€æ–‡æ¡£
- å¯ä»¥æµ‹è¯• CBD åœ¨æ£€æµ?éƒ¨åˆ†æ–‡æœ¬é‡å "æ–¹é¢çš„èƒ½åŠ?
**Hugging Face æŸ¥è¯¢å‘½ä»¤ï¼?*
```python
# CNN/DailyMail æ‘˜è¦æ•°æ®é›?dataset = load_dataset("cnn_dailymail", "3.0.0")

# XSum æç«¯æ‘˜è¦æ•°æ®é›?dataset = load_dataset("xsum")
```

---

### 3. å¼€æ”¾åŸŸé—®ç­”

**å…³é”®è¯?ç­›é€‰æ¡ä»¶ï¼š**
```
qa + open-domain
question-answering + wikipedia
natural questions
trivia qa
```

**ç›®æ ‡æ•°æ®é›†ï¼š**
- `natural_questions` - Google Natural Questions
- `trivia_qa` - TriviaQA é—®ç­”åŸºå‡†
- `squad` - Stanford Question Answering Dataset
- `hotpot_qa` - å¤šè·³æ¨ç†é—®ç­”

**æ½œåœ¨é£é™©ï¼?*
- é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆå¯èƒ½ç›´æ¥æ¥è‡?Wikipedia çš„æŸä¸ªç‰ˆæœ?- ä¸è®­ç»ƒæ•°æ®æºï¼ˆå¦‚ Common Crawlã€Wikipedia dumpsï¼‰é‡å æé«?- çŸ¥è¯†å‹æ³„éœ²çš„ç»å…¸æ¥æº

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
- å¼€æ”¾åŸŸé—®ç­”æ˜?LLM è¯„ä¼°çš„æ ¸å¿ƒåœºæ™?- å¯ä»¥ç›´æ¥æµ‹è¯• CBD å¯?çŸ¥è¯†è®°å¿†å‹æ³„éœ?çš„æ£€æµ‹èƒ½åŠ?
**Hugging Face æŸ¥è¯¢å‘½ä»¤ï¼?*
```python
# Natural Questions
dataset = load_dataset("natural_questions")

# TriviaQA
dataset = load_dataset("trivia_qa", "unfiltered")

# SQuAD v2.0
dataset = load_dataset("squad_v2")
```

---

### 4. æ£€ç´¢å¢å¼ºç”Ÿæˆ?(RAG) è¯„ä¼°é›?
**å…³é”®è¯?ç­›é€‰æ¡ä»¶ï¼š**
```
retrieval + wikipedia
rag + evaluation
ms_marco
```

**ç›®æ ‡æ•°æ®é›†ï¼š**
- `ms_marco` - Microsoft Machine Reading Comprehension
- `beir` - Benchmarking IR (ä¿¡æ¯æ£€ç´¢åŸºå‡?
- `wiki_qa` - Wikipedia é—®ç­”å¯?
**æ½œåœ¨é£é™©ï¼?*
- æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µæœ¬èº«å¯èƒ½å°±æ˜¯è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†
- ä¸Šä¸‹æ–‡æ³„éœ²ï¼šæ¨¡å‹å¯èƒ½å·²ç»"çœ‹è¿‡"æ£€ç´¢æ–‡æ¡?- æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨çš„åŒé‡æ³„éœ²é£é™?
**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
- éªŒè¯ CBD åœ?RAG åœºæ™¯ä¸‹çš„èƒ½åŠ›
- æµ‹è¯•å¤šé˜¶æ®µç³»ç»Ÿï¼ˆæ£€ç´?+ ç”Ÿæˆï¼‰çš„æ³„éœ²æ£€æµ?
**Hugging Face æŸ¥è¯¢å‘½ä»¤ï¼?*
```python
# MS MARCO
dataset = load_dataset("ms_marco", "v2.1")

# BEIR åŸºå‡†
# æ³¨æ„ï¼šBEIR éœ€è¦å•ç‹¬å®‰è£?beir åŒ?from beir import util, datasets as beir_datasets
dataset = "nfcorpus"
data_path = util.download_and_unzip(f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip", "datasets")
```

---

## ç­–ç•¥ Bï¼šè·å?è®­ç»ƒé›?ä»£è¡¨æ€§æ ·æœ?
ä¸ºäº†è¿›è¡ŒçœŸå®çš„äº¤å‰æ£€æµ‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸?*"é€šç”¨è®­ç»ƒé›?çš„ä»£è¡¨æ€§æ ·æœ?*ã€?
### 1. é€šç”¨çŸ¥è¯†è¯­æ–™

**ç›®æ ‡æ•°æ®ï¼šWikipedia**

**å»ºè®®æ¥æºï¼?*
- `wikipedia` (2022/2023 versions)
- ç‰¹å®šè¯­è¨€ç‰ˆæœ¬ï¼š`20220301.en`, `20230301.en`

**ç­›é€‰ç­–ç•¥ï¼š**
- ç­›é€‰å‡ºåŒ…å«ç‰¹å®šçŸ¥è¯†å®ä½“çš„æ–‡ç« ï¼ˆä¾‹å¦‚ï¼šå†å²äº‹ä»¶ã€åäººä¼ è®°ã€ç§‘å­¦æ¦‚å¿µï¼‰
- æŒ‰ç±»åˆ«é‡‡æ ·ï¼šç§‘æŠ€ã€å†å²ã€åœ°ç†ã€è‰ºæœ¯ç­‰
- æå–å…³é”®æ®µè½ï¼ˆè€Œéå…¨æ–‡ï¼‰ï¼Œæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®çš„ç‰‡æ®µåŒ–ç‰¹æ€?
**ç”¨é€”ï¼š**
- ä½œä¸ºæ¨¡æ‹Ÿçš?LLM è®­ç»ƒé›†è¾“å…?- æ„å»º"å·²çŸ¥æ³„éœ²"çš„åŸºå‡†æ•°æ®é›†

**Hugging Face æŸ¥è¯¢å‘½ä»¤ï¼?*
```python
# Wikipedia 2022å¹?æœˆç‰ˆæœ¬ï¼ˆè‹±æ–‡ï¼?dataset = load_dataset("wikipedia", "20220301.en")

# éšæœºé‡‡æ · 10,000 ä¸ªæ–‡æ¡?sampled_docs = dataset["train"].shuffle(seed=42).select(range(10000))
```

---

### 2. å¯¹è¯/ç½‘ç»œæ–‡æœ¬

**ç›®æ ‡æ•°æ®ï¼šC4 æˆ?The Pile**

**å»ºè®®æ¥æºï¼?*
- `c4` - Colossal Clean Crawled Corpus
- `the_pile` - EleutherAI çš„å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™ï¼ˆéœ€ç‰¹æ®Šè®¿é—®ï¼?- `openwebtext` - Reddit æå–çš„ç½‘ç»œæ–‡æœ?
**ç­›é€‰ç­–ç•¥ï¼š**
- éšæœºé‡‡æ · 10,000 ä¸ªæ–‡æ¡£ç‰‡æ®?- æŒ‰é¢†åŸŸåˆ†å±‚é‡‡æ ·ï¼šæ–°é—»ã€è®ºå›ã€åšå®¢ã€é—®ç­”ç½‘ç«?- ä¿ç•™åŸå§‹å™ªéŸ³ï¼ˆæ‹¼å†™é”™è¯¯ã€å£è¯­åŒ–è¡¨è¾¾ï¼‰ï¼Œä½¿æ¨¡æ‹Ÿæ›´çœŸå®

**ç”¨é€”ï¼š**
- å¢åŠ è®­ç»ƒé›†çš„å¤šæ ·æ€§å’Œå™ªéŸ³
- æ¨¡æ‹ŸçœŸå® LLM é¢„è®­ç»ƒè¯­æ–™çš„å¤šæ ·æ€?- æµ‹è¯• CBD åœ¨ä½è´¨é‡ã€é«˜å™ªéŸ³æ•°æ®ä¸Šçš„é²æ£’æ€?
**Hugging Face æŸ¥è¯¢å‘½ä»¤ï¼?*
```python
# C4 æ•°æ®é›†ï¼ˆen ç‰ˆæœ¬ï¼?dataset = load_dataset("c4", "en", streaming=True)

# é‡‡æ · 10,000 ä¸ªæ ·æœ¬ï¼ˆç”±äºæ•°æ®é›†å·¨å¤§ï¼Œä½¿ç”¨æµå¼åŠ è½½ï¼?sampled = []
for i, item in enumerate(dataset["train"]):
    if i >= 10000:
        break
    sampled.append(item)

# OpenWebText
dataset = load_dataset("openwebtext")
sampled_docs = dataset["train"].shuffle(seed=42).select(range(10000))
```

---

## å¯å®æ–½çš„ä¸‹ä¸€æ­¥è¡ŒåŠ?
### æ­¥éª¤ 1ï¼šåˆæ­¥æ•°æ®æœç´¢ä¸ä¸‹è½½
```python
# ä½¿ç”¨ datasets åº“æœç´¢å…³é”®è¯
from datasets import list_datasets

# æœç´¢ç›¸å…³æ•°æ®é›?translation_datasets = [d for d in list_datasets() if "translation" in d.lower()]
qa_datasets = [d for d in list_datasets() if "qa" in d.lower() or "question" in d.lower()]
summarization_datasets = [d for d in list_datasets() if "summarization" in d.lower() or "summary" in d.lower()]

print("Translation datasets:", translation_datasets[:10])
print("QA datasets:", qa_datasets[:10])
print("Summarization datasets:", summarization_datasets[:10])
```

### æ­¥éª¤ 2ï¼šä¸‹è½?2-3 ä¸ªå°å‹æ•°æ®é›†è¿›è¡Œåˆç­›
```python
# ä¸‹è½½ä¼˜å…ˆçº§åˆ—è¡¨ï¼ˆæŒ‰æ˜“ç”¨æ€§å’Œä»£è¡¨æ€§æ’åºï¼‰
priority_datasets = [
    ("squad_v2", "é—®ç­”", "é«?),
    ("cnn_dailymail", "æ‘˜è¦", "é«?),
    ("wmt14", "ç¿»è¯‘", "ä¸?),
    ("natural_questions", "é—®ç­”", "é«?),
    ("wikipedia", "è®­ç»ƒé›†ä»£è¡?, "é«?)
]

# ç¤ºä¾‹ï¼šä¸‹è½?SQuAD v2.0
from datasets import load_dataset
squad = load_dataset("squad_v2")
print(f"SQuAD v2 train size: {len(squad['train'])}")
print(f"Sample: {squad['train'][0]}")
```

### æ­¥éª¤ 3ï¼šæ•°æ®é¢„å¤„ç†ä¸æ ¼å¼åŒ–
```python
# å°†ä¸‹è½½çš„æ•°æ®é›†è½¬æ¢ä¸º CBD æ¡†æ¶æ‰€éœ€çš„æ ¼å¼?# æå–ä¸‰å…ƒç»„ï¼š(è®­ç»ƒæ ·æœ¬, è¯„ä¼°é—®é¢˜, æ ‡ç­¾)

def extract_qa_pairs(dataset, split="train", max_samples=1000):
    """ä»é—®ç­”æ•°æ®é›†æå–æ ·æœ¬"""
    samples = []
    for i, item in enumerate(dataset[split]):
        if i >= max_samples:
            break
        samples.append({
            "context": item["context"],
            "question": item["question"],
            "answers": item["answers"]
        })
    return samples

# ç¤ºä¾‹ä½¿ç”¨
qa_pairs = extract_qa_pairs(squad, split="validation", max_samples=500)
```

---

## æ•°æ®è´¨é‡è¯„ä¼°æ ‡å‡†

åœ¨ä¸‹è½½å’Œä½¿ç”¨æ•°æ®é›†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°ä»¥ä¸‹æ ‡å‡†ï¼š

| æ ‡å‡† | æè¿° | è¯„åˆ†æ–¹æ³• |
|------|------|----------|
| **ç›¸å…³æ€?* | æ•°æ®é›†ä¸"äº¤å‰æ±¡æŸ“"ä¸»é¢˜çš„ç›¸å…³åº¦ | é«?ä¸?ä½?|
| **æ•°æ®é‡?* | æ•°æ®é›†å¤§å°æ˜¯å¦é€‚åˆå®éªŒ | < 10K: å°? 10K-100K: ä¸? > 100K: å¤?|
| **ä¸‹è½½éš¾åº¦** | æ˜¯å¦æ˜“äºé€šè¿‡ HF datasets ä¸‹è½½ | æ˜?ä¸?éš?|
| **æ–‡æ¡£è´¨é‡** | æ•°æ®é›†æ˜¯å¦æœ‰æ¸…æ™°çš„æ–‡æ¡£è¯´æ˜?| å¥?ä¸€èˆ?å·?|
| **è®¸å¯è¯?* | æ•°æ®é›†çš„ä½¿ç”¨è®¸å¯ | å¼€æ”?é™åˆ¶/æœªçŸ¥ |

---

## é¢„æœŸè¾“å‡º

å®Œæˆæ•°æ®æ”¶é›†åï¼Œæˆ‘ä»¬å°†å¾—åˆ°ï¼š

1. **é«˜é£é™©è¯„ä¼°æ•°æ®é›†åˆ—è¡¨** (5-10 ä¸ªæ•°æ®é›†)
   - æ¯ä¸ªæ•°æ®é›†åŒ…å«å…ƒä¿¡æ¯ï¼šåç§°ã€å¤§å°ã€ç”¨é€”ã€é£é™©ç­‰çº?   
2. **è®­ç»ƒé›†ä»£è¡¨æ€§æ ·æœ?* (10,000-50,000 ä¸ªæ–‡æ¡?
   - Wikipedia æ–‡ç« ç‰‡æ®µ
   - C4/OpenWebText ç½‘ç»œæ–‡æœ¬ç‰‡æ®µ

3. **æ•°æ®é›†ç›®å½•æ–‡ä»?* (`DATASET_INVENTORY.csv`)
   ```csv
   dataset_name,task_type,size,risk_level,hf_id,notes
   squad_v2,qa,150K,high,squad_v2,Wikipedia-based QA
   cnn_dailymail,summarization,300K,high,cnn_dailymail,News summarization
   wmt14,translation,4.5M,medium,wmt14,Machine translation
   ...
   ```

4. **åˆæ­¥æ³„éœ²æ£€æµ‹æŠ¥å‘?*
   - å¯¹ä¸‹è½½çš„æ•°æ®é›†è¿è¡?CBD æ¡†æ¶çš„åˆæ­¥åˆ†æ?   - è¯†åˆ«å“ªäº›æ•°æ®é›†æœ€é€‚åˆæ„å»ºæ¡ˆä¾‹ç ”ç©¶

---

## æ—¶é—´è¡?
- **ç¬?1 å¤©ï¼š** æ•°æ®é›†æœç´¢ä¸è¯„ä¼°ï¼ˆå®Œæˆæ•°æ®é›†ç›®å½•ï¼?- **ç¬?2-3 å¤©ï¼š** ä¸‹è½½å’Œé¢„å¤„ç†ä¼˜å…ˆçº§æ•°æ®é›†
- **ç¬?4-5 å¤©ï¼š** æ„å»º"è®­ç»ƒé›†ä»£è¡¨æ€§æ ·æœ?
- **ç¬?6-7 å¤©ï¼š** åˆæ­¥æ³„éœ²æ£€æµ‹å®éªŒï¼Œç­›é€‰æœ€ä½³æ¡ˆä¾?
---

## å‚è€ƒèµ„æº?
### Hugging Face æ•°æ®é›†æœç´¢é¡µé?- https://huggingface.co/datasets

### å…³é”®æ•°æ®é›†é“¾æ?- SQuAD v2: https://huggingface.co/datasets/squad_v2
- CNN/DailyMail: https://huggingface.co/datasets/cnn_dailymail
- Natural Questions: https://huggingface.co/datasets/natural_questions
- Wikipedia: https://huggingface.co/datasets/wikipedia
- C4: https://huggingface.co/datasets/c4

---

**æœ€åæ›´æ–°ï¼š** 2024-10-27
**è´Ÿè´£äººï¼š** Hongping Zhang
**çŠ¶æ€ï¼š** å¾…å®æ–?
