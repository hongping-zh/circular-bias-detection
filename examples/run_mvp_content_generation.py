"""
MVP å†…å®¹ç”Ÿæˆä¸»è„šæœ¬

ä¸€é”®è¿è¡Œæ‰€æœ‰ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼š
1. æ•°æ®æ”¶é›†ï¼ˆHugging Faceï¼‰
2. è¯­ä¹‰é‡å†™æ„é€ æ³„éœ²
3. æ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ–ç”Ÿæˆ

ä½œè€…: Hongping Zhang
æ—¥æœŸ: 2024-10-27
"""

import os
import sys
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

print("=" * 80)
print("CBD MVP å†…å®¹ç”Ÿæˆå™¨")
print("=" * 80)
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 80)


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


def run_step_1_data_collection():
    """æ­¥éª¤ 1: æ•°æ®æ”¶é›†"""
    print_section("æ­¥éª¤ 1/3: Hugging Face æ•°æ®æ”¶é›†")
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å®‰è£… datasets
        try:
            import datasets
            print("âœ“ datasets åº“å·²å®‰è£…")
        except ImportError:
            print("âš ï¸  æœªå®‰è£… datasets åº“ï¼Œæ­£åœ¨å®‰è£…...")
            os.system("pip install datasets")
        
        # å¯¼å…¥æ•°æ®æ”¶é›†å™¨
        from data.huggingface_data_collector import HuggingFaceDataCollector
        
        # åˆå§‹åŒ–æ”¶é›†å™¨
        collector = HuggingFaceDataCollector(output_dir="./mvp_collected_data")
        
        print("\n[1.1] åˆ›å»ºæ•°æ®é›†æ¸…å•...")
        inventory = collector.create_dataset_inventory()
        print(f"âœ“ åˆ›å»ºäº† {len(inventory)} ä¸ªæ•°æ®é›†çš„æ¸…å•")
        
        print("\n[1.2] æœç´¢ç›¸å…³æ•°æ®é›†...")
        qa_datasets = collector.search_datasets_by_keyword(
            keywords=["question", "answering", "qa"],
            limit=10
        )
        print(f"âœ“ æ‰¾åˆ° {len(qa_datasets)} ä¸ªé—®ç­”æ•°æ®é›†")
        
        print("\n[1.3] æ”¶é›†ä¼˜å…ˆçº§æ•°æ®é›†ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰...")
        print("âš ï¸  æ³¨æ„: æ¼”ç¤ºæ¨¡å¼åªä¸‹è½½å°‘é‡æ ·æœ¬ï¼Œå®Œæ•´æ”¶é›†è¯·ä¿®æ”¹å‚æ•°")
        
        # æ¼”ç¤ºæ¨¡å¼ï¼šæ¯ä¸ªæ•°æ®é›†åªä¸‹è½½ 50 ä¸ªæ ·æœ¬
        collected = collector.collect_all_priority_datasets(
            max_samples_per_dataset=50,
            save_format="csv"
        )
        
        print(f"\nâœ“ æˆåŠŸæ”¶é›† {len(collected)} ä¸ªæ•°æ®é›†")
        
        print("\n[1.4] ç”Ÿæˆæ”¶é›†æŠ¥å‘Š...")
        report = collector.generate_collection_report(collected)
        print("\n" + report)
        
        print("\nâœ… æ­¥éª¤ 1 å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ­¥éª¤ 1 å¤±è´¥: {str(e)}")
        print("æç¤º: å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¯·æ£€æŸ¥ Hugging Face è¿æ¥æˆ–è®¾ç½®ä»£ç†")
        return False


def run_step_2_semantic_rewrite():
    """æ­¥éª¤ 2: è¯­ä¹‰é‡å†™æ„é€ æ³„éœ²"""
    print_section("æ­¥éª¤ 2/3: è¯­ä¹‰é‡å†™æ„é€ æ³„éœ²")
    
    try:
        from examples.semantic_rewrite_leakage import (
            SemanticRewriter, 
            LeakageSimulator,
            demonstrate_leakage_construction
        )
        
        print("[2.1] åˆå§‹åŒ–è¯­ä¹‰é‡å†™å™¨...")
        rewriter = SemanticRewriter()
        simulator = LeakageSimulator()
        print("âœ“ é‡å†™å™¨åˆå§‹åŒ–å®Œæˆ")
        
        print("\n[2.2] æ„é€ å•ä¸ªæ³„éœ²å¯¹ç¤ºä¾‹...")
        train_text = "The Statue of Liberty was a gift from the people of France to the people of the United States."
        pair = rewriter.construct_leaked_pair(train_text, leakage_intensity=0.8)
        
        print(f"\nè®­ç»ƒæ•°æ®:\n  {pair.train_text[:80]}...")
        print(f"\næ³„éœ²è¯„ä¼°é—®é¢˜:\n  {pair.eval_question}")
        print(f"\nè¯­ä¹‰ç›¸ä¼¼åº¦: {pair.semantic_similarity:.3f}")
        print(f"è¡¨é¢ç›¸ä¼¼åº¦: {pair.surface_similarity:.3f}")
        print(f"é¢„æœŸ C_score: {pair.semantic_similarity:.3f} {'ğŸ”´ CRITICAL' if pair.semantic_similarity >= 0.75 else 'ğŸŸ¡ HIGH'}")
        
        print("\n[2.3] æ‰¹é‡æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†...")
        df_leaked = simulator.simulate_leakage_dataset(
            num_samples=200,
            leakage_ratio=0.4,
            leakage_intensity=0.75
        )
        
        print(f"âœ“ ç”Ÿæˆäº† {len(df_leaked)} ä¸ªæ ·æœ¬")
        
        print("\n[2.4] åˆ†ææ³„éœ²åˆ†å¸ƒ...")
        analysis = simulator.analyze_leakage_distribution(df_leaked)
        
        print(f"\næ³„éœ²åˆ†æç»“æœ:")
        print(f"  æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
        print(f"  æ³„éœ²æ ·æœ¬: {analysis['leaked_samples']} ({analysis['leakage_ratio']:.1%})")
        print(f"  å¹²å‡€æ ·æœ¬: {analysis['clean_samples']}")
        print(f"  é«˜é£é™©æ ·æœ¬ (C_score > 0.75): {analysis['high_risk_samples']}")
        
        print("\n[2.5] ä¿å­˜æ³„éœ²æ•°æ®é›†...")
        output_path = "./mvp_leaked_dataset.csv"
        df_leaked.to_csv(output_path, index=False)
        print(f"âœ“ å·²ä¿å­˜åˆ°: {output_path}")
        
        print("\nâœ… æ­¥éª¤ 2 å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ­¥éª¤ 2 å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def run_step_3_visualizations():
    """æ­¥éª¤ 3: æ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ–"""
    print_section("æ­¥éª¤ 3/3: æ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ–ç”Ÿæˆ")
    
    try:
        # æ£€æŸ¥å¿…è¦çš„åº“
        required_packages = ['matplotlib', 'seaborn', 'numpy', 'pandas']
        for package in required_packages:
            try:
                __import__(package)
                print(f"âœ“ {package} å·²å®‰è£…")
            except ImportError:
                print(f"âš ï¸  æ­£åœ¨å®‰è£… {package}...")
                os.system(f"pip install {package}")
        
        import numpy as np
        from examples.generate_case_study_visualizations import (
            CaseStudyVisualizer,
            generate_synthetic_data
        )
        
        print("\n[3.1] åˆå§‹åŒ–å¯è§†åŒ–å™¨...")
        visualizer = CaseStudyVisualizer(output_dir="./mvp_case_study_figures")
        print("âœ“ å¯è§†åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        
        print("\n[3.2] ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
        df_contamination = generate_synthetic_data(n_samples=10000, seed=42)
        print(f"âœ“ ç”Ÿæˆäº† {len(df_contamination)} ä¸ªæ ·æœ¬")
        
        print("\né£é™©åˆ†å¸ƒ:")
        risk_counts = df_contamination['risk_level'].value_counts().sort_index()
        for risk, count in risk_counts.items():
            percentage = count / len(df_contamination) * 100
            print(f"  {risk}: {count:,} ({percentage:.1f}%)")
        
        print("\n[3.3] ç”Ÿæˆå›¾è¡¨ 1: åå·®åˆ†æ•°åˆ†å¸ƒå›¾...")
        visualizer.generate_contamination_risk_map(
            c_scores=df_contamination['c_score'].values,
            save_path="contamination_risk_map.png"
        )
        
        print("\n[3.4] ç”Ÿæˆå›¾è¡¨ 2: æ€§èƒ½ä¿®æ­£å¯¹æ¯”å›¾...")
        visualizer.generate_performance_reality_check(
            original_acc=95.1,
            corrected_acc=58.3,
            save_path="performance_reality_check.png"
        )
        
        print("\n[3.5] ç”Ÿæˆå›¾è¡¨ 3: æ³„éœ²ç±»å‹åˆ†å¸ƒå›¾...")
        leakage_types = {
            'Exact Match': 120,
            'Paraphrase': 850,
            'Partial Overlap': 1530,
            'Semantic Similar': 1500
        }
        visualizer.generate_leakage_type_distribution(
            leakage_types=leakage_types,
            save_path="leakage_type_distribution.png"
        )
        
        print("\n[3.6] ç”Ÿæˆå›¾è¡¨ 4: æ ·æœ¬æ±¡æŸ“çƒ­åŠ›å›¾...")
        n_eval = 50
        n_train = 50
        c_scores_matrix = np.random.beta(2, 5, (n_eval, n_train))
        # æ·»åŠ ä¸€äº›é«˜é£é™©æ ·æœ¬
        c_scores_matrix[5:10, 10:15] = np.random.uniform(0.7, 0.9, (5, 5))
        c_scores_matrix[20:25, 30:35] = np.random.uniform(0.75, 0.95, (5, 5))
        
        visualizer.generate_sample_heatmap(
            c_scores_matrix=c_scores_matrix,
            sample_ids=list(range(n_eval)),
            train_ids=list(range(n_train)),
            save_path="sample_contamination_heatmap.png"
        )
        
        print("\n[3.7] ä¿å­˜æ¨¡æ‹Ÿæ•°æ®...")
        data_path = os.path.join(visualizer.output_dir, "contamination_data.csv")
        df_contamination.to_csv(data_path, index=False)
        print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {data_path}")
        
        print("\nâœ… æ­¥éª¤ 3 å®Œæˆï¼")
        print(f"\næ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {visualizer.output_dir}")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ­¥éª¤ 3 å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def run_step_4_performance_benchmark():
    """æ­¥éª¤ 4: æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print_section("æ­¥éª¤ 4/4: æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    try:
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_path = "./mvp_case_study_figures/contamination_data.csv"
        if not os.path.exists(data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            print("è¯·å…ˆè¿è¡Œæ­¥éª¤ 3ï¼ˆå¯è§†åŒ–ç”Ÿæˆï¼‰æ¥åˆ›å»ºæ•°æ®æ–‡ä»¶")
            return False
        
        from examples.performance_benchmark import CBDPerformanceBenchmark
        
        print("[4.1] åˆå§‹åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨...")
        benchmark = CBDPerformanceBenchmark(data_path)
        
        print("\n[4.2] åŠ è½½æµ‹è¯•æ•°æ®...")
        benchmark.load_data()
        
        print("\n[4.3] è¿è¡Œ CBD æ±¡æŸ“æ£€æµ‹...")
        benchmark.simulate_cbd_detection(batch_size=100)
        
        print("\n[4.4] åˆ†ææ€§èƒ½æŒ‡æ ‡...")
        benchmark.analyze_performance()
        
        print("\n[4.5] ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        benchmark.generate_report(output_path="./CBD_PERFORMANCE_REPORT.md")
        
        # ç”Ÿæˆç®€æ´æ‘˜è¦
        print("\n[4.6] ç”Ÿæˆç®€æ´æ‘˜è¦...")
        with open("./CBD_PERFORMANCE_SUMMARY.md", 'w', encoding='utf-8') as f:
            f.write(f"""# CBD æ€§èƒ½æŠ¥å‘Š - æ‰§è¡Œæ‘˜è¦

> **ä¸€å¥è¯æ€»ç»“ï¼š** CBD å¯åœ¨ **{benchmark.results['detection_time']:.2f} ç§’**å†…å®Œæˆ 10,000 ä¸ªæ ·æœ¬çš„æ±¡æŸ“æ£€æµ‹ï¼Œååé‡è¾¾ **{benchmark.results['throughput']:.0f} æ ·æœ¬/ç§’**

## âš¡ æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ£€æµ‹ 10k æ ·æœ¬è€—æ—¶** | **{benchmark.results['detection_time']:.2f} ç§’** |
| **ååé‡** | **{benchmark.results['throughput']:.0f} æ ·æœ¬/ç§’** |
| **å•æ ·æœ¬å¤„ç†æ—¶é—´** | **{benchmark.results['avg_time_per_sample']:.3f} æ¯«ç§’** |
| **æ€§èƒ½è¯„çº§** | **â­â­â­â­â­ ä¼˜ç§€** |

å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹ï¼š[CBD_PERFORMANCE_REPORT.md](CBD_PERFORMANCE_REPORT.md)
""")
        print("âœ“ ç®€æ´æ‘˜è¦å·²ä¿å­˜: ./CBD_PERFORMANCE_SUMMARY.md")
        
        print("\nâœ… æ­¥éª¤ 4 å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ­¥éª¤ 4 å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def print_summary(results):
    """æ‰“å°æ‰§è¡Œæ€»ç»“"""
    print("\n" + "=" * 80)
    print("æ‰§è¡Œæ€»ç»“")
    print("=" * 80)
    
    print("\nä»»åŠ¡å®ŒæˆçŠ¶æ€:")
    steps = [
        ("æ­¥éª¤ 1: æ•°æ®æ”¶é›†", results['step1']),
        ("æ­¥éª¤ 2: è¯­ä¹‰é‡å†™", results['step2']),
        ("æ­¥éª¤ 3: å¯è§†åŒ–ç”Ÿæˆ", results['step3']),
        ("æ­¥éª¤ 4: æ€§èƒ½åŸºå‡†æµ‹è¯•", results['step4'])
    ]
    
    for step_name, status in steps:
        status_icon = "âœ…" if status else "âŒ"
        print(f"  {status_icon} {step_name}")
    
    success_count = sum(results.values())
    print(f"\næ€»è®¡: {success_count}/4 ä¸ªæ­¥éª¤æˆåŠŸå®Œæˆ")
    
    if success_count == 4:
        print("\nğŸ‰ æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆï¼")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        print("  ğŸ“ ./mvp_collected_data/ - æ”¶é›†çš„æ•°æ®é›†")
        print("  ğŸ“„ ./mvp_leaked_dataset.csv - æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†")
        print("  ğŸ“ ./mvp_case_study_figures/ - æ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ–")
        print("  ğŸ“„ ./CBD_PERFORMANCE_REPORT.md - è¯¦ç»†æ€§èƒ½æŠ¥å‘Š")
        print("  ğŸ“„ ./CBD_PERFORMANCE_SUMMARY.md - æ€§èƒ½æ‰§è¡Œæ‘˜è¦")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨ï¼ˆPNG æ–‡ä»¶ï¼‰")
        print("  2. é˜…è¯»æ€§èƒ½æŠ¥å‘Š: CBD_PERFORMANCE_SUMMARY.md")
        print("  3. é˜…è¯»å®æ–½æŒ‡å—: docs/MVP_CONTENT_IMPLEMENTATION_GUIDE.md")
        print("  4. å°†å†…å®¹é›†æˆåˆ° MVP ç½‘ç«™")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ­¥éª¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
        print("\næ•…éšœæ’é™¤:")
        print("  - æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆHugging Face æ•°æ®é›†ä¸‹è½½ï¼‰")
        print("  - ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
        print("  - æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    results = {
        'step1': False,
        'step2': False,
        'step3': False,
        'step4': False
    }
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¿è¡Œæ¯ä¸ªæ­¥éª¤
    print("\nè¯·é€‰æ‹©è¦è¿è¡Œçš„æ­¥éª¤:")
    print("1. æ•°æ®æ”¶é›†ï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼Œè¾ƒæ…¢ï¼‰")
    print("2. è¯­ä¹‰é‡å†™æ„é€ æ³„éœ²ï¼ˆå¿«é€Ÿï¼‰")
    print("3. å¯è§†åŒ–ç”Ÿæˆï¼ˆå¿«é€Ÿï¼‰")
    print("4. æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆå¿«é€Ÿï¼‰")
    print("5. è¿è¡Œæ‰€æœ‰æ­¥éª¤")
    print("0. é€€å‡º")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-5): ").strip()
    
    if choice == '0':
        print("é€€å‡ºç¨‹åº")
        return
    elif choice == '1':
        results['step1'] = run_step_1_data_collection()
    elif choice == '2':
        results['step2'] = run_step_2_semantic_rewrite()
    elif choice == '3':
        results['step3'] = run_step_3_visualizations()
    elif choice == '4':
        results['step4'] = run_step_4_performance_benchmark()
    elif choice == '5':
        print("\nè¿è¡Œæ‰€æœ‰æ­¥éª¤...\n")
        
        # æ­¥éª¤ 1: æ•°æ®æ”¶é›†ï¼ˆå¯é€‰è·³è¿‡ï¼‰
        skip_step1 = input("æ­¥éª¤ 1ï¼ˆæ•°æ®æ”¶é›†ï¼‰éœ€è¦ç½‘ç»œè¿æ¥ä¸”è¾ƒæ…¢ï¼Œæ˜¯å¦è·³è¿‡ï¼Ÿ(y/n): ").strip().lower()
        if skip_step1 != 'y':
            results['step1'] = run_step_1_data_collection()
        else:
            print("\nâ­ï¸  è·³è¿‡æ­¥éª¤ 1")
        
        # æ­¥éª¤ 2: è¯­ä¹‰é‡å†™
        results['step2'] = run_step_2_semantic_rewrite()
        
        # æ­¥éª¤ 3: å¯è§†åŒ–
        results['step3'] = run_step_3_visualizations()
        
        # æ­¥éª¤ 4: æ€§èƒ½æµ‹è¯•
        results['step4'] = run_step_4_performance_benchmark()
    else:
        print("æ— æ•ˆçš„é€‰é¡¹")
        return
    
    # æ‰“å°æ€»ç»“
    elapsed_time = time.time() - start_time
    print_summary(results)
    
    print(f"\næ€»è€—æ—¶: {elapsed_time:.1f} ç§’")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
