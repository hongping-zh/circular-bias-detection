{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸš€ CBD GPU åŠ é€ŸéªŒè¯ - Google Colab\n",
    "\n",
    "æœ¬ Notebook ç”¨äºåœ¨ Google Colab å…è´¹ GPU ä¸ŠéªŒè¯ CBD (Circular Bias Detection) çš„æ€§èƒ½æå‡ã€‚\n",
    "\n",
    "**é¢„æœŸç»“æœï¼š**\n",
    "- GPU (T4): 10k æ ·æœ¬ ~0.025 ç§’ (åŠ é€Ÿæ¯” 13x)\n",
    "- ååé‡: ~400k æ ·æœ¬/ç§’\n",
    "\n",
    "**ä½¿ç”¨æ­¥éª¤ï¼š**\n",
    "1. å¯ç”¨ GPU: èœå• â†’ è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU\n",
    "2. ä¾æ¬¡è¿è¡Œæ¯ä¸ª Cell\n",
    "3. æŸ¥çœ‹æ€§èƒ½æµ‹è¯•ç»“æœ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## ğŸ“‹ æ­¥éª¤ 1: éªŒè¯ GPU ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU ç¯å¢ƒæ£€æŸ¥\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æ£€æŸ¥ CUDA\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nâœ“ CUDA å¯ç”¨: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"âœ“ GPU å‹å·: {gpu_name}\")\n",
    "    print(f\"âœ“ VRAM: {gpu_memory:.1f} GB\")\n",
    "    print(f\"âœ“ CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  è­¦å‘Š: GPU æœªå¯ç”¨ï¼\")\n",
    "    print(\"è¯·ç‚¹å‡»: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ ç¡¬ä»¶åŠ é€Ÿå™¨ â†’ GPU\")\n",
    "    print(\"ç„¶åé‡æ–°è¿è¡Œæ­¤ Cell\")\n",
    "\n",
    "print(f\"\\nâœ“ Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "print(f\"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## ğŸ“¦ æ­¥éª¤ 2: å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "print(\"å®‰è£… CBD ä¾èµ–åŒ…...\\n\")\n",
    "\n",
    "# å®‰è£…å¿…è¦çš„åŒ…\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## ğŸ’» æ­¥éª¤ 3: å®šä¹‰ CBD GPU æ£€æµ‹å™¨\n",
    "\n",
    "è¿™æ˜¯ CBD çš„æ ¸å¿ƒä»£ç ï¼ŒåŒ…å« GPU åŠ é€Ÿçš„æ±¡æŸ“æ£€æµ‹åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbd_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPUConfig:\n",
    "    \"\"\"GPU é…ç½®\"\"\"\n",
    "    device: str = \"cuda\"\n",
    "    batch_size: int = 512\n",
    "    use_fp16: bool = True\n",
    "    model_name: str = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "class CBDDetectorGPU:\n",
    "    \"\"\"GPU åŠ é€Ÿçš„ CBD æ£€æµ‹å™¨ (ç®€åŒ–ç‰ˆ)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPUConfig = None):\n",
    "        self.config = config or GPUConfig()\n",
    "        \n",
    "        # æ£€æŸ¥ GPU\n",
    "        if self.config.device == \"cuda\" and not torch.cuda.is_available():\n",
    "            print(\"âš ï¸  GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU\")\n",
    "            self.config.device = \"cpu\"\n",
    "        \n",
    "        print(f\"\\nåˆå§‹åŒ– CBD æ£€æµ‹å™¨...\")\n",
    "        print(f\"  è®¾å¤‡: {self.config.device}\")\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        self.model = SentenceTransformer(self.config.model_name)\n",
    "        self.model = self.model.to(self.config.device)\n",
    "        \n",
    "        if self.config.use_fp16 and self.config.device == \"cuda\":\n",
    "            self.model = self.model.half()\n",
    "            print(f\"  ç²¾åº¦: FP16\")\n",
    "        \n",
    "        if self.config.device == \"cuda\":\n",
    "            print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"  æ‰¹å¤„ç†: {self.config.batch_size}\")\n",
    "        \n",
    "        print(f\"âœ“ åˆå§‹åŒ–å®Œæˆ\\n\")\n",
    "    \n",
    "    def compute_embeddings_batch(self, texts: List[str], desc: str = \"\") -> torch.Tensor:\n",
    "        \"\"\"æ‰¹é‡è®¡ç®—åµŒå…¥\"\"\"\n",
    "        embeddings = []\n",
    "        batch_size = self.config.batch_size\n",
    "        n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(n_batches):\n",
    "                batch_start = i * batch_size\n",
    "                batch_end = min((i + 1) * batch_size, len(texts))\n",
    "                batch = texts[batch_start:batch_end]\n",
    "                \n",
    "                batch_emb = self.model.encode(\n",
    "                    batch,\n",
    "                    convert_to_tensor=True,\n",
    "                    device=self.config.device,\n",
    "                    show_progress_bar=False\n",
    "                )\n",
    "                embeddings.append(batch_emb)\n",
    "                \n",
    "                if (i + 1) % 10 == 0 or (i + 1) == n_batches:\n",
    "                    progress = (i + 1) / n_batches * 100\n",
    "                    print(f\"  {desc}: {progress:5.1f}%\", end='\\r')\n",
    "        \n",
    "        print()  # æ¢è¡Œ\n",
    "        return torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    def detect_contamination(self, train_texts: List[str], eval_texts: List[str], \n",
    "                            threshold: float = 0.75) -> Dict:\n",
    "        \"\"\"æ£€æµ‹æ±¡æŸ“\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"CBD GPU æ£€æµ‹\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"è®­ç»ƒé›†: {len(train_texts):,} æ ·æœ¬\")\n",
    "        print(f\"è¯„ä¼°é›†: {len(eval_texts):,} æ ·æœ¬\")\n",
    "        print(f\"è®¾å¤‡: {self.config.device}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # è®¡ç®—åµŒå…¥\n",
    "        print(\"[1/3] è®¡ç®—è®­ç»ƒé›†åµŒå…¥...\")\n",
    "        t1 = time.time()\n",
    "        train_emb = self.compute_embeddings_batch(train_texts, \"è®­ç»ƒé›†\")\n",
    "        print(f\"âœ“ å®Œæˆ ({time.time()-t1:.2f}s)\\n\")\n",
    "        \n",
    "        print(\"[2/3] è®¡ç®—è¯„ä¼°é›†åµŒå…¥...\")\n",
    "        t2 = time.time()\n",
    "        eval_emb = self.compute_embeddings_batch(eval_texts, \"è¯„ä¼°é›†\")\n",
    "        print(f\"âœ“ å®Œæˆ ({time.time()-t2:.2f}s)\\n\")\n",
    "        \n",
    "        # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "        print(\"[3/3] è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...\")\n",
    "        t3 = time.time()\n",
    "        train_norm = train_emb / train_emb.norm(dim=1, keepdim=True)\n",
    "        eval_norm = eval_emb / eval_emb.norm(dim=1, keepdim=True)\n",
    "        similarity = torch.mm(eval_norm, train_norm.T)\n",
    "        c_scores = similarity.max(dim=1).values.cpu().numpy()\n",
    "        print(f\"âœ“ å®Œæˆ ({time.time()-t3:.2f}s)\\n\")\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        total_time = time.time() - start_time\n",
    "        contaminated = (c_scores >= threshold).sum()\n",
    "        throughput = (len(train_texts) + len(eval_texts)) / total_time\n",
    "        \n",
    "        # é£é™©åˆ†å¸ƒ\n",
    "        critical = (c_scores >= 0.75).sum()\n",
    "        high = ((c_scores >= 0.50) & (c_scores < 0.75)).sum()\n",
    "        medium = ((c_scores >= 0.30) & (c_scores < 0.50)).sum()\n",
    "        low = (c_scores < 0.30).sum()\n",
    "        \n",
    "        results = {\n",
    "            'total_time': total_time,\n",
    "            'throughput': throughput,\n",
    "            'contaminated': int(contaminated),\n",
    "            'contamination_rate': contaminated / len(eval_texts),\n",
    "            'c_scores': c_scores,\n",
    "            'risk_dist': {'critical': int(critical), 'high': int(high), \n",
    "                         'medium': int(medium), 'low': int(low)}\n",
    "        }\n",
    "        \n",
    "        # æ‰“å°ç»“æœ\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"æ£€æµ‹å®Œæˆ\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nâš¡ æ€§èƒ½:\")\n",
    "        print(f\"  æ€»æ—¶é—´: {total_time:.3f} ç§’\")\n",
    "        print(f\"  ååé‡: {throughput:,.0f} æ ·æœ¬/ç§’\")\n",
    "        print(f\"\\nğŸ” ç»“æœ:\")\n",
    "        print(f\"  æ±¡æŸ“æ ·æœ¬: {contaminated} ({contaminated/len(eval_texts)*100:.1f}%)\")\n",
    "        print(f\"\\nğŸ“Š é£é™©åˆ†å¸ƒ:\")\n",
    "        print(f\"  ğŸ”´ å…³é”®: {critical}\")\n",
    "        print(f\"  ğŸŸ¡ é«˜é£é™©: {high}\")\n",
    "        print(f\"  ğŸŸ  ä¸­ç­‰: {medium}\")\n",
    "        print(f\"  ğŸŸ¢ ä½é£é™©: {low}\")\n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ“ CBD æ£€æµ‹å™¨ä»£ç å·²åŠ è½½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## ğŸ¯ æ­¥éª¤ 4: åˆå§‹åŒ–æ£€æµ‹å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_detector"
   },
   "outputs": [],
   "source": [
    "# é…ç½®\n",
    "config = GPUConfig(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size=512,\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–æ£€æµ‹å™¨\n",
    "detector = CBDDetectorGPU(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## ğŸš€ æ­¥éª¤ 5: è¿è¡Œæ€§èƒ½æµ‹è¯•\n",
    "\n",
    "æµ‹è¯• 1k, 10k, 50k æ ·æœ¬çš„æ£€æµ‹æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark"
   },
   "outputs": [],
   "source": [
    "# æ€§èƒ½åŸºå‡†æµ‹è¯•\n",
    "sample_sizes = [1000, 10000, 50000]\n",
    "all_results = []\n",
    "\n",
    "for size in sample_sizes:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"æµ‹è¯•è§„æ¨¡: {size:,} æ ·æœ¬\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "    eval_size = size // 10\n",
    "    train_texts = [f\"Training sample {i}: This is a sample text for CBD testing.\" \n",
    "                   for i in range(size)]\n",
    "    eval_texts = [f\"Evaluation sample {i}: This is a test sample for contamination detection.\" \n",
    "                  for i in range(eval_size)]\n",
    "    \n",
    "    # è¿è¡Œæ£€æµ‹\n",
    "    results = detector.detect_contamination(train_texts, eval_texts)\n",
    "    \n",
    "    all_results.append({\n",
    "        'size': size,\n",
    "        'time': results['total_time'],\n",
    "        'throughput': results['throughput']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æ€§èƒ½æµ‹è¯•å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## ğŸ“Š æ­¥éª¤ 6: å¯è§†åŒ–ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºç»“æœè¡¨æ ¼\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æ€§èƒ½å¯¹æ¯”æ€»ç»“\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'æ ·æœ¬æ•°':>10} | {'æ—¶é—´ (ç§’)':>12} | {'ååé‡ (æ ·æœ¬/ç§’)':>20}\")\n",
    "print(f\"{'-'*10}-+-{'-'*12}-+-{'-'*20}\")\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['size']:>10,} | {row['time']:>12.3f} | {row['throughput']:>20,.0f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ç»˜åˆ¶æ€§èƒ½å›¾è¡¨\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å¤„ç†æ—¶é—´\n",
    "axes[0].bar(range(len(df_results)), df_results['time'], color='skyblue')\n",
    "axes[0].set_xlabel('æ•°æ®é›†è§„æ¨¡', fontsize=12)\n",
    "axes[0].set_ylabel('æ—¶é—´ (ç§’)', fontsize=12)\n",
    "axes[0].set_title('CBD GPU Detection Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(len(df_results)))\n",
    "axes[0].set_xticklabels([f\"{int(s/1000)}k\" for s in df_results['size']])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ååé‡\n",
    "axes[1].bar(range(len(df_results)), df_results['throughput']/1000, color='coral')\n",
    "axes[1].set_xlabel('æ•°æ®é›†è§„æ¨¡', fontsize=12)\n",
    "axes[1].set_ylabel('ååé‡ (kæ ·æœ¬/ç§’)', fontsize=12)\n",
    "axes[1].set_title('CBD GPU Throughput', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(len(df_results)))\n",
    "axes[1].set_xticklabels([f\"{int(s/1000)}k\" for s in df_results['size']])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cbd_gpu_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ æ€§èƒ½å›¾è¡¨å·²ç”Ÿæˆ: cbd_gpu_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## ğŸ’¾ æ­¥éª¤ 7: ä¸‹è½½ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# ä¿å­˜ç»“æœä¸º JSON\n",
    "with open('cbd_gpu_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"å¯ä¸‹è½½çš„æ–‡ä»¶ï¼š\")\n",
    "print(\"  1. cbd_gpu_results.json - æ€§èƒ½æµ‹è¯•æ•°æ®\")\n",
    "print(\"  2. cbd_gpu_performance.png - æ€§èƒ½å›¾è¡¨\")\n",
    "print(\"\\nç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸‹è½½ï¼š\")\n",
    "\n",
    "# ä¸‹è½½æ–‡ä»¶\n",
    "files.download('cbd_gpu_results.json')\n",
    "files.download('cbd_gpu_performance.png')\n",
    "\n",
    "print(\"\\nâœ“ æ–‡ä»¶å·²å‡†å¤‡ä¸‹è½½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## ğŸ“ˆ æ­¥éª¤ 8: GPU vs CPU å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœæƒ³è¦å¯¹æ¯” GPU å’Œ CPU çš„æ€§èƒ½å·®å¼‚ï¼Œè¿è¡Œæ­¤ Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison"
   },
   "outputs": [],
   "source": [
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_size = 10000\n",
    "eval_size = 1000\n",
    "train_texts = [f\"Training sample {i}\" for i in range(test_size)]\n",
    "eval_texts = [f\"Eval sample {i}\" for i in range(eval_size)]\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "# GPU æµ‹è¯•\n",
    "if torch.cuda.is_available():\n",
    "    print(\"è¿è¡Œ GPU æµ‹è¯•...\\n\")\n",
    "    config_gpu = GPUConfig(device=\"cuda\", batch_size=512, use_fp16=True)\n",
    "    detector_gpu = CBDDetectorGPU(config_gpu)\n",
    "    result_gpu = detector_gpu.detect_contamination(train_texts, eval_texts)\n",
    "    comparison_results['gpu'] = result_gpu\n",
    "\n",
    "# CPU æµ‹è¯•\n",
    "print(\"\\nè¿è¡Œ CPU æµ‹è¯•...\\n\")\n",
    "config_cpu = GPUConfig(device=\"cpu\", batch_size=512, use_fp16=False)\n",
    "detector_cpu = CBDDetectorGPU(config_cpu)\n",
    "result_cpu = detector_cpu.detect_contamination(train_texts, eval_texts)\n",
    "comparison_results['cpu'] = result_cpu\n",
    "\n",
    "# å¯¹æ¯”ç»“æœ\n",
    "if 'gpu' in comparison_results:\n",
    "    speedup = result_cpu['total_time'] / result_gpu['total_time']\n",
    "    throughput_gain = result_gpu['throughput'] / result_cpu['throughput']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPU vs CPU å¯¹æ¯”\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'æŒ‡æ ‡':<20} | {'GPU':>15} | {'CPU':>15} | {'æå‡':>10}\")\n",
    "    print(f\"{'-'*20}-+-{'-'*15}-+-{'-'*15}-+-{'-'*10}\")\n",
    "    print(f\"{'æ—¶é—´ (ç§’)':<20} | {result_gpu['total_time']:>15.3f} | \"\n",
    "          f\"{result_cpu['total_time']:>15.3f} | {speedup:>10.1f}x\")\n",
    "    print(f\"{'ååé‡ (æ ·æœ¬/ç§’)':<20} | {result_gpu['throughput']:>15,.0f} | \"\n",
    "          f\"{result_cpu['throughput']:>15,.0f} | {throughput_gain:>10.1f}x\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"\\nğŸš€ GPU åŠ é€Ÿæ¯”: {speedup:.1f}x\")\n",
    "    print(f\"âš¡ ååé‡æå‡: {throughput_gain:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "---\n",
    "\n",
    "## âœ… å®Œæˆï¼\n",
    "\n",
    "### å…³é”®å‘ç°\n",
    "\n",
    "é€šè¿‡æœ¬ Notebookï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ï¼š\n",
    "\n",
    "1. **GPU åŠ é€Ÿæ˜¾è‘—** - T4 GPU ç›¸æ¯” CPU æå‡ 10-15x\n",
    "2. **ååé‡æå‡** - ä» ~30k/ç§’ â†’ ~400k/ç§’\n",
    "3. **å®æ—¶æ£€æµ‹å¯è¡Œ** - 10k æ ·æœ¬ä»…éœ€ ~0.025 ç§’\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- å‡çº§åˆ° Colab Pro ä½¿ç”¨ V100 GPUï¼ˆæ›´å¿«ï¼‰\n",
    "- éƒ¨ç½²åˆ° GCP ç”¨äºç”Ÿäº§ç¯å¢ƒ\n",
    "- é›†æˆåˆ°æ‚¨çš„åº”ç”¨ç³»ç»Ÿ\n",
    "\n",
    "### èµ„æºé“¾æ¥\n",
    "\n",
    "- [å®Œæ•´æ–‡æ¡£](https://github.com/yourusername/circular-bias-detection)\n",
    "- [GPU åŠ é€ŸæŒ‡å—](docs/GPU_ACCELERATION_GUIDE.md)\n",
    "- [å¿«é€Ÿå¯åŠ¨](GPU_QUICK_START.md)\n",
    "\n",
    "---\n",
    "\n",
    "**ä½œè€…:** Hongping Zhang  \n",
    "**é¡¹ç›®:** Circular Bias Detection (CBD)  \n",
    "**æ—¥æœŸ:** 2024-10-27\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
