{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circular Bias Detection: Visualization & Model Card\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Visualizing permutation test distributions\n",
    "2. Analyzing p-value confidence intervals\n",
    "3. Generating model audit cards\n",
    "4. Performance optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from cbd.api import detect_bias\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a well-generalized model\n",
    "model_good = LogisticRegression(random_state=42, max_iter=1000, C=1.0)\n",
    "model_good.fit(X, y)\n",
    "\n",
    "# Train an overfitted model (high capacity)\n",
    "model_overfit = DecisionTreeClassifier(random_state=42, max_depth=None)\n",
    "model_overfit.fit(X, y)\n",
    "\n",
    "print(f\"Good model accuracy: {accuracy_score(y, model_good.predict(X)):.3f}\")\n",
    "print(f\"Overfit model accuracy: {accuracy_score(y, model_overfit.predict(X)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Bias Detection with Full Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect bias in good model\n",
    "result_good = detect_bias(\n",
    "    model_good,\n",
    "    X, y,\n",
    "    metric=accuracy_score,\n",
    "    n_permutations=1000,\n",
    "    random_state=42,\n",
    "    return_permutations=True,\n",
    "    n_jobs=-1,  # Use all CPUs\n",
    "    backend='threads'\n",
    ")\n",
    "\n",
    "# Detect bias in overfitted model\n",
    "result_overfit = detect_bias(\n",
    "    model_overfit,\n",
    "    X, y,\n",
    "    metric=accuracy_score,\n",
    "    n_permutations=1000,\n",
    "    random_state=42,\n",
    "    return_permutations=True,\n",
    "    n_jobs=-1,\n",
    "    backend='threads'\n",
    ")\n",
    "\n",
    "print(\"\\n=== Good Model ===\")\n",
    "print(f\"Observed metric: {result_good['observed_metric']:.4f}\")\n",
    "print(f\"P-value: {result_good['p_value']:.4f}\")\n",
    "if 'p_value_ci' in result_good:\n",
    "    ci = result_good['p_value_ci']\n",
    "    print(f\"P-value 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "print(f\"Conclusion: {result_good['conclusion']}\")\n",
    "\n",
    "print(\"\\n=== Overfit Model ===\")\n",
    "print(f\"Observed metric: {result_overfit['observed_metric']:.4f}\")\n",
    "print(f\"P-value: {result_overfit['p_value']:.4f}\")\n",
    "if 'p_value_ci' in result_overfit:\n",
    "    ci = result_overfit['p_value_ci']\n",
    "    print(f\"P-value 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "print(f\"Conclusion: {result_overfit['conclusion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Permutation Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_distribution(result, title, ax=None):\n",
    "    \"\"\"Plot histogram of permuted metrics with observed value.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    permuted = np.array(result['permuted_metrics'])\n",
    "    observed = result['observed_metric']\n",
    "    p_value = result['p_value']\n",
    "    \n",
    "    # Plot histogram\n",
    "    ax.hist(permuted, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Mark observed value\n",
    "    ax.axvline(observed, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Observed: {observed:.4f}')\n",
    "    \n",
    "    # Add percentile line\n",
    "    percentile_95 = np.percentile(permuted, 95)\n",
    "    ax.axvline(percentile_95, color='orange', linestyle=':', linewidth=2,\n",
    "               label=f'95th percentile: {percentile_95:.4f}')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Metric Value', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nP-value: {p_value:.4f}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Create side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "plot_permutation_distribution(result_good, 'Well-Generalized Model', axes[0])\n",
    "plot_permutation_distribution(result_overfit, 'Overfitted Model', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution_stats(result):\n",
    "    \"\"\"Compute statistical summary of permutation distribution.\"\"\"\n",
    "    permuted = np.array(result['permuted_metrics'])\n",
    "    observed = result['observed_metric']\n",
    "    \n",
    "    stats = {\n",
    "        'Mean': np.mean(permuted),\n",
    "        'Std': np.std(permuted),\n",
    "        'Median': np.median(permuted),\n",
    "        'Min': np.min(permuted),\n",
    "        'Max': np.max(permuted),\n",
    "        '95th Percentile': np.percentile(permuted, 95),\n",
    "        'Observed': observed,\n",
    "        'Z-score': (observed - np.mean(permuted)) / np.std(permuted),\n",
    "        'P-value': result['p_value']\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"\\n=== Good Model Statistics ===\")\n",
    "for key, value in compute_distribution_stats(result_good).items():\n",
    "    print(f\"{key:20s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== Overfit Model Statistics ===\")\n",
    "for key, value in compute_distribution_stats(result_overfit).items():\n",
    "    print(f\"{key:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Card Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(model, result, model_name, dataset_info):\n",
    "    \"\"\"Generate a model audit card with bias detection results.\"\"\"\n",
    "    \n",
    "    card = f\"\"\"\n",
    "# Model Card: {model_name}\n",
    "\n",
    "## Model Details\n",
    "- **Model Type**: {type(model).__name__}\n",
    "- **Training Date**: {np.datetime64('today')}\n",
    "- **Framework**: scikit-learn\n",
    "\n",
    "## Dataset Information\n",
    "- **Number of Samples**: {dataset_info['n_samples']}\n",
    "- **Number of Features**: {dataset_info['n_features']}\n",
    "- **Task**: {dataset_info['task']}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Observed Metric**: {result['observed_metric']:.4f}\n",
    "- **Metric Type**: {dataset_info['metric_name']}\n",
    "\n",
    "## Circular Bias Detection Results\n",
    "- **P-value**: {result['p_value']:.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    if 'p_value_ci' in result:\n",
    "        ci = result['p_value_ci']\n",
    "        card += f\"- **P-value {int(result['confidence_level']*100)}% CI**: [{ci[0]:.4f}, {ci[1]:.4f}]\\n\"\n",
    "    \n",
    "    card += f\"\"\"\n",
    "- **Number of Permutations**: {result['n_permutations']}\n",
    "- **Null Method**: {result['null_method']}\n",
    "- **Parallel Backend**: {result['backend']}\n",
    "- **Conclusion**: {result['conclusion']}\n",
    "\n",
    "## Interpretation\n",
    "\"\"\"\n",
    "    \n",
    "    if result['p_value'] <= 0.05:\n",
    "        card += \"\"\"\n",
    "⚠️ **WARNING**: This model shows signs of potential circular bias.\n",
    "The observed performance is suspiciously high compared to the null distribution.\n",
    "\n",
    "**Recommendations**:\n",
    "1. Verify data leakage between training and evaluation\n",
    "2. Check for feature engineering using target information\n",
    "3. Ensure proper train/test split\n",
    "4. Consider using cross-validation\n",
    "5. Review feature selection process\n",
    "\"\"\"\n",
    "    else:\n",
    "        card += \"\"\"\n",
    "✅ **PASSED**: No strong evidence of circular bias detected.\n",
    "The model's performance is consistent with the null distribution.\n",
    "\n",
    "**Note**: This does not guarantee absence of all forms of bias.\n",
    "Continue to monitor model performance and fairness metrics.\n",
    "\"\"\"\n",
    "    \n",
    "    card += f\"\"\"\n",
    "\n",
    "## Computational Details\n",
    "- **Samples Used**: {result['n_samples']}\n",
    "- **Subsampled**: {result['subsampled']}\n",
    "- **Parallel Workers**: {result['n_jobs']}\n",
    "\n",
    "## Limitations\n",
    "- This test detects circular bias through permutation testing\n",
    "- Results are specific to the provided dataset\n",
    "- Does not detect all forms of data leakage\n",
    "- Should be combined with other validation methods\n",
    "\n",
    "## Contact\n",
    "For questions about this model card, please contact the model owner.\n",
    "\n",
    "---\n",
    "*Generated using Circular Bias Detector (CBD)*\n",
    "\"\"\"\n",
    "    \n",
    "    return card\n",
    "\n",
    "# Generate cards\n",
    "dataset_info = {\n",
    "    'n_samples': X.shape[0],\n",
    "    'n_features': X.shape[1],\n",
    "    'task': 'Binary Classification',\n",
    "    'metric_name': 'Accuracy'\n",
    "}\n",
    "\n",
    "card_good = generate_model_card(model_good, result_good, \"Logistic Regression\", dataset_info)\n",
    "card_overfit = generate_model_card(model_overfit, result_overfit, \"Decision Tree\", dataset_info)\n",
    "\n",
    "# Save cards\n",
    "with open('model_card_logistic.md', 'w') as f:\n",
    "    f.write(card_good)\n",
    "\n",
    "with open('model_card_tree.md', 'w') as f:\n",
    "    f.write(card_overfit)\n",
    "\n",
    "print(\"Model cards saved!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(card_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create larger dataset\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=50,\n",
    "    n_informative=25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_large = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_large.fit(X_large, y_large)\n",
    "\n",
    "print(\"Performance Comparison:\\n\")\n",
    "\n",
    "# Sequential\n",
    "start = time.time()\n",
    "result_seq = detect_bias(\n",
    "    model_large, X_large, y_large,\n",
    "    metric=accuracy_score,\n",
    "    n_permutations=100,\n",
    "    n_jobs=1,\n",
    "    random_state=42\n",
    ")\n",
    "time_seq = time.time() - start\n",
    "print(f\"Sequential (n_jobs=1): {time_seq:.2f}s\")\n",
    "\n",
    "# Parallel threads\n",
    "start = time.time()\n",
    "result_par = detect_bias(\n",
    "    model_large, X_large, y_large,\n",
    "    metric=accuracy_score,\n",
    "    n_permutations=100,\n",
    "    n_jobs=-1,\n",
    "    backend='threads',\n",
    "    random_state=42\n",
    ")\n",
    "time_par = time.time() - start\n",
    "print(f\"Parallel threads (n_jobs=-1): {time_par:.2f}s\")\n",
    "print(f\"Speedup: {time_seq/time_par:.2f}x\")\n",
    "\n",
    "# With subsampling\n",
    "start = time.time()\n",
    "result_sub = detect_bias(\n",
    "    model_large, X_large, y_large,\n",
    "    metric=accuracy_score,\n",
    "    n_permutations=100,\n",
    "    n_jobs=-1,\n",
    "    backend='threads',\n",
    "    subsample_size=1000,  # Use only 1000 samples\n",
    "    random_state=42\n",
    ")\n",
    "time_sub = time.time() - start\n",
    "print(f\"\\nWith subsampling (n=1000): {time_sub:.2f}s\")\n",
    "print(f\"Speedup vs sequential: {time_seq/time_sub:.2f}x\")\n",
    "print(f\"P-value (full): {result_par['p_value']:.4f}\")\n",
    "print(f\"P-value (subsampled): {result_sub['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced: Retrain Null Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative test: retrain model on each permutation\n",
    "print(\"Running conservative retrain test (this may take a while)...\\n\")\n",
    "\n",
    "result_retrain = detect_bias(\n",
    "    model_good,\n",
    "    X, y,\n",
    "    metric=accuracy_score,\n",
    "    n_permutations=50,  # Fewer permutations due to computational cost\n",
    "    null_method='retrain',\n",
    "    n_jobs=-1,\n",
    "    backend='threads',\n",
    "    random_state=42,\n",
    "    return_permutations=True\n",
    ")\n",
    "\n",
    "print(f\"Permute method p-value: {result_good['p_value']:.4f}\")\n",
    "print(f\"Retrain method p-value: {result_retrain['p_value']:.4f}\")\n",
    "print(f\"\\nRetrain is more conservative and computationally expensive.\")\n",
    "print(f\"Use it when you need the most rigorous test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Probability-Based Metrics (AUC, Log Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with AUC metric\n",
    "def auc_metric(y_true, y_proba):\n",
    "    if y_proba.ndim == 2:\n",
    "        y_proba = y_proba[:, 1]\n",
    "    return roc_auc_score(y_true, y_proba)\n",
    "\n",
    "result_auc = detect_bias(\n",
    "    model_good,\n",
    "    X, y,\n",
    "    metric=auc_metric,\n",
    "    n_permutations=500,\n",
    "    allow_proba=True,  # Use predict_proba\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_permutations=True\n",
    ")\n",
    "\n",
    "print(\"AUC-based Detection:\")\n",
    "print(f\"Observed AUC: {result_auc['observed_metric']:.4f}\")\n",
    "print(f\"P-value: {result_auc['p_value']:.4f}\")\n",
    "print(f\"Conclusion: {result_auc['conclusion']}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_permutation_distribution(result_auc, 'AUC Metric with Probabilities', ax)\n",
    "plt.savefig('auc_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ **Visualization**: Histogram plots showing permutation distributions\n",
    "2. ✅ **Statistical Analysis**: Computing distribution statistics and z-scores\n",
    "3. ✅ **Model Cards**: Automated generation of audit documentation\n",
    "4. ✅ **Performance**: Parallel execution and subsampling strategies\n",
    "5. ✅ **Advanced Methods**: Retrain null method and probability metrics\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use `n_permutations >= 1000` for reliable p-values\n",
    "- Enable parallelization (`n_jobs=-1`) for large datasets\n",
    "- Use `subsample_size` for datasets > 10,000 samples\n",
    "- Consider `null_method='retrain'` for conservative testing\n",
    "- Set `allow_proba=True` for AUC, log loss, and other probability metrics\n",
    "- Always visualize the permutation distribution\n",
    "- Generate model cards for audit trails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
