"""
è¯­ä¹‰é‡å†™æ„é€ æ³„éœ² (Semantic Rewrite for Leakage Construction)

æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•é€šè¿‡è¯­ä¹‰é‡å†™æŠ€æœ¯æ„é€ "éšè”½çš„æ•°æ®æ³„éœ²"æ¡ˆä¾‹ï¼Œ
ä»¥éªŒè¯ CBD æ¡†æ¶åœ¨æ£€æµ‹è¯­ä¹‰ç›¸ä¼¼ä½†è¡¨é¢ä¸åŒçš„æ•°æ®æ±¡æŸ“æ–¹é¢çš„èƒ½åŠ›ã€‚

æ ¸å¿ƒç†å¿µï¼š
- è¡¨é¢æªè¾ä¸åŒï¼ˆSurface-level dissimilarityï¼‰
- è¯­ä¹‰ç›¸ä¼¼åº¦é«˜ï¼ˆSemantic similarityï¼‰
- è¶³ä»¥è®©æ¨¡å‹"è®°å¿†"ï¼ˆSufficient for memorizationï¼‰

ä½œè€…: Hongping Zhang
æ—¥æœŸ: 2024-10-27
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
import re
from dataclasses import dataclass
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class LeakagePair:
    """æ³„éœ²æ•°æ®å¯¹"""
    train_text: str  # è®­ç»ƒæ•°æ®ä¸­çš„æ–‡æœ¬
    eval_question: str  # è¯„ä¼°é—®é¢˜ï¼ˆæ³„éœ²ç‰ˆæœ¬ï¼‰
    eval_question_clean: str  # è¯„ä¼°é—®é¢˜ï¼ˆå¹²å‡€ç‰ˆæœ¬ï¼‰
    semantic_similarity: float  # è¯­ä¹‰ç›¸ä¼¼åº¦ (0-1)
    surface_similarity: float  # è¡¨é¢ç›¸ä¼¼åº¦ (0-1)
    leakage_type: str  # æ³„éœ²ç±»å‹


class SemanticRewriter:
    """è¯­ä¹‰é‡å†™å™¨ï¼šç”¨äºæ„é€ æ³„éœ²æ•°æ®å¯¹"""
    
    # åŒä¹‰è¯æ›¿æ¢è¡¨
    SYNONYM_MAP = {
        # åè¯æ›¿æ¢
        "gift": ["present", "offering", "contribution", "donation"],
        "people": ["population", "citizens", "inhabitants", "nation"],
        "country": ["nation", "state", "territory"],
        "Statue of Liberty": ["Lady Liberty", "Liberty Monument", "the Liberty statue"],
        
        # åŠ¨è¯æ›¿æ¢
        "was": ["became", "turned into"],
        "provided": ["gave", "presented", "offered", "supplied"],
        "gave": ["presented", "provided", "offered", "supplied"],
        
        # å½¢å®¹è¯æ›¿æ¢
        "large": ["big", "huge", "massive", "enormous"],
        "important": ["significant", "crucial", "vital", "key"],
        
        # ç–‘é—®è¯
        "which": ["what", "that"],
        "who": ["what person", "which individual"],
        "where": ["in what place", "at what location"],
    }
    
    # å¥å¼è½¬æ¢æ¨¡æ¿
    TRANSFORMATION_TEMPLATES = {
        "active_to_passive": {
            "X gave Y to Z": "Y was given to Z by X",
            "X provided Y to Z": "Y was provided to Z by X",
            "X built Y": "Y was built by X",
            "X created Y": "Y was created by X",
        },
        "question_patterns": {
            "What is X?": ["Can you identify X?", "X refers to what?"],
            "Who did X?": ["Which entity performed X?", "X was done by whom?"],
            "Where is X?": ["What is the location of X?", "X can be found where?"],
        }
    }
    
    def __init__(self):
        """åˆå§‹åŒ–è¯­ä¹‰é‡å†™å™¨"""
        logger.info("è¯­ä¹‰é‡å†™å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def synonym_replacement(self, text: str, replacement_rate: float = 0.3) -> str:
        """
        åŒä¹‰è¯æ›¿æ¢
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            replacement_rate: æ›¿æ¢æ¯”ä¾‹ (0-1)
            
        Returns:
            æ›¿æ¢åçš„æ–‡æœ¬
        """
        words = text.split()
        num_replacements = int(len(words) * replacement_rate)
        
        replaced_text = text
        replacements_made = 0
        
        for original, synonyms in self.SYNONYM_MAP.items():
            if replacements_made >= num_replacements:
                break
            
            if original in replaced_text:
                synonym = np.random.choice(synonyms)
                replaced_text = replaced_text.replace(original, synonym, 1)
                replacements_made += 1
        
        return replaced_text
    
    def sentence_restructuring(self, text: str, pattern: str = "active_to_passive") -> str:
        """
        å¥å¼é‡ç»„ï¼ˆä¸»åŠ¨ â†” è¢«åŠ¨ï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            pattern: è½¬æ¢æ¨¡å¼
            
        Returns:
            é‡ç»„åçš„æ–‡æœ¬
        """
        if pattern in self.TRANSFORMATION_TEMPLATES:
            templates = self.TRANSFORMATION_TEMPLATES[pattern]
            
            for original, transformed in templates.items():
                # ç®€å•çš„æ¨¡å¼åŒ¹é…å’Œæ›¿æ¢
                if self._pattern_match(text, original):
                    return self._apply_transformation(text, original, transformed)
        
        return text
    
    def _pattern_match(self, text: str, pattern: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…æ¨¡å¼"""
        # ç®€åŒ–ç‰ˆæ¨¡å¼åŒ¹é…
        pattern_words = set(pattern.lower().split())
        text_words = set(text.lower().split())
        return len(pattern_words & text_words) >= len(pattern_words) * 0.5
    
    def _apply_transformation(self, text: str, original: str, transformed: str) -> str:
        """åº”ç”¨è½¬æ¢"""
        # ç®€åŒ–ç‰ˆè½¬æ¢ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†ï¼‰
        return text
    
    def paraphrase_question(
        self,
        original_statement: str,
        question_type: str = "wh_question"
    ) -> str:
        """
        å°†é™ˆè¿°å¥è½¬æ¢ä¸ºé—®å¥ï¼ˆé‡Šä¹‰ï¼‰
        
        Args:
            original_statement: åŸå§‹é™ˆè¿°å¥
            question_type: é—®é¢˜ç±»å‹ (wh_question/yes_no/fill_blank)
            
        Returns:
            ç”Ÿæˆçš„é—®é¢˜
        """
        # ç¤ºä¾‹ï¼šå°†é™ˆè¿°å¥è½¬æ¢ä¸º Wh- é—®é¢˜
        if "was a gift from" in original_statement.lower():
            # æå–å…³é”®å®ä½“
            parts = original_statement.split("was a gift from")
            subject = parts[0].strip()
            source = parts[1].strip().rstrip(".")
            
            # ç”Ÿæˆä¸åŒå½¢å¼çš„é—®é¢˜
            questions = [
                f"Which entity provided {subject}?",
                f"What was the source of {subject}?",
                f"{subject} was a gift from which party?",
                f"The origin of {subject} can be traced to which nation?"
            ]
            
            return np.random.choice(questions)
        
        return f"What is the main subject of: {original_statement}?"
    
    def construct_leaked_pair(
        self,
        train_sentence: str,
        leakage_intensity: float = 0.7
    ) -> LeakagePair:
        """
        æ„é€ æ³„éœ²æ•°æ®å¯¹
        
        Args:
            train_sentence: è®­ç»ƒæ•°æ®ä¸­çš„å¥å­
            leakage_intensity: æ³„éœ²å¼ºåº¦ (0=å®Œå…¨ä¸åŒ, 1=å®Œå…¨ç›¸åŒ)
            
        Returns:
            LeakagePair å¯¹è±¡
        """
        # æ­¥éª¤ 1: ç”Ÿæˆæ³„éœ²è¯„ä¼°é—®é¢˜ï¼ˆé«˜è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        leaked_question = self.paraphrase_question(train_sentence)
        
        # æ­¥éª¤ 2: åº”ç”¨åŒä¹‰è¯æ›¿æ¢ï¼ˆé™ä½è¡¨é¢ç›¸ä¼¼åº¦ï¼‰
        replacement_rate = 1 - leakage_intensity  # å¼ºåº¦è¶Šé«˜ï¼Œæ›¿æ¢è¶Šå°‘
        leaked_question = self.synonym_replacement(leaked_question, replacement_rate)
        
        # æ­¥éª¤ 3: ç”Ÿæˆå¹²å‡€çš„è¯„ä¼°é—®é¢˜ï¼ˆä½è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        clean_question = self._generate_clean_question(train_sentence)
        
        # æ­¥éª¤ 4: è®¡ç®—ç›¸ä¼¼åº¦
        semantic_sim = self._compute_semantic_similarity(train_sentence, leaked_question)
        surface_sim = self._compute_surface_similarity(train_sentence, leaked_question)
        
        return LeakagePair(
            train_text=train_sentence,
            eval_question=leaked_question,
            eval_question_clean=clean_question,
            semantic_similarity=semantic_sim,
            surface_similarity=surface_sim,
            leakage_type="semantic_paraphrase"
        )
    
    def _generate_clean_question(self, train_sentence: str) -> str:
        """ç”Ÿæˆå¹²å‡€çš„è¯„ä¼°é—®é¢˜ï¼ˆä½ç›¸ä¼¼åº¦ï¼‰"""
        # æå–ä¸»é¢˜ï¼Œä½†ç”¨å®Œå…¨ä¸åŒçš„æªè¾
        if "Statue of Liberty" in train_sentence:
            return "Which country provided the Lady Liberty monument to the United States?"
        elif "Eiffel Tower" in train_sentence:
            return "What nation built the iron structure in Paris?"
        else:
            return "What is the answer to this unrelated question?"
    
    def _compute_semantic_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ sentence-transformers æˆ–å…¶ä»–è¯­ä¹‰æ¨¡å‹
        """
        # ä½¿ç”¨è¯æ±‡é‡å ä½œä¸ºè¿‘ä¼¼
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        if union == 0:
            return 0.0
        
        jaccard_sim = intersection / union
        
        # è°ƒæ•´åˆ°åˆç†èŒƒå›´ (0.6-0.9 for leaked pairs)
        return min(0.9, jaccard_sim + 0.3)
    
    def _compute_surface_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—è¡¨é¢ç›¸ä¼¼åº¦ï¼ˆå­—ç¬¦çº§åˆ«ï¼‰
        """
        # æœ€é•¿å…¬å…±å­åºåˆ— (LCS) ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher
        
        matcher = SequenceMatcher(None, text1, text2)
        return matcher.ratio()


class LeakageSimulator:
    """æ³„éœ²æ¨¡æ‹Ÿå™¨ï¼šæ„é€ å®Œæ•´çš„æ³„éœ²æ•°æ®é›†"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ³„éœ²æ¨¡æ‹Ÿå™¨"""
        self.rewriter = SemanticRewriter()
        logger.info("æ³„éœ²æ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_knowledge_base(self) -> List[str]:
        """
        åˆ›å»ºçŸ¥è¯†åº“ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ï¼‰
        
        Returns:
            çŸ¥è¯†å¥å­åˆ—è¡¨
        """
        knowledge_base = [
            "The Statue of Liberty was a gift from the people of France to the people of the United States.",
            "The Eiffel Tower was built by Gustave Eiffel in Paris, France.",
            "The Great Wall of China was constructed over many centuries to protect against invasions.",
            "Mount Everest is the highest mountain in the world, located in the Himalayas.",
            "The Amazon Rainforest is the largest tropical rainforest on Earth.",
            "Shakespeare wrote Romeo and Juliet in the late 16th century.",
            "Albert Einstein developed the theory of relativity in 1905.",
            "The Internet was invented by ARPA in the United States in the 1960s.",
            "The Mona Lisa was painted by Leonardo da Vinci in the early 16th century.",
            "The Roman Empire reached its greatest extent under Emperor Trajan."
        ]
        
        logger.info(f"åˆ›å»ºçŸ¥è¯†åº“: {len(knowledge_base)} ä¸ªæ¡ç›®")
        return knowledge_base
    
    def simulate_leakage_dataset(
        self,
        num_samples: int = 50,
        leakage_ratio: float = 0.4,
        leakage_intensity: float = 0.75
    ) -> pd.DataFrame:
        """
        æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†
        
        Args:
            num_samples: æ ·æœ¬æ•°é‡
            leakage_ratio: æ³„éœ²æ ·æœ¬æ¯”ä¾‹ (0-1)
            leakage_intensity: æ³„éœ²å¼ºåº¦ (0-1)
            
        Returns:
            åŒ…å«æ³„éœ²æ ‡ç­¾çš„ DataFrame
        """
        logger.info(f"å¼€å§‹æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†: {num_samples} ä¸ªæ ·æœ¬, æ³„éœ²ç‡ {leakage_ratio:.0%}")
        
        knowledge_base = self.create_knowledge_base()
        num_leaked = int(num_samples * leakage_ratio)
        num_clean = num_samples - num_leaked
        
        leaked_pairs = []
        
        # ç”Ÿæˆæ³„éœ²æ ·æœ¬
        for i in range(num_leaked):
            train_sentence = np.random.choice(knowledge_base)
            pair = self.rewriter.construct_leaked_pair(train_sentence, leakage_intensity)
            
            leaked_pairs.append({
                "sample_id": i,
                "train_text": pair.train_text,
                "eval_question": pair.eval_question,
                "is_leaked": True,
                "semantic_similarity": pair.semantic_similarity,
                "surface_similarity": pair.surface_similarity,
                "leakage_type": pair.leakage_type,
                "c_score_expected": pair.semantic_similarity  # CBD é¢„æœŸæ£€æµ‹åˆ°çš„åˆ†æ•°
            })
        
        # ç”Ÿæˆå¹²å‡€æ ·æœ¬
        for i in range(num_clean):
            train_sentence = np.random.choice(knowledge_base)
            clean_question = self.rewriter._generate_clean_question(train_sentence)
            
            leaked_pairs.append({
                "sample_id": num_leaked + i,
                "train_text": train_sentence,
                "eval_question": clean_question,
                "is_leaked": False,
                "semantic_similarity": np.random.uniform(0.1, 0.3),
                "surface_similarity": np.random.uniform(0.05, 0.2),
                "leakage_type": "clean",
                "c_score_expected": np.random.uniform(0.1, 0.3)
            })
        
        df = pd.DataFrame(leaked_pairs)
        logger.info(f"âœ“ æ³„éœ²æ•°æ®é›†æ¨¡æ‹Ÿå®Œæˆ: {len(df)} ä¸ªæ ·æœ¬")
        
        return df
    
    def analyze_leakage_distribution(self, df: pd.DataFrame) -> Dict:
        """
        åˆ†ææ³„éœ²æ•°æ®åˆ†å¸ƒ
        
        Args:
            df: æ³„éœ²æ•°æ®é›† DataFrame
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        leaked_samples = df[df['is_leaked'] == True]
        clean_samples = df[df['is_leaked'] == False]
        
        analysis = {
            "total_samples": len(df),
            "leaked_samples": len(leaked_samples),
            "clean_samples": len(clean_samples),
            "leakage_ratio": len(leaked_samples) / len(df),
            "avg_semantic_sim_leaked": leaked_samples['semantic_similarity'].mean(),
            "avg_semantic_sim_clean": clean_samples['semantic_similarity'].mean(),
            "avg_surface_sim_leaked": leaked_samples['surface_similarity'].mean(),
            "avg_surface_sim_clean": clean_samples['surface_similarity'].mean(),
            "high_risk_samples": len(df[df['c_score_expected'] > 0.75])
        }
        
        return analysis


def demonstrate_leakage_construction():
    """æ¼”ç¤ºæ³„éœ²æ„é€ æµç¨‹"""
    
    print("=" * 70)
    print("è¯­ä¹‰é‡å†™æ„é€ æ³„éœ² - æ¼”ç¤º")
    print("=" * 70)
    
    # åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨
    simulator = LeakageSimulator()
    
    # ç¤ºä¾‹ 1: å•ä¸ªæ³„éœ²å¯¹æ„é€ 
    print("\nã€ç¤ºä¾‹ 1ã€‘æ„é€ å•ä¸ªæ³„éœ²å¯¹")
    print("-" * 70)
    
    train_text = "The Statue of Liberty was a gift from the people of France to the people of the United States."
    rewriter = SemanticRewriter()
    
    pair = rewriter.construct_leaked_pair(train_text, leakage_intensity=0.8)
    
    print(f"è®­ç»ƒæ•°æ®:\n  {pair.train_text}\n")
    print(f"æ³„éœ²è¯„ä¼°é—®é¢˜:\n  {pair.eval_question}\n")
    print(f"å¹²å‡€è¯„ä¼°é—®é¢˜:\n  {pair.eval_question_clean}\n")
    print(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {pair.semantic_similarity:.3f}")
    print(f"è¡¨é¢ç›¸ä¼¼åº¦: {pair.surface_similarity:.3f}")
    print(f"é¢„æœŸ C_score: {pair.semantic_similarity:.3f} (ğŸ”´ CRITICAL)\n")
    
    # ç¤ºä¾‹ 2: æ‰¹é‡æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†
    print("\nã€ç¤ºä¾‹ 2ã€‘æ‰¹é‡æ¨¡æ‹Ÿæ³„éœ²æ•°æ®é›†")
    print("-" * 70)
    
    df_leaked = simulator.simulate_leakage_dataset(
        num_samples=100,
        leakage_ratio=0.4,
        leakage_intensity=0.75
    )
    
    print(f"\nç”Ÿæˆçš„æ•°æ®é›†æ ·æœ¬:")
    print(df_leaked.head(10).to_string())
    
    # åˆ†ææ³„éœ²åˆ†å¸ƒ
    analysis = simulator.analyze_leakage_distribution(df_leaked)
    
    print("\n\nã€æ³„éœ²åˆ†ææŠ¥å‘Šã€‘")
    print("-" * 70)
    print(f"æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
    print(f"æ³„éœ²æ ·æœ¬: {analysis['leaked_samples']} ({analysis['leakage_ratio']:.1%})")
    print(f"å¹²å‡€æ ·æœ¬: {analysis['clean_samples']}")
    print(f"\nè¯­ä¹‰ç›¸ä¼¼åº¦:")
    print(f"  æ³„éœ²æ ·æœ¬å¹³å‡: {analysis['avg_semantic_sim_leaked']:.3f}")
    print(f"  å¹²å‡€æ ·æœ¬å¹³å‡: {analysis['avg_semantic_sim_clean']:.3f}")
    print(f"\nè¡¨é¢ç›¸ä¼¼åº¦:")
    print(f"  æ³„éœ²æ ·æœ¬å¹³å‡: {analysis['avg_surface_sim_leaked']:.3f}")
    print(f"  å¹²å‡€æ ·æœ¬å¹³å‡: {analysis['avg_surface_sim_clean']:.3f}")
    print(f"\né«˜é£é™©æ ·æœ¬ (C_score > 0.75): {analysis['high_risk_samples']}")
    
    # ä¿å­˜æ•°æ®é›†
    output_path = "leaked_dataset_sample.csv"
    df_leaked.to_csv(output_path, index=False)
    print(f"\nâœ“ æ³„éœ²æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")
    
    print("\n" + "=" * 70)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    demonstrate_leakage_construction()
