"""
CBD GPU åŠ é€Ÿæ£€æµ‹å™¨

ä½¿ç”¨ PyTorch + CUDA å®ç°é«˜æ€§èƒ½æ±¡æŸ“æ£€æµ‹ã€‚
æ”¯æŒæ‰¹å¤„ç†ã€æ··åˆç²¾åº¦å’Œå¤š GPU å¹¶è¡Œã€‚

ä½œè€…: Hongping Zhang
æ—¥æœŸ: 2024-10-27
"""

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple, Optional
import time
from dataclasses import dataclass
import sys
import os


@dataclass
class GPUConfig:
    """GPU é…ç½®"""
    device: str = "cuda"  # "cuda" æˆ– "cpu"
    batch_size: int = 512  # GPU æ‰¹å¤„ç†å¤§å°ï¼ˆæ ¹æ® VRAM è°ƒæ•´ï¼‰
    use_fp16: bool = True  # ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
    num_workers: int = 4   # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    model_name: str = "all-MiniLM-L6-v2"  # åµŒå…¥æ¨¡å‹


class CBDDetectorGPU:
    """GPU åŠ é€Ÿçš„ CBD æ£€æµ‹å™¨"""
    
    def __init__(self, config: GPUConfig = None):
        """
        åˆå§‹åŒ– GPU æ£€æµ‹å™¨
        
        Args:
            config: GPU é…ç½®
        """
        self.config = config or GPUConfig()
        
        # æ£€æŸ¥ GPU å¯ç”¨æ€§
        self.has_cuda = torch.cuda.is_available()
        if self.config.device == "cuda" and not self.has_cuda:
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
            self.config.device = "cpu"
        
        # åŠ è½½æ¨¡å‹åˆ° GPU
        print(f"\nåˆå§‹åŒ– CBD GPU æ£€æµ‹å™¨...")
        print(f"  - è®¾å¤‡: {self.config.device}")
        print(f"  - æ¨¡å‹: {self.config.model_name}")
        
        self.model = SentenceTransformer(self.config.model_name)
        self.model = self.model.to(self.config.device)
        
        # å¯ç”¨åŠç²¾åº¦åŠ é€Ÿ
        if self.config.use_fp16 and self.config.device == "cuda":
            self.model = self.model.half()
            print(f"  - ç²¾åº¦: FP16 (åŠç²¾åº¦)")
        else:
            print(f"  - ç²¾åº¦: FP32 (å…¨ç²¾åº¦)")
        
        # æ‰“å° GPU ä¿¡æ¯
        if self.config.device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"  - GPU: {gpu_name}")
            print(f"  - VRAM: {gpu_memory:.1f} GB")
            print(f"  - æ‰¹å¤„ç†å¤§å°: {self.config.batch_size}")
        
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ\n")
    
    def compute_embeddings_batch(
        self, 
        texts: List[str],
        show_progress: bool = True,
        description: str = "è®¡ç®—åµŒå…¥"
    ) -> torch.Tensor:
        """
        æ‰¹é‡è®¡ç®—æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆGPU åŠ é€Ÿï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            description: è¿›åº¦æè¿°
            
        Returns:
            åµŒå…¥å‘é‡å¼ é‡ (n_texts, embedding_dim)
        """
        if len(texts) == 0:
            return torch.zeros((0, 384), device=self.config.device)
        
        embeddings = []
        batch_size = self.config.batch_size
        n_batches = (len(texts) + batch_size - 1) // batch_size
        
        start_time = time.time()
        
        with torch.no_grad():
            for i in range(n_batches):
                batch_start = i * batch_size
                batch_end = min((i + 1) * batch_size, len(texts))
                batch_texts = texts[batch_start:batch_end]
                
                # è®¡ç®—åµŒå…¥
                batch_embeddings = self.model.encode(
                    batch_texts,
                    convert_to_tensor=True,
                    device=self.config.device,
                    show_progress_bar=False,
                    batch_size=len(batch_texts)
                )
                
                embeddings.append(batch_embeddings)
                
                if show_progress:
                    progress = (i + 1) / n_batches * 100
                    elapsed = time.time() - start_time
                    samples_done = min(batch_end, len(texts))
                    speed = samples_done / elapsed if elapsed > 0 else 0
                    print(f"  {description}: {progress:5.1f}% ({samples_done:>6,}/{len(texts):>6,}) | "
                          f"{speed:>8,.0f} æ ·æœ¬/ç§’", end='\r')
        
        if show_progress:
            print()  # æ¢è¡Œ
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        return torch.cat(embeddings, dim=0)
    
    def compute_similarity_matrix_gpu(
        self,
        train_embeddings: torch.Tensor,
        eval_embeddings: torch.Tensor,
        chunk_size: int = 1000
    ) -> torch.Tensor:
        """
        åœ¨ GPU ä¸Šè®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆåˆ†å—å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰
        
        Args:
            train_embeddings: è®­ç»ƒé›†åµŒå…¥ (n_train, dim)
            eval_embeddings: è¯„ä¼°é›†åµŒå…¥ (n_eval, dim)
            chunk_size: åˆ†å—å¤§å°
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ (n_eval, n_train)
        """
        n_eval = eval_embeddings.shape[0]
        n_train = train_embeddings.shape[0]
        
        # å½’ä¸€åŒ–
        train_norm = train_embeddings / train_embeddings.norm(dim=1, keepdim=True)
        eval_norm = eval_embeddings / eval_embeddings.norm(dim=1, keepdim=True)
        
        # å¦‚æœæ•°æ®é›†è¾ƒå°ï¼Œç›´æ¥è®¡ç®—
        if n_eval * n_train < 10_000_000:
            return torch.mm(eval_norm, train_norm.T)
        
        # å¦åˆ™åˆ†å—è®¡ç®—
        similarity_chunks = []
        n_chunks = (n_eval + chunk_size - 1) // chunk_size
        
        for i in range(n_chunks):
            start = i * chunk_size
            end = min((i + 1) * chunk_size, n_eval)
            chunk = torch.mm(eval_norm[start:end], train_norm.T)
            similarity_chunks.append(chunk)
            
            if (i + 1) % 10 == 0:
                print(f"  è®¡ç®—ç›¸ä¼¼åº¦: {(i+1)/n_chunks*100:.1f}%", end='\r')
        
        return torch.cat(similarity_chunks, dim=0)
    
    def detect_contamination(
        self,
        train_texts: List[str],
        eval_texts: List[str],
        threshold: float = 0.75,
        return_details: bool = False
    ) -> Dict:
        """
        æ£€æµ‹æ•°æ®æ±¡æŸ“ï¼ˆGPU åŠ é€Ÿå®Œæ•´æµç¨‹ï¼‰
        
        Args:
            train_texts: è®­ç»ƒé›†æ–‡æœ¬åˆ—è¡¨
            eval_texts: è¯„ä¼°é›†æ–‡æœ¬åˆ—è¡¨
            threshold: æ±¡æŸ“é˜ˆå€¼
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†ç»“æœ
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        print(f"\n{'='*70}")
        print(f"CBD GPU åŠ é€Ÿæ£€æµ‹")
        print(f"{'='*70}")
        print(f"è®­ç»ƒé›†æ ·æœ¬: {len(train_texts):>10,}")
        print(f"è¯„ä¼°é›†æ ·æœ¬: {len(eval_texts):>10,}")
        print(f"æ±¡æŸ“é˜ˆå€¼:   {threshold:>10.2f}")
        print(f"è®¾å¤‡:       {self.config.device:>10}")
        if self.config.device == "cuda":
            print(f"æ‰¹å¤„ç†:     {self.config.batch_size:>10,}")
        print(f"{'='*70}\n")
        
        # 1. è®¡ç®—è®­ç»ƒé›†åµŒå…¥
        print("[1/4] è®¡ç®—è®­ç»ƒé›†åµŒå…¥...")
        t1 = time.time()
        train_embeddings = self.compute_embeddings_batch(
            train_texts, 
            description="è®­ç»ƒé›†"
        )
        t1_duration = time.time() - t1
        print(f"âœ“ å®Œæˆ ({t1_duration:.2f}s, {len(train_texts)/t1_duration:,.0f} æ ·æœ¬/ç§’)\n")
        
        # 2. è®¡ç®—è¯„ä¼°é›†åµŒå…¥
        print("[2/4] è®¡ç®—è¯„ä¼°é›†åµŒå…¥...")
        t2 = time.time()
        eval_embeddings = self.compute_embeddings_batch(
            eval_texts,
            description="è¯„ä¼°é›†"
        )
        t2_duration = time.time() - t2
        print(f"âœ“ å®Œæˆ ({t2_duration:.2f}s, {len(eval_texts)/t2_duration:,.0f} æ ·æœ¬/ç§’)\n")
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        print("[3/4] è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        t3 = time.time()
        similarity_matrix = self.compute_similarity_matrix_gpu(
            train_embeddings, 
            eval_embeddings
        )
        t3_duration = time.time() - t3
        print(f"âœ“ å®Œæˆ ({t3_duration:.2f}s)\n")
        
        # 4. åˆ†ææ±¡æŸ“
        print("[4/4] åˆ†ææ±¡æŸ“æƒ…å†µ...")
        t4 = time.time()
        
        # è®¡ç®—æ¯ä¸ªè¯„ä¼°æ ·æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆC_scoreï¼‰
        c_scores_tensor = similarity_matrix.max(dim=1).values
        max_indices = similarity_matrix.max(dim=1).indices
        
        # è½¬æ¢åˆ° CPU ç”¨äºåç»­åˆ†æ
        c_scores = c_scores_tensor.cpu().numpy()
        max_idx = max_indices.cpu().numpy()
        
        # ç»Ÿè®¡ä¸åŒé£é™©ç­‰çº§
        critical = (c_scores >= 0.75).sum()
        high = ((c_scores >= 0.50) & (c_scores < 0.75)).sum()
        medium = ((c_scores >= 0.30) & (c_scores < 0.50)).sum()
        low = (c_scores < 0.30).sum()
        
        contaminated = (c_scores >= threshold).sum()
        contamination_rate = contaminated / len(eval_texts) if len(eval_texts) > 0 else 0
        
        t4_duration = time.time() - t4
        print(f"âœ“ å®Œæˆ ({t4_duration:.2f}s)\n")
        
        total_time = time.time() - start_time
        throughput = (len(train_texts) + len(eval_texts)) / total_time
        
        # æ„å»ºç»“æœ
        results = {
            'c_scores': c_scores,
            'contaminated_count': int(contaminated),
            'contamination_rate': float(contamination_rate),
            'risk_distribution': {
                'critical': int(critical),
                'high': int(high),
                'medium': int(medium),
                'low': int(low)
            },
            'timing': {
                'total': total_time,
                'train_embedding': t1_duration,
                'eval_embedding': t2_duration,
                'similarity': t3_duration,
                'analysis': t4_duration
            },
            'throughput': throughput,
            'device': self.config.device,
            'n_train': len(train_texts),
            'n_eval': len(eval_texts)
        }
        
        if return_details:
            results['max_train_indices'] = max_idx
            results['similarity_matrix'] = similarity_matrix.cpu().numpy()
        
        # æ‰“å°æ€»ç»“
        self._print_results(results)
        
        return results
    
    def _print_results(self, results: Dict):
        """æ‰“å°æ£€æµ‹ç»“æœ"""
        print(f"{'='*70}")
        print(f"æ£€æµ‹å®Œæˆ")
        print(f"{'='*70}")
        
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æ€»æ—¶é—´:         {results['timing']['total']:>10.3f} ç§’")
        print(f"  ååé‡:         {results['throughput']:>10,.0f} æ ·æœ¬/ç§’")
        print(f"  è®­ç»ƒé›†åµŒå…¥:     {results['timing']['train_embedding']:>10.3f} ç§’")
        print(f"  è¯„ä¼°é›†åµŒå…¥:     {results['timing']['eval_embedding']:>10.3f} ç§’")
        print(f"  ç›¸ä¼¼åº¦è®¡ç®—:     {results['timing']['similarity']:>10.3f} ç§’")
        
        print(f"\nğŸ” æ£€æµ‹ç»“æœ:")
        print(f"  æ±¡æŸ“æ ·æœ¬:       {results['contaminated_count']:>10,} "
              f"({results['contamination_rate']*100:>5.1f}%)")
        
        print(f"\nğŸ“Š é£é™©åˆ†å¸ƒ:")
        dist = results['risk_distribution']
        total = sum(dist.values())
        print(f"  ğŸ”´ å…³é”®é£é™©:    {dist['critical']:>10,} ({dist['critical']/total*100:>5.1f}%)")
        print(f"  ğŸŸ¡ é«˜é£é™©:      {dist['high']:>10,} ({dist['high']/total*100:>5.1f}%)")
        print(f"  ğŸŸ  ä¸­ç­‰é£é™©:    {dist['medium']:>10,} ({dist['medium']/total*100:>5.1f}%)")
        print(f"  ğŸŸ¢ ä½é£é™©:      {dist['low']:>10,} ({dist['low']/total*100:>5.1f}%)")
        
        print(f"\n{'='*70}\n")
    
    def benchmark_performance(
        self, 
        sample_sizes: List[int] = None,
        train_eval_ratio: float = 10.0
    ):
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            sample_sizes: è¦æµ‹è¯•çš„æ ·æœ¬è§„æ¨¡åˆ—è¡¨
            train_eval_ratio: è®­ç»ƒé›†/è¯„ä¼°é›†æ¯”ä¾‹
        """
        if sample_sizes is None:
            sample_sizes = [100, 1000, 10000, 50000]
        
        print(f"\n{'='*70}")
        print(f"GPU æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"{'='*70}")
        print(f"è®¾å¤‡: {self.config.device}")
        if self.config.device == "cuda":
            print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"è®­ç»ƒ/è¯„ä¼°æ¯”ä¾‹: {train_eval_ratio}:1")
        print(f"{'='*70}\n")
        
        results = []
        
        for size in sample_sizes:
            eval_size = int(size / train_eval_ratio)
            print(f"{'='*70}")
            print(f"æµ‹è¯•è§„æ¨¡: è®­ç»ƒé›† {size:,} æ ·æœ¬, è¯„ä¼°é›† {eval_size:,} æ ·æœ¬")
            print(f"{'='*70}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            train_texts = [f"Training sample {i}: This is a sample text for testing." for i in range(size)]
            eval_texts = [f"Evaluation sample {i}: This is a test sample." for i in range(eval_size)]
            
            # è¿è¡Œæ£€æµ‹
            result = self.detect_contamination(
                train_texts, 
                eval_texts, 
                threshold=0.75
            )
            
            results.append({
                'train_size': size,
                'eval_size': eval_size,
                'total_size': size + eval_size,
                'time': result['timing']['total'],
                'throughput': result['throughput'],
                'contamination_rate': result['contamination_rate']
            })
        
        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print(f"\n{'='*70}")
        print(f"æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print(f"{'='*70}\n")
        print(f"{'è®­ç»ƒé›†':>10} | {'è¯„ä¼°é›†':>10} | {'æ€»æ—¶é—´ (ç§’)':>12} | {'ååé‡ (æ ·æœ¬/ç§’)':>20}")
        print(f"{'-'*10}-+-{'-'*10}-+-{'-'*12}-+-{'-'*20}")
        for r in results:
            print(f"{r['train_size']:>10,} | {r['eval_size']:>10,} | "
                  f"{r['time']:>12.3f} | {r['throughput']:>20,.0f}")
        print(f"{'='*70}\n")
        
        # è®¡ç®—åŠ é€Ÿæ¯”ï¼ˆç›¸å¯¹äºç¬¬ä¸€ä¸ªæµ‹è¯•ï¼‰
        if len(results) > 1:
            base_throughput = results[0]['throughput']
            print(f"ååé‡æ‰©å±•æ€§:")
            for r in results:
                scale_factor = r['total_size'] / results[0]['total_size']
                efficiency = (r['throughput'] / base_throughput) / scale_factor * 100
                print(f"  {r['total_size']:>10,} æ ·æœ¬: {efficiency:>6.1f}% æ•ˆç‡")
            print()
        
        return results
    
    def compare_with_cpu(self, sample_size: int = 10000):
        """
        GPU vs CPU æ€§èƒ½å¯¹æ¯”
        
        Args:
            sample_size: æµ‹è¯•æ ·æœ¬è§„æ¨¡
        """
        print(f"\n{'='*70}")
        print(f"GPU vs CPU æ€§èƒ½å¯¹æ¯”")
        print(f"{'='*70}\n")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        eval_size = sample_size // 10
        train_texts = [f"Training sample {i}" for i in range(sample_size)]
        eval_texts = [f"Evaluation sample {i}" for i in range(eval_size)]
        
        results = {}
        
        # GPU æµ‹è¯•
        if torch.cuda.is_available():
            print("è¿è¡Œ GPU æµ‹è¯•...")
            self.config.device = "cuda"
            self.model = self.model.to("cuda")
            result_gpu = self.detect_contamination(train_texts, eval_texts)
            results['gpu'] = result_gpu
        
        # CPU æµ‹è¯•
        print("è¿è¡Œ CPU æµ‹è¯•...")
        self.config.device = "cpu"
        self.model = self.model.to("cpu")
        result_cpu = self.detect_contamination(train_texts, eval_texts)
        results['cpu'] = result_cpu
        
        # å¯¹æ¯”
        if 'gpu' in results and 'cpu' in results:
            speedup = results['cpu']['timing']['total'] / results['gpu']['timing']['total']
            throughput_improvement = results['gpu']['throughput'] / results['cpu']['throughput']
            
            print(f"\n{'='*70}")
            print(f"å¯¹æ¯”æ€»ç»“")
            print(f"{'='*70}\n")
            print(f"{'æŒ‡æ ‡':<20} | {'GPU':>15} | {'CPU':>15} | {'åŠ é€Ÿæ¯”':>10}")
            print(f"{'-'*20}-+-{'-'*15}-+-{'-'*15}-+-{'-'*10}")
            print(f"{'æ€»æ—¶é—´ (ç§’)':<20} | {results['gpu']['timing']['total']:>15.3f} | "
                  f"{results['cpu']['timing']['total']:>15.3f} | {speedup:>10.1f}x")
            print(f"{'ååé‡ (æ ·æœ¬/ç§’)':<20} | {results['gpu']['throughput']:>15,.0f} | "
                  f"{results['cpu']['throughput']:>15,.0f} | {throughput_improvement:>10.1f}x")
            print(f"{'='*70}\n")
            
            print(f"ğŸš€ GPU åŠ é€Ÿæ¯”: {speedup:.1f}x")
            print(f"âš¡ ååé‡æå‡: {throughput_improvement:.1f}x\n")
        
        return results


def main():
    """æ¼”ç¤º GPU åŠ é€Ÿæ£€æµ‹"""
    
    # æ£€æŸ¥ CUDA
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼")
        print("è¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„ PyTorch GPU ç‰ˆæœ¬")
        print("å®‰è£…å‘½ä»¤: pip install torch --index-url https://download.pytorch.org/whl/cu118\n")
    
    # é…ç½®
    config = GPUConfig(
        device="cuda" if torch.cuda.is_available() else "cpu",
        batch_size=512,
        use_fp16=True
    )
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = CBDDetectorGPU(config)
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆæ¨èï¼‰")
    print("2. GPU vs CPU å¯¹æ¯”")
    print("3. å¿«é€Ÿæµ‹è¯•ï¼ˆ1k æ ·æœ¬ï¼‰")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-3ï¼Œé»˜è®¤ 1): ").strip() or "1"
        
        if choice == "1":
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            detector.benchmark_performance(
                sample_sizes=[100, 1000, 10000, 50000]
            )
        elif choice == "2":
            # GPU vs CPU å¯¹æ¯”
            if torch.cuda.is_available():
                detector.compare_with_cpu(sample_size=10000)
            else:
                print("é”™è¯¯: GPU ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
        elif choice == "3":
            # å¿«é€Ÿæµ‹è¯•
            detector.benchmark_performance(
                sample_sizes=[1000]
            )
        else:
            print("æ— æ•ˆçš„é€‰é¡¹")
    
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
