{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Test Visualization\n",
    "\n",
    "This notebook demonstrates how to visualize permutation test results and understand the null distribution of bias detection metrics.\n",
    "\n",
    "## Contents\n",
    "1. Basic permutation test visualization\n",
    "2. Comparing observed vs null distribution\n",
    "3. Multiple metrics comparison\n",
    "4. Model card generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from circular_bias_detector.core.permutation import permutation_test, adaptive_permutation_test\n",
    "from circular_bias_detector.core.metrics import compute_psi, compute_ccs, compute_rho_pc\n",
    "from circular_bias_detector.detection import BiasDetector\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic evaluation data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Scenario 1: No bias (random data)\n",
    "T, K, p = 20, 5, 3\n",
    "perf_no_bias = np.random.rand(T, K)\n",
    "const_no_bias = np.random.rand(T, p)\n",
    "\n",
    "# Scenario 2: With bias (correlated performance and constraints)\n",
    "perf_with_bias = np.random.rand(T, K)\n",
    "const_with_bias = np.random.rand(T, p)\n",
    "# Add correlation\n",
    "for i in range(T):\n",
    "    perf_with_bias[i, :] += 0.5 * np.mean(const_with_bias[i, :])\n",
    "\n",
    "print(\"Data generated:\")\n",
    "print(f\"  Time periods: {T}\")\n",
    "print(f\"  Algorithms: {K}\")\n",
    "print(f\"  Constraints: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run permutation tests for both scenarios\n",
    "n_perm = 1000\n",
    "\n",
    "print(\"Running permutation tests...\")\n",
    "\n",
    "# No bias scenario\n",
    "results_no_bias = permutation_test(\n",
    "    perf_no_bias, const_no_bias, compute_psi,\n",
    "    n_permutations=n_perm, random_seed=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# With bias scenario\n",
    "results_with_bias = permutation_test(\n",
    "    perf_with_bias, const_with_bias, compute_psi,\n",
    "    n_permutations=n_perm, random_seed=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nNo Bias Scenario:\")\n",
    "print(f\"  Observed PSI: {results_no_bias['observed']:.4f}\")\n",
    "print(f\"  p-value: {results_no_bias['p_value']:.4f}\")\n",
    "\n",
    "print(f\"\\nWith Bias Scenario:\")\n",
    "print(f\"  Observed PSI: {results_with_bias['observed']:.4f}\")\n",
    "print(f\"  p-value: {results_with_bias['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Permutation Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_distribution(results, title=\"Permutation Test Results\"):\n",
    "    \"\"\"\n",
    "    Plot permutation distribution with observed value.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot histogram of permuted values\n",
    "    ax.hist(results['permuted_values'], bins=50, alpha=0.7, \n",
    "            color='skyblue', edgecolor='black', density=True, label='Null distribution')\n",
    "    \n",
    "    # Plot observed value\n",
    "    ax.axvline(results['observed'], color='red', linestyle='--', \n",
    "               linewidth=2, label=f\"Observed: {results['observed']:.4f}\")\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    ax.axvline(results['ci_lower'], color='orange', linestyle=':', \n",
    "               linewidth=1.5, alpha=0.7, label=f\"95% CI\")\n",
    "    ax.axvline(results['ci_upper'], color='orange', linestyle=':', \n",
    "               linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    # Add p-value annotation\n",
    "    ax.text(0.02, 0.98, f\"p-value = {results['p_value']:.4f}\\nn = {results['n_permutations']}\",\n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "            fontsize=11)\n",
    "    \n",
    "    ax.set_xlabel('PSI Score', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot both scenarios\n",
    "fig1 = plot_permutation_distribution(results_no_bias, \"No Bias Scenario\")\n",
    "plt.show()\n",
    "\n",
    "fig2 = plot_permutation_distribution(results_with_bias, \"With Bias Scenario\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run permutation tests for all three metrics\n",
    "metrics = {\n",
    "    'PSI': compute_psi,\n",
    "    'CCS': lambda p, c: compute_ccs(c),\n",
    "    'œÅ_PC': compute_rho_pc\n",
    "}\n",
    "\n",
    "results_all = {}\n",
    "for name, metric_func in metrics.items():\n",
    "    results_all[name] = permutation_test(\n",
    "        perf_with_bias, const_with_bias, metric_func,\n",
    "        n_permutations=500, random_seed=42, n_jobs=-1\n",
    "    )\n",
    "    print(f\"{name}: observed={results_all[name]['observed']:.4f}, p={results_all[name]['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all metrics together\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, results) in enumerate(results_all.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(results['permuted_values'], bins=40, alpha=0.7, \n",
    "            color='skyblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # Observed value\n",
    "    ax.axvline(results['observed'], color='red', linestyle='--', \n",
    "               linewidth=2, label=f\"Observed: {results['observed']:.4f}\")\n",
    "    \n",
    "    # CI\n",
    "    ax.axvline(results['ci_lower'], color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    ax.axvline(results['ci_upper'], color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    # Annotations\n",
    "    significance = \"***\" if results['p_value'] < 0.001 else \"**\" if results['p_value'] < 0.01 else \"*\" if results['p_value'] < 0.05 else \"ns\"\n",
    "    ax.text(0.02, 0.98, f\"p = {results['p_value']:.4f} {significance}\",\n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel(f'{name} Score', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(name, fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Permutation Test Results for All Metrics', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adaptive Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adaptive permutation test\n",
    "adaptive_results = adaptive_permutation_test(\n",
    "    perf_with_bias, const_with_bias, compute_psi,\n",
    "    max_permutations=5000,\n",
    "    min_permutations=100,\n",
    "    precision=0.01,\n",
    "    random_seed=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nAdaptive Permutation Test:\")\n",
    "print(f\"  Converged: {adaptive_results['converged']}\")\n",
    "print(f\"  Permutations used: {adaptive_results['n_permutations']} / {adaptive_results['max_permutations']}\")\n",
    "print(f\"  p-value: {adaptive_results['p_value']:.4f}\")\n",
    "print(f\"  Observed: {adaptive_results['observed']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(perf_matrix, const_matrix, results_dict, algorithm_names=None):\n",
    "    \"\"\"\n",
    "    Generate a model card summarizing bias detection results.\n",
    "    \"\"\"\n",
    "    T, K = perf_matrix.shape\n",
    "    _, p = const_matrix.shape\n",
    "    \n",
    "    if algorithm_names is None:\n",
    "        algorithm_names = [f\"Algorithm_{i+1}\" for i in range(K)]\n",
    "    \n",
    "    card = f\"\"\"\n",
    "# Bias Detection Model Card\n",
    "\n",
    "## Evaluation Metadata\n",
    "- **Time Periods**: {T}\n",
    "- **Algorithms Evaluated**: {K}\n",
    "- **Constraint Types**: {p}\n",
    "- **Algorithm Names**: {', '.join(algorithm_names)}\n",
    "\n",
    "## Statistical Testing\n",
    "- **Method**: Permutation Test\n",
    "- **Permutations**: {results_dict['PSI']['n_permutations']}\n",
    "- **Significance Level**: Œ± = 0.05\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "### PSI (Parameter Stability Index)\n",
    "- **Observed**: {results_dict['PSI']['observed']:.4f}\n",
    "- **95% CI**: [{results_dict['PSI']['ci_lower']:.4f}, {results_dict['PSI']['ci_upper']:.4f}]\n",
    "- **p-value**: {results_dict['PSI']['p_value']:.4f}\n",
    "- **Interpretation**: {'‚ö†Ô∏è UNSTABLE' if results_dict['PSI']['p_value'] < 0.05 else '‚úÖ STABLE'}\n",
    "\n",
    "### CCS (Constraint Consistency Score)\n",
    "- **Observed**: {results_dict['CCS']['observed']:.4f}\n",
    "- **95% CI**: [{results_dict['CCS']['ci_lower']:.4f}, {results_dict['CCS']['ci_upper']:.4f}]\n",
    "- **p-value**: {results_dict['CCS']['p_value']:.4f}\n",
    "- **Interpretation**: {'‚ö†Ô∏è INCONSISTENT' if results_dict['CCS']['p_value'] < 0.05 else '‚úÖ CONSISTENT'}\n",
    "\n",
    "### œÅ_PC (Performance-Constraint Correlation)\n",
    "- **Observed**: {results_dict['œÅ_PC']['observed']:.4f}\n",
    "- **95% CI**: [{results_dict['œÅ_PC']['ci_lower']:.4f}, {results_dict['œÅ_PC']['ci_upper']:.4f}]\n",
    "- **p-value**: {results_dict['œÅ_PC']['p_value']:.4f}\n",
    "- **Interpretation**: {'‚ö†Ô∏è CORRELATED' if results_dict['œÅ_PC']['p_value'] < 0.05 else '‚úÖ INDEPENDENT'}\n",
    "\n",
    "## Overall Assessment\n",
    "\n",
    "{'üö® **BIAS DETECTED**: Multiple indicators suggest circular reasoning bias in the evaluation process.' if sum(r['p_value'] < 0.05 for r in results_dict.values()) >= 2 else '‚úÖ **NO BIAS DETECTED**: Evaluation appears free from circular reasoning bias.'}\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "{'- Review evaluation methodology for potential circular dependencies\\n- Consider independent validation dataset\\n- Audit constraint specification process' if sum(r['p_value'] < 0.05 for r in results_dict.values()) >= 2 else '- Continue current evaluation practices\\n- Maintain documentation of methodology\\n- Periodic re-evaluation recommended'}\n",
    "\n",
    "---\n",
    "*Generated by Sleuth - Circular Bias Detector*\n",
    "\"\"\"\n",
    "    return card\n",
    "\n",
    "# Generate and display model card\n",
    "model_card = generate_model_card(\n",
    "    perf_with_bias, const_with_bias, results_all,\n",
    "    algorithm_names=[f\"Model_{chr(65+i)}\" for i in range(perf_with_bias.shape[1])]\n",
    ")\n",
    "\n",
    "print(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model card to file\n",
    "with open('bias_detection_model_card.md', 'w') as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card saved to: bias_detection_model_card.md\")\n",
    "\n",
    "# Save figures\n",
    "fig1.savefig('permutation_no_bias.png', dpi=300, bbox_inches='tight')\n",
    "fig2.savefig('permutation_with_bias.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"Figures saved:\")\n",
    "print(\"  - permutation_no_bias.png\")\n",
    "print(\"  - permutation_with_bias.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive plot (requires plotly)\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('PSI', 'CCS', 'œÅ_PC')\n",
    "    )\n",
    "    \n",
    "    for idx, (name, results) in enumerate(results_all.items(), 1):\n",
    "        # Histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=results['permuted_values'],\n",
    "                name=f'{name} Null',\n",
    "                opacity=0.7,\n",
    "                nbinsx=40,\n",
    "                histnorm='probability density'\n",
    "            ),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "        \n",
    "        # Observed line\n",
    "        fig.add_vline(\n",
    "            x=results['observed'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            annotation_text=f\"Observed: {results['observed']:.3f}\",\n",
    "            row=1, col=idx\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive Permutation Test Results\",\n",
    "        showlegend=False,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Plotly not installed. Install with: pip install plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Running permutation tests with parallel processing\n",
    "2. ‚úÖ Visualizing null distributions and observed values\n",
    "3. ‚úÖ Comparing multiple bias detection metrics\n",
    "4. ‚úÖ Using adaptive permutation testing for efficiency\n",
    "5. ‚úÖ Generating model cards for audit trails\n",
    "\n",
    "For more information, see the [documentation](https://github.com/hongping-zh/circular-bias-detection)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
