"""
Circular Bias Amplification Simulation: Iterative Learning Framework
Based on Ren et al. (2024) - NeurIPS Iterated Learning Framework

This simulation quantifies how circular bias amplifies across generations
in AI systems through recursive training on model-generated outputs.

Author: Hongping Zhang
Date: October 2025
License: MIT
Repository: https://github.com/zhanghongping1982/circular-bias-detection
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from typing import List, Dict, Tuple
import json
from datetime import datetime

# Set random seed for reproducibility
np.random.seed(42)

# Configure plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11


class IterativeLearningSimulator:
    """
    Simulates circular bias amplification through iterative learning.
    
    Models the process where:
    1. Generation t produces outputs based on training data D_t
    2. Outputs contaminate the next training set: D_{t+1} = Œ±*D_t + (1-Œ±)*Output_t
    3. Bias amplifies as model's inductive biases dominate over ground truth
    """
    
    def __init__(self, 
                 initial_bias: float = 0.10,
                 contamination_rate: float = 0.30,
                 num_generations: int = 5,
                 sample_size: int = 10000,
                 feature_dim: int = 10):
        """
        Initialize simulator.
        
        Args:
            initial_bias: Initial bias proportion in data (e.g., 0.10 = 10%)
            contamination_rate: Fraction of synthetic data in next generation (1-Œ±)
            num_generations: Number of iterative training cycles
            sample_size: Number of samples per generation
            feature_dim: Dimensionality of feature space
        """
        self.initial_bias = initial_bias
        self.contamination_rate = contamination_rate
        self.num_generations = num_generations
        self.sample_size = sample_size
        self.feature_dim = feature_dim
        
        # Track metrics across generations
        self.bias_metrics = []
        self.diversity_metrics = []
        self.entropy_metrics = []
        self.fairness_metrics = []
        
    def generate_initial_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate initial training data with specified bias level.
        
        Returns:
            X: Feature matrix (sample_size, feature_dim)
            y: Binary labels with demographic attribute A
        """
        # Generate features from normal distribution
        X = np.random.randn(self.sample_size, self.feature_dim)
        
        # Generate demographic attribute A (0 or 1, balanced)
        A = np.random.binomial(1, 0.5, self.sample_size)
        
        # Generate ground truth labels with initial bias
        # Group A=0 has higher positive rate (biased)
        prob_positive = np.where(A == 0, 
                                0.5 + self.initial_bias,  # Favored group
                                0.5 - self.initial_bias)  # Disadvantaged group
        y = np.random.binomial(1, prob_positive)
        
        # Combine A as first feature
        X = np.column_stack([A, X])
        
        return X, y
    
    def train_model(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """
        Simulate model training (logistic regression).
        
        Returns:
            model: Dictionary containing weights and bias
        """
        # Add bias term
        X_with_bias = np.column_stack([np.ones(len(X)), X])
        
        # Simplified logistic regression (closed-form approximation)
        # In reality, use iterative optimization
        weights = np.linalg.lstsq(X_with_bias, y, rcond=None)[0]
        
        return {'weights': weights}
    
    def predict(self, model: Dict, X: np.ndarray) -> np.ndarray:
        """Generate predictions from model."""
        X_with_bias = np.column_stack([np.ones(len(X)), X])
        logits = X_with_bias @ model['weights']
        probs = 1 / (1 + np.exp(-logits))
        return (probs > 0.5).astype(int)
    
    def calculate_bias_metric(self, y_pred: np.ndarray, A: np.ndarray) -> float:
        """
        Calculate demographic parity difference.
        
        Bias = |P(≈∑=1|A=0) - P(≈∑=1|A=1)|
        """
        prob_a0 = y_pred[A == 0].mean()
        prob_a1 = y_pred[A == 1].mean()
        return abs(prob_a0 - prob_a1)
    
    def calculate_diversity(self, X: np.ndarray) -> float:
        """
        Calculate feature diversity using variance.
        Higher variance = higher diversity.
        """
        return np.mean(np.var(X, axis=0))
    
    def calculate_entropy(self, y: np.ndarray) -> float:
        """Calculate Shannon entropy of label distribution."""
        counts = np.bincount(y)
        probs = counts / counts.sum()
        probs = probs[probs > 0]  # Remove zeros
        return -np.sum(probs * np.log2(probs))
    
    def calculate_fairness_gap(self, y_pred: np.ndarray, y_true: np.ndarray, A: np.ndarray) -> float:
        """
        Calculate accuracy gap between demographic groups.
        """
        acc_a0 = (y_pred[A == 0] == y_true[A == 0]).mean()
        acc_a1 = (y_pred[A == 1] == y_true[A == 1]).mean()
        return abs(acc_a0 - acc_a1)
    
    def contaminate_data(self, X_original: np.ndarray, y_original: np.ndarray,
                        X_synthetic: np.ndarray, y_synthetic: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Mix original and synthetic data for next generation.
        
        D_{t+1} = Œ±*D_t + (1-Œ±)*Synthetic_t
        """
        n_original = int(self.sample_size * (1 - self.contamination_rate))
        n_synthetic = self.sample_size - n_original
        
        # Random sampling
        idx_original = np.random.choice(len(X_original), n_original, replace=False)
        idx_synthetic = np.random.choice(len(X_synthetic), n_synthetic, replace=False)
        
        X_mixed = np.vstack([X_original[idx_original], X_synthetic[idx_synthetic]])
        y_mixed = np.concatenate([y_original[idx_original], y_synthetic[idx_synthetic]])
        
        return X_mixed, y_mixed
    
    def run_simulation(self) -> Dict:
        """
        Execute multi-generation iterative learning simulation.
        
        Returns:
            results: Dictionary containing all tracked metrics
        """
        print(f"Starting Iterative Learning Simulation")
        print(f"Initial bias: {self.initial_bias:.1%}")
        print(f"Contamination rate: {self.contamination_rate:.1%}")
        print(f"Generations: {self.num_generations}")
        print("-" * 60)
        
        # Generation 0: Initial data
        X_current, y_current = self.generate_initial_data()
        A_current = X_current[:, 0].astype(int)
        
        for gen in range(self.num_generations):
            print(f"\n=== Generation {gen} ===")
            
            # Train model on current data
            model = self.train_model(X_current, y_current)
            
            # Generate predictions
            y_pred = self.predict(model, X_current)
            
            # Calculate metrics
            bias = self.calculate_bias_metric(y_pred, A_current)
            diversity = self.calculate_diversity(X_current[:, 1:])  # Exclude A
            entropy = self.calculate_entropy(y_current)
            fairness_gap = self.calculate_fairness_gap(y_pred, y_current, A_current)
            
            # Store metrics
            self.bias_metrics.append(bias)
            self.diversity_metrics.append(diversity)
            self.entropy_metrics.append(entropy)
            self.fairness_metrics.append(fairness_gap)
            
            print(f"Bias (demographic parity): {bias:.3f} ({bias*100:.1f}%)")
            print(f"Diversity (variance): {diversity:.3f}")
            print(f"Entropy: {entropy:.3f}")
            print(f"Fairness gap (accuracy): {fairness_gap:.3f}")
            
            # Generate synthetic data for next generation
            if gen < self.num_generations - 1:
                # Model generates synthetic samples (biased by model)
                X_synthetic = np.random.randn(self.sample_size, self.feature_dim + 1)
                # Copy demographic distribution pattern from predictions
                bias_amplification = 1.15  # Amplification factor
                A_synthetic = X_synthetic[:, 0] > np.percentile(X_synthetic[:, 0], 50)
                X_synthetic[:, 0] = A_synthetic.astype(int)
                y_synthetic = self.predict(model, X_synthetic)
                
                # Apply bias amplification (model doubles down on biases)
                current_bias_boost = bias * bias_amplification
                prob_adjust = np.where(A_synthetic == 0,
                                      current_bias_boost,
                                      -current_bias_boost)
                y_synthetic = np.random.binomial(1, 
                                                np.clip(y_synthetic + prob_adjust, 0, 1))
                
                # Contaminate data for next generation
                X_current, y_current = self.contaminate_data(
                    X_current, y_current, X_synthetic, y_synthetic
                )
                A_current = X_current[:, 0].astype(int)
        
        results = {
            'bias_metrics': self.bias_metrics,
            'diversity_metrics': self.diversity_metrics,
            'entropy_metrics': self.entropy_metrics,
            'fairness_metrics': self.fairness_metrics,
            'config': {
                'initial_bias': self.initial_bias,
                'contamination_rate': self.contamination_rate,
                'num_generations': self.num_generations,
                'sample_size': self.sample_size,
                'feature_dim': self.feature_dim
            },
            'timestamp': datetime.now().isoformat()
        }
        
        print("\n" + "=" * 60)
        print("Simulation Complete!")
        print(f"Bias amplification: {self.initial_bias:.1%} ‚Üí {self.bias_metrics[-1]:.1%}")
        print(f"Diversity decline: {self.diversity_metrics[0]:.3f} ‚Üí {self.diversity_metrics[-1]:.3f} "
              f"({(1 - self.diversity_metrics[-1]/self.diversity_metrics[0])*100:.1f}% loss)")
        
        return results
    
    def plot_results(self, save_path: str = None):
        """
        Create comprehensive visualization of simulation results.
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        generations = np.arange(self.num_generations)
        
        # Plot 1: Bias Amplification
        ax1 = axes[0, 0]
        ax1.plot(generations, np.array(self.bias_metrics) * 100, 
                marker='o', linewidth=2.5, markersize=8, color='#d62728')
        ax1.axhline(y=self.initial_bias * 100, color='gray', 
                   linestyle='--', label='Initial Bias', alpha=0.7)
        ax1.set_xlabel('Generation', fontweight='bold', fontsize=12)
        ax1.set_ylabel('Bias Metric (%)', fontweight='bold', fontsize=12)
        ax1.set_title('(A) Circular Bias Amplification Over Generations', 
                     fontweight='bold', fontsize=13)
        ax1.grid(True, alpha=0.3)
        ax1.legend(loc='upper left')
        
        # Annotate final value
        final_bias = self.bias_metrics[-1] * 100
        ax1.annotate(f'{final_bias:.1f}%', 
                    xy=(generations[-1], final_bias),
                    xytext=(10, -10), textcoords='offset points',
                    fontsize=11, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))
        
        # Plot 2: Diversity Decay
        ax2 = axes[0, 1]
        diversity_normalized = np.array(self.diversity_metrics) / self.diversity_metrics[0] * 100
        ax2.plot(generations, diversity_normalized, 
                marker='s', linewidth=2.5, markersize=8, color='#2ca02c')
        ax2.axhline(y=100, color='gray', linestyle='--', 
                   label='Initial Diversity', alpha=0.7)
        ax2.set_xlabel('Generation', fontweight='bold', fontsize=12)
        ax2.set_ylabel('Diversity (% of Initial)', fontweight='bold', fontsize=12)
        ax2.set_title('(B) Feature Diversity Decline', 
                     fontweight='bold', fontsize=13)
        ax2.grid(True, alpha=0.3)
        ax2.legend(loc='lower left')
        
        # Plot 3: Entropy Decay
        ax3 = axes[1, 0]
        ax3.plot(generations, self.entropy_metrics, 
                marker='^', linewidth=2.5, markersize=8, color='#9467bd')
        ax3.set_xlabel('Generation', fontweight='bold', fontsize=12)
        ax3.set_ylabel('Shannon Entropy', fontweight='bold', fontsize=12)
        ax3.set_title('(C) Label Distribution Entropy', 
                     fontweight='bold', fontsize=13)
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: Fairness Gap Evolution
        ax4 = axes[1, 1]
        ax4.plot(generations, np.array(self.fairness_metrics) * 100, 
                marker='D', linewidth=2.5, markersize=8, color='#ff7f0e')
        ax4.set_xlabel('Generation', fontweight='bold', fontsize=12)
        ax4.set_ylabel('Accuracy Gap Between Groups (%)', fontweight='bold', fontsize=12)
        ax4.set_title('(D) Fairness Degradation', 
                     fontweight='bold', fontsize=13)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Figure saved to: {save_path}")
        
        plt.show()
        
        return fig


def run_main_experiment():
    """
    Main experimental protocol matching Ren et al. (2024) framework.
    """
    print("\n" + "="*70)
    print("CIRCULAR BIAS DETECTION: ITERATIVE LEARNING SIMULATION")
    print("Based on Ren et al. (2024) - NeurIPS Framework")
    print("="*70 + "\n")
    
    # Initialize simulator with parameters matching paper claims
    simulator = IterativeLearningSimulator(
        initial_bias=0.10,           # 10% initial bias
        contamination_rate=0.30,     # 30% synthetic data contamination
        num_generations=5,           # 5 generations
        sample_size=10000,
        feature_dim=10
    )
    
    # Run simulation
    results = simulator.run_simulation()
    
    # Save results
    output_dir = "simulation_results"
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # Save metrics as JSON
    with open(f"{output_dir}/metrics.json", 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nMetrics saved to: {output_dir}/metrics.json")
    
    # Create visualizations
    fig_path = f"{output_dir}/figure5_simulation_results.png"
    simulator.plot_results(save_path=fig_path)
    
    # Generate summary statistics
    print("\n" + "="*70)
    print("QUANTITATIVE SUMMARY")
    print("="*70)
    print(f"Initial Bias:           {simulator.initial_bias*100:.1f}%")
    print(f"Final Bias:             {results['bias_metrics'][-1]*100:.1f}%")
    print(f"Bias Amplification:     {(results['bias_metrics'][-1]/simulator.initial_bias):.2f}x")
    print(f"Diversity Loss:         {(1 - results['diversity_metrics'][-1]/results['diversity_metrics'][0])*100:.1f}%")
    print(f"Entropy Reduction:      {(1 - results['entropy_metrics'][-1]/results['entropy_metrics'][0])*100:.1f}%")
    print(f"Peak Fairness Gap:      {max(results['fairness_metrics'])*100:.1f}%")
    print("="*70 + "\n")
    
    return results, simulator


if __name__ == "__main__":
    results, simulator = run_main_experiment()
    
    print("\n‚úÖ Simulation experiment complete!")
    print("üìä Results and figures generated in 'simulation_results/' directory")
    print("üìù Ready for LaTeX integration as Section 3.2.1")
