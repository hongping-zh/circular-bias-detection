# Circular Bias Detection Synthetic Citation Datasets

This document describes the synthetic citation datasets generated by `generate_data.py` and how to reproduce, use, and cite them.

## Files

- `*_edges_labeled.csv`
- `*_nodes.csv`
- `*_params.json`
- Legacy edge list: `*.csv` (kept for backward compatibility)

## Schemas

- Edges (labeled):
  - `citing_paper_id` (int)
  - `cited_paper_id` (int)
  - `edge_source` (string; `base` or `bias`)
  - `edge_is_biased` (int; 1 if injected intra-group edge, else 0)
  - `citing_publication_year` (int)
  - `cited_publication_year` (int)

- Nodes:
  - `paper_id` (int)
  - `publication_year` (int)
  - `biased_group_id` (int; -1 if not in a biased group)
  - `is_biased_member` (int; 1 if in a biased group, else 0)

- Params JSON:
  - Records generation parameters (e.g., `num_papers`, `years`, `group` settings, `seed`, `timestamp`, `out_prefix`).

## Generation

Run from the repository root:

```bash
# Small dataset (deterministic)
python generate_data.py

# Custom run with explicit seed and prefix
python -c "from generate_data import generate_citation_dataset; generate_citation_dataset(num_papers=1000, start_year=2010, end_year=2023, base_citation_rate=1.5, num_biased_groups=5, biased_group_size=4, bias_intensity=2, file_name='synthetic_custom.csv', seed=123, out_prefix='synthetic_custom')"

# Negative control (no biased groups):
python -c "from generate_data import generate_citation_dataset; generate_citation_dataset(num_papers=1000, num_biased_groups=0, file_name='neg_control.csv', out_prefix='neg_control', seed=42)"
```

Notes:
- Base edges are generated with the constraint `citing_year > cited_year` to avoid cycles in the healthy network.
- Biased groups are then injected by setting group members to the same year and adding intra-group edges, which can create circular structures by design.

## Intended Use

- Designed to validate circular (in-group) citation bias detection methods.
- Synthetic and safe to share publicly.
- Provides ground-truth labels at edge and node level for quantitative evaluation (precision/recall/F1).

## Example Evaluation

Use `evaluate_citation_bias.py` to compute simple baseline metrics on small/medium datasets. See that script for details.

## License

- Data: intended to be distributed under the same terms as the repository; if you prefer a data-specific license, we recommend CC0 or CC-BY. Update this section to reflect the chosen license.

## Citation and Archival (Zenodo)

- Archive the exact CSVs and `generate_data.py` on Zenodo and obtain a DOI.
- Cite the DOI in this README and in the JOSS manuscript's Data Availability section.
- Reference the Git release tag (e.g., `v0.1.0-data`) that the deposit points to.
