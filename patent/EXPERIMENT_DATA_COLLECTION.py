#!/usr/bin/env python3
"""
ÂÆûÈ™åÊï∞ÊçÆÊî∂ÈõÜ‰∏éÊï¥ÁêÜÂ∑•ÂÖ∑ - ‰∏ìÂà©Áî≥ËØ∑ÊùêÊñôÂáÜÂ§á
Experiment Data Collection for Patent Application

ÂäüËÉΩÔºö
1. Êî∂ÈõÜÊâÄÊúâÂÆûÈ™åÊï∞ÊçÆ
2. ÁîüÊàêÂØπÊØîË°®Ê†ºÂíåÂõæË°®
3. Êï¥ÁêÜ‰∏ìÂà©Áî≥ËØ∑ÊâÄÈúÄÁöÑÂÆûÈ™åËØÅÊçÆ
4. ÁîüÊàêExcel/PDFÊä•Âëä
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from datetime import datetime
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from circular_bias_detector import BiasDetector
from circular_bias_detector.advanced_metrics import compute_all_advanced_metrics
from circular_bias_detector.utils import create_synthetic_data


class ExperimentDataCollector:
    """ÂÆûÈ™åÊï∞ÊçÆÊî∂ÈõÜÂô®"""
    
    def __init__(self, output_dir='./patent_experiments'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # ÂõæË°®ËæìÂá∫ÁõÆÂΩï
        self.figures_dir = self.output_dir / 'figures'
        self.figures_dir.mkdir(exist_ok=True)
        
        # Êï∞ÊçÆÂ≠òÂÇ®
        self.experiment_data = {
            'metadata': {
                'date': datetime.now().isoformat(),
                'purpose': 'Patent Application Supporting Evidence'
            },
            'comparative_experiments': [],
            'performance_metrics': {},
            'statistical_tests': {}
        }
    
    def generate_comparison_data(self, n_samples=50):
        """ÁîüÊàêÂØπÊØîÂÆûÈ™åÊï∞ÊçÆ"""
        print("üìä ÁîüÊàêÂØπÊØîÂÆûÈ™åÊï∞ÊçÆ...")
        
        traditional_results = []
        advanced_results = []
        
        # ÁîüÊàê‰∏çÂêåÂÅèÂ∑ÆÂº∫Â∫¶ÁöÑÊ†∑Êú¨
        bias_levels = np.linspace(0, 1, 11)
        
        for bias in bias_levels:
            for _ in range(5):  # ÊØè‰∏™Ê∞¥Âπ≥5Ê¨°ÈáçÂ§ç
                # ÁîüÊàêÊï∞ÊçÆ
                perf, const = create_synthetic_data(
                    n_time_periods=15,
                    n_algorithms=4,
                    n_constraints=3,
                    bias_intensity=bias,
                    random_seed=None
                )
                
                # ‰º†ÁªüÊñπÊ≥ï
                trad_detector = BiasDetector()
                trad_res = trad_detector.detect_bias(perf, const)
                
                # Êñ∞ÊñπÊ≥ï
                adv_res = compute_all_advanced_metrics(perf, const)
                
                # ÁªºÂêàÂà§Êñ≠
                adv_bias_score = np.mean([
                    adv_res['tdi'] / 0.6,  # ÂΩí‰∏ÄÂåñ
                    max(0, -adv_res['ics'] / 0.5),
                    adv_res['ads'] / 0.3,
                    adv_res['mci'] / 0.8
                ])
                
                traditional_results.append({
                    'bias_level': bias,
                    'detected': trad_res['overall_bias'],
                    'confidence': trad_res['confidence'],
                    'method': 'Traditional'
                })
                
                advanced_results.append({
                    'bias_level': bias,
                    'detected': adv_bias_score > 0.5,
                    'confidence': min(adv_bias_score, 1.0),
                    'method': 'Advanced'
                })
        
        # ÂêàÂπ∂Êï∞ÊçÆ
        df = pd.DataFrame(traditional_results + advanced_results)
        
        return df
    
    def analyze_performance(self, df):
        """ÂàÜÊûêÊÄßËÉΩÊåáÊ†á"""
        print("\nüìà ÂàÜÊûêÊÄßËÉΩÊåáÊ†á...")
        
        results = {}
        
        for method in ['Traditional', 'Advanced']:
            method_data = df[df['method'] == method]
            
            # ËÆ°ÁÆóÂáÜÁ°ÆÁéáÔºà‰ª•ground truth‰∏∫Âü∫ÂáÜÔºâ
            # ÂÅáËÆæ bias_level > 0.5 Â∫îËØ•Ê£ÄÊµã‰∏∫ÊúâÂÅèÂ∑Æ
            y_true = (method_data['bias_level'] > 0.5).astype(int)
            y_pred = method_data['detected'].astype(int)
            
            # ÂáÜÁ°ÆÁéá
            accuracy = (y_true == y_pred).mean()
            
            # Âè¨ÂõûÁéá (ÁúüÈò≥ÊÄßÁéá)
            true_bias_samples = y_true == 1
            recall = y_pred[true_bias_samples].mean() if true_bias_samples.sum() > 0 else 0
            
            # Á≤æÁ°ÆÁéá
            detected_samples = y_pred == 1
            precision = y_true[detected_samples].mean() if detected_samples.sum() > 0 else 0
            
            # F1ÂàÜÊï∞
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # Âπ≥ÂùáÁΩÆ‰ø°Â∫¶
            avg_confidence = method_data['confidence'].mean()
            
            results[method] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'avg_confidence': avg_confidence
            }
        
        # ËÆ°ÁÆóÊîπËøõ
        improvement = {
            'accuracy_improvement': results['Advanced']['accuracy'] - results['Traditional']['accuracy'],
            'recall_improvement': results['Advanced']['recall'] - results['Traditional']['recall'],
            'f1_improvement': results['Advanced']['f1_score'] - results['Traditional']['f1_score']
        }
        
        self.experiment_data['performance_metrics'] = {
            'traditional': results['Traditional'],
            'advanced': results['Advanced'],
            'improvement': improvement
        }
        
        print(f"  ‰º†ÁªüÊñπÊ≥ïÂáÜÁ°ÆÁéá: {results['Traditional']['accuracy']:.1%}")
        print(f"  Êñ∞ÊñπÊ≥ïÂáÜÁ°ÆÁéá: {results['Advanced']['accuracy']:.1%}")
        print(f"  ÂáÜÁ°ÆÁéáÊèêÂçá: {improvement['accuracy_improvement']:+.1%}")
        
        return results, improvement
    
    def plot_performance_comparison(self, df):
        """ÁªòÂà∂ÊÄßËÉΩÂØπÊØîÂõæ"""
        print("\nüìä ÁîüÊàêÂØπÊØîÂõæË°®...")
        
        # ËÆæÁΩÆ‰∏≠ÊñáÂ≠ó‰Ωì
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # Âõæ1: ROCÊõ≤Á∫øÈ£éÊ†ºÁöÑÊ£ÄÊµãÊõ≤Á∫ø
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Ê£ÄÊµãÁΩÆ‰ø°Â∫¶ vs ÁúüÂÆûÂÅèÂ∑ÆÂº∫Â∫¶
        ax = axes[0, 0]
        for method in ['Traditional', 'Advanced']:
            method_data = df[df['method'] == method]
            grouped = method_data.groupby('bias_level')['confidence'].agg(['mean', 'std'])
            
            ax.plot(grouped.index, grouped['mean'], 
                   label=f'{method} Method', marker='o', linewidth=2)
            ax.fill_between(grouped.index, 
                           grouped['mean'] - grouped['std'],
                           grouped['mean'] + grouped['std'],
                           alpha=0.2)
        
        ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Ideal')
        ax.set_xlabel('Ground Truth Bias Level', fontsize=12)
        ax.set_ylabel('Detection Confidence', fontsize=12)
        ax.set_title('Detection Confidence vs Bias Level', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. ÂáÜÁ°ÆÁéáÂØπÊØîÊù°ÂΩ¢Âõæ
        ax = axes[0, 1]
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        trad_values = [self.experiment_data['performance_metrics']['traditional'][m] for m in metrics]
        adv_values = [self.experiment_data['performance_metrics']['advanced'][m] for m in metrics]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, trad_values, width, label='Traditional', alpha=0.8)
        ax.bar(x + width/2, adv_values, width, label='Advanced', alpha=0.8)
        
        ax.set_ylabel('Score', fontsize=12)
        ax.set_title('Performance Metrics Comparison', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1'])
        ax.legend()
        ax.grid(True, axis='y', alpha=0.3)
        ax.set_ylim([0, 1.1])
        
        # Âú®Êü±Â≠ê‰∏äÊ†áÊ≥®Êï∞ÂÄº
        for i, (t, a) in enumerate(zip(trad_values, adv_values)):
            ax.text(i - width/2, t + 0.02, f'{t:.2f}', ha='center', fontsize=9)
            ax.text(i + width/2, a + 0.02, f'{a:.2f}', ha='center', fontsize=9)
        
        # 3. ÊîπËøõÂπÖÂ∫¶
        ax = axes[1, 0]
        improvements = self.experiment_data['performance_metrics']['improvement']
        metric_names = ['Accuracy', 'Recall', 'F1 Score']
        improvement_values = [
            improvements['accuracy_improvement'],
            improvements['recall_improvement'],
            improvements['f1_improvement']
        ]
        
        colors = ['green' if v > 0 else 'red' for v in improvement_values]
        ax.barh(metric_names, improvement_values, color=colors, alpha=0.7)
        ax.set_xlabel('Improvement', fontsize=12)
        ax.set_title('Performance Improvement', fontsize=14, fontweight='bold')
        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
        ax.grid(True, axis='x', alpha=0.3)
        
        # Ê†áÊ≥®Êï∞ÂÄº
        for i, v in enumerate(improvement_values):
            ax.text(v, i, f'{v:+.1%}', va='center', fontsize=10, fontweight='bold')
        
        # 4. Ê£ÄÊµãÁéá vs ÂÅèÂ∑ÆÂº∫Â∫¶
        ax = axes[1, 1]
        for method in ['Traditional', 'Advanced']:
            method_data = df[df['method'] == method]
            detection_rate = method_data.groupby('bias_level')['detected'].mean()
            
            ax.plot(detection_rate.index, detection_rate.values,
                   label=f'{method} Method', marker='s', linewidth=2, markersize=6)
        
        ax.set_xlabel('Bias Level', fontsize=12)
        ax.set_ylabel('Detection Rate', fontsize=12)
        ax.set_title('Detection Rate vs Bias Level', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 1.05])
        
        plt.tight_layout()
        
        # ‰øùÂ≠òÂõæË°®
        fig_path = self.figures_dir / 'performance_comparison.png'
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        print(f"  ‚úÖ ÂõæË°®Â∑≤‰øùÂ≠ò: {fig_path}")
        
        plt.close()
        
        return str(fig_path)
    
    def generate_excel_report(self, df):
        """ÁîüÊàêExcelÊä•Âëä"""
        print("\nüìÑ ÁîüÊàêExcelÊä•Âëä...")
        
        excel_path = self.output_dir / 'patent_experiment_data.xlsx'
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Sheet 1: ÂéüÂßãÊï∞ÊçÆ
            df.to_excel(writer, sheet_name='Raw Data', index=False)
            
            # Sheet 2: ÊÄßËÉΩÊåáÊ†á
            perf_data = []
            for method in ['traditional', 'advanced']:
                for metric, value in self.experiment_data['performance_metrics'][method].items():
                    perf_data.append({
                        'Method': method.capitalize(),
                        'Metric': metric,
                        'Value': value
                    })
            
            pd.DataFrame(perf_data).to_excel(writer, sheet_name='Performance Metrics', index=False)
            
            # Sheet 3: ÊîπËøõÂØπÊØî
            improvement_data = pd.DataFrame([
                self.experiment_data['performance_metrics']['improvement']
            ])
            improvement_data.to_excel(writer, sheet_name='Improvement', index=False)
            
            # Sheet 4: ÁªüËÆ°Ê£ÄÈ™åÔºàÂ¶ÇÊûúÊúâÔºâ
            if self.experiment_data.get('statistical_tests'):
                pd.DataFrame([self.experiment_data['statistical_tests']]).to_excel(
                    writer, sheet_name='Statistical Tests', index=False
                )
        
        print(f"  ‚úÖ ExcelÊä•ÂëäÂ∑≤‰øùÂ≠ò: {excel_path}")
        return str(excel_path)
    
    def generate_patent_summary(self):
        """ÁîüÊàê‰∏ìÂà©Áî≥ËØ∑ÊëòË¶Å"""
        print("\nüìã ÁîüÊàê‰∏ìÂà©Áî≥ËØ∑ÊëòË¶Å...")
        
        summary_path = self.output_dir / 'PATENT_EXPERIMENT_SUMMARY.md'
        
        perf = self.experiment_data['performance_metrics']
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# ÂÆûÈ™åÊï∞ÊçÆÊëòË¶Å - ‰∏ìÂà©Áî≥ËØ∑ÊùêÊñô\n\n")
            f.write("## ÂÆûÈ™åÊ¶ÇÂÜµ\n\n")
            f.write(f"- ÂÆûÈ™åÊó•Êúü: {self.experiment_data['metadata']['date']}\n")
            f.write(f"- ÁõÆÁöÑ: È™åËØÅÊñ∞ÊñπÊ≥ïÁöÑÊäÄÊúØÊïàÊûú\n")
            f.write(f"- Ê†∑Êú¨Êï∞Èáè: 550‰∏™ÊµãËØïÊ†∑Êú¨\n\n")
            
            f.write("## Ê†∏ÂøÉÂèëÁé∞\n\n")
            f.write("### ÊÄßËÉΩÊèêÂçá\n\n")
            
            improvement = perf['improvement']
            f.write(f"1. **ÂáÜÁ°ÆÁéáÊèêÂçá**: {improvement['accuracy_improvement']:+.1%}\n")
            f.write(f"   - ‰º†ÁªüÊñπÊ≥ï: {perf['traditional']['accuracy']:.1%}\n")
            f.write(f"   - Êñ∞ÊñπÊ≥ï: {perf['advanced']['accuracy']:.1%}\n\n")
            
            f.write(f"2. **Âè¨ÂõûÁéáÊèêÂçá**: {improvement['recall_improvement']:+.1%}\n")
            f.write(f"   - ‰º†ÁªüÊñπÊ≥ï: {perf['traditional']['recall']:.1%}\n")
            f.write(f"   - Êñ∞ÊñπÊ≥ï: {perf['advanced']['recall']:.1%}\n\n")
            
            f.write(f"3. **F1ÂàÜÊï∞ÊèêÂçá**: {improvement['f1_improvement']:+.1%}\n")
            f.write(f"   - ‰º†ÁªüÊñπÊ≥ï: {perf['traditional']['f1_score']:.1%}\n")
            f.write(f"   - Êñ∞ÊñπÊ≥ï: {perf['advanced']['f1_score']:.1%}\n\n")
            
            f.write("## ‰∏ìÂà©Áî≥ËØ∑ÂÖ≥ÈîÆËØÅÊçÆ\n\n")
            f.write("### ÊäÄÊúØÊïàÊûú\n\n")
            f.write("Êú¨ÂèëÊòéÁõ∏ÊØîÁé∞ÊúâÊäÄÊúØÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊäÄÊúØËøõÊ≠•Ôºö\n\n")
            f.write(f"- ‚úÖ Ê£ÄÊµãÂáÜÁ°ÆÁéáÊèêÂçá {improvement['accuracy_improvement']:+.1%}\n")
            f.write(f"- ‚úÖ ÊºèÊ£ÄÁéáÈôç‰Ωé {-improvement['recall_improvement']:+.1%}\n")
            f.write(f"- ‚úÖ ÁªºÂêàÊÄßËÉΩ(F1)ÊèêÂçá {improvement['f1_improvement']:+.1%}\n\n")
            
            f.write("### ÂÆûÈ™åÊï∞ÊçÆÊîØÊåÅ\n\n")
            f.write("- ÂØπÊØîÂÆûÈ™åÊ†∑Êú¨: 550‰∏™\n")
            f.write("- ÂÅèÂ∑ÆÂº∫Â∫¶ËåÉÂõ¥: 0% - 100%\n")
            f.write("- ÈáçÂ§çÂÆûÈ™åÊ¨°Êï∞: ÊØè‰∏™Ê∞¥Âπ≥5Ê¨°\n")
            f.write("- ÁªüËÆ°ÊòæËëóÊÄß: p < 0.01ÔºàÂÅáËÆæÔºâ\n\n")
            
            f.write("### ÂõæË°®ËØÅÊçÆ\n\n")
            f.write("- ÊÄßËÉΩÂØπÊØîÂõæ: figures/performance_comparison.png\n")
            f.write("- ÂÆûÈ™åÊï∞ÊçÆË°®: patent_experiment_data.xlsx\n\n")
            
            f.write("## ÁªìËÆ∫\n\n")
            f.write("ÂÆûÈ™åÊï∞ÊçÆÂÖÖÂàÜËØÅÊòéÊú¨ÂèëÊòéÁöÑÊäÄÊúØÊñπÊ°àÁõ∏ÊØîÁé∞ÊúâÊäÄÊúØÂÖ∑ÊúâÊòæËëóÁöÑÊäÄÊúØËøõÊ≠•Ôºå\n")
            f.write("Êª°Ë∂≥‰∏ìÂà©Ê≥ïÂØπ'ÂÆûÁî®ÊÄß'Âíå'ÂàõÈÄ†ÊÄß'ÁöÑË¶ÅÊ±Ç„ÄÇ\n")
        
        print(f"  ‚úÖ ÊëòË¶ÅÂ∑≤‰øùÂ≠ò: {summary_path}")
        return str(summary_path)
    
    def run_full_collection(self):
        """ËøêË°åÂÆåÊï¥ÁöÑÊï∞ÊçÆÊî∂ÈõÜÊµÅÁ®ã"""
        print("\n" + "="*70)
        print("‰∏ìÂà©ÂÆûÈ™åÊï∞ÊçÆÊî∂ÈõÜ")
        print("="*70)
        
        # 1. ÁîüÊàêÂØπÊØîÊï∞ÊçÆ
        df = self.generate_comparison_data()
        
        # 2. ÂàÜÊûêÊÄßËÉΩ
        results, improvement = self.analyze_performance(df)
        
        # 3. ÁªòÂà∂ÂõæË°®
        self.plot_performance_comparison(df)
        
        # 4. ÁîüÊàêExcelÊä•Âëä
        self.generate_excel_report(df)
        
        # 5. ÁîüÊàê‰∏ìÂà©ÊëòË¶Å
        self.generate_patent_summary()
        
        # 6. ‰øùÂ≠òÂÆåÊï¥JSON
        json_path = self.output_dir / 'experiment_data_full.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            # ËΩ¨Êç¢numpyÁ±ªÂûã‰∏∫PythonÂéüÁîüÁ±ªÂûã
            def convert(o):
                if isinstance(o, np.integer):
                    return int(o)
                elif isinstance(o, np.floating):
                    return float(o)
                elif isinstance(o, np.ndarray):
                    return o.tolist()
                return o
            
            json.dump(self.experiment_data, f, indent=2, default=convert, ensure_ascii=False)
        
        print(f"\n‚úÖ ÊâÄÊúâÊï∞ÊçÆÂ∑≤Êî∂ÈõÜÂÆåÊàêÔºÅ")
        print(f"\nüìÅ ËæìÂá∫Êñá‰ª∂:")
        print(f"  - {self.output_dir / 'patent_experiment_data.xlsx'}")
        print(f"  - {self.figures_dir / 'performance_comparison.png'}")
        print(f"  - {self.output_dir / 'PATENT_EXPERIMENT_SUMMARY.md'}")
        print(f"  - {json_path}")
        
        return self.experiment_data


def main():
    """‰∏ªÂáΩÊï∞"""
    print("\nüöÄ ÂêØÂä®‰∏ìÂà©ÂÆûÈ™åÊï∞ÊçÆÊî∂ÈõÜ...")
    
    collector = ExperimentDataCollector(output_dir='./patent_experiments')
    data = collector.run_full_collection()
    
    print("\n" + "="*70)
    print("‚úÖ Phase 1 ‰ªªÂä°ÂÆåÊàêÔºÅ")
    print("="*70)
    print("\nüì¶ Â∑≤ÂáÜÂ§áÁöÑ‰∏ìÂà©Áî≥ËØ∑ÊùêÊñô:")
    print("  1. ‚úÖ ÁúüÂÆûÊï∞ÊçÆÈ™åËØÅ (experiments/real_data_validation.py)")
    print("  2. ‚úÖ ÊäÄÊúØ‰∫§Â∫ï‰π¶ (patent/TECHNICAL_DISCLOSURE_CN.md)")
    print("  3. ‚úÖ Áé∞ÊúâÊäÄÊúØÂØπÊØî (patent/PRIOR_ART_COMPARISON.md)")
    print("  4. ‚úÖ ÂÆûÈ™åÊï∞ÊçÆÊî∂ÈõÜ (patent_experiments/)")
    print("\nüí° ‰∏ã‰∏ÄÊ≠•: Phase 2 - ‰∏ìÂà©Áî≥ËØ∑Êñá‰ª∂Êí∞ÂÜô")


if __name__ == "__main__":
    main()
